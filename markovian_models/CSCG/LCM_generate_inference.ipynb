{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0f398a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LCM_infer(X, opts=None):\n",
    "    \"\"\"\n",
    "    Particle filtering or local MAP inference for the latent cause model of associative learning.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): [T x D] stimulus inputs, where T is the number of timepoints and D is the number of stimulus features.\n",
    "                       Binary features are assumed. The first feature (column 0) is the US, and the rest are CSs.\n",
    "    opts (dict): Optional dictionary containing various options (see LCM_opts). If opts['M'] == 1, local MAP is used.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with results, containing:\n",
    "        - 'opts': Options used\n",
    "        - 'V': [T x 1] US predictions\n",
    "        - 'post': [T x K] latent cause posterior probabilities\n",
    "    \"\"\"\n",
    "    # Set parameters\n",
    "    opts = LCM_opts(opts) if opts else {}\n",
    "    M = opts.get('M', 1)\n",
    "    a = opts.get('a', 1)\n",
    "    b = opts.get('b', 1)\n",
    "    alpha = opts.get('alpha', 0)\n",
    "    stickiness = opts.get('stickiness', 0)\n",
    "    K = opts.get('K', 1 if alpha == 0 else opts.get('K'))\n",
    "\n",
    "    results = {'opts': opts}\n",
    "    T, D = X.shape\n",
    "#     print(T,D)\n",
    "\n",
    "    # Initialization\n",
    "    post = np.zeros((1, K))\n",
    "    post[0, 0] = 1\n",
    "    post0 = np.zeros((M, K))\n",
    "    post0[:, 0] = 1\n",
    "    N = np.zeros((M, K, D))  # feature-cause co-occurrence counts\n",
    "    B = np.zeros((M, K, D))  # co-occurrence counts for features absent\n",
    "    Nk = np.zeros((M, K))     # cause counts\n",
    "    results['post'] = np.hstack([np.ones((T, 1)), np.zeros((T, K - 1))])\n",
    "    results['V'] = np.zeros((T, 1))\n",
    "    z = np.ones(M, dtype=int)\n",
    "\n",
    "    # Loop over trials\n",
    "    for t in range(T):\n",
    "        # Calculate likelihood\n",
    "        lik = N.copy()\n",
    "        lik[:, :, X[t, :] == 0] = B[:, :, X[t, :] == 0]\n",
    "        lik = (lik + a) / (Nk[:, :, np.newaxis] + a + b)\n",
    "#         print(lik)\n",
    "\n",
    "        if alpha > 0:  # Only update if concentration parameter is non-zero\n",
    "            # Calculate CRP prior\n",
    "            prior = Nk.copy()\n",
    "            for m in range(M):\n",
    "                prior[m, z[m] - 1] += stickiness  # Add stickiness\n",
    "                prior[m, np.where(prior[m, :] == 0)[0][0]] = alpha  # New latent cause\n",
    "            prior /= np.sum(prior, axis=1, keepdims=True)\n",
    "\n",
    "            # Posterior conditional on CS only\n",
    "            post = prior * np.prod(lik[:, :, 1:D], axis=2)\n",
    "            post0 = post / np.sum(post, axis=1, keepdims=True)\n",
    "\n",
    "            # Posterior conditional on CS and US\n",
    "            post *= lik[:, :, 0]\n",
    "            post /= np.sum(post)\n",
    "\n",
    "        results['post'][t, :] = np.mean(post / np.sum(post, axis=1, keepdims=True), axis=0)\n",
    "\n",
    "        # Posterior predictive mean for US\n",
    "        pUS = (N[:, :, 0] + a) / (Nk + a + b)\n",
    "        results['V'][t, 0] = np.dot(post0.flatten(), pUS.flatten()) / M\n",
    "\n",
    "        # Sample new particles\n",
    "        x1 = X[t, :] == 1 # indices where obs occurred in a trial\n",
    "        x0 = X[t, :] == 0 # indices where obs didn't occur in a trial\n",
    "\n",
    "        if M == 1:\n",
    "            z = np.argmax(post) + 1  # Max a posteriori\n",
    "            Nk[0, z - 1] += 1\n",
    "            N[0, z - 1, x1] += 1\n",
    "            B[0, z - 1, x0] += 1\n",
    "        else:\n",
    "            Nk_old, N_old, B_old = Nk.copy(), N.copy(), B.copy()\n",
    "            for m in range(M):\n",
    "                row = np.min(np.where(np.random.rand() < np.cumsum(np.sum(post, axis=1)))[0])\n",
    "                # out of the N hypothesis, \n",
    "\n",
    "                Nk[m, :] = Nk_old[row, :]\n",
    "                N[m, :, :] = N_old[row, :, :]\n",
    "                B[m, :, :] = B_old[row, :, :]\n",
    "                a = np.random.rand()\n",
    "                col = np.min(np.where(a < np.cumsum(post[row, :] / np.sum(post[row, :])))[0])\n",
    "                # why do np.cumsum()? because we want to random select within expanded latent causes\n",
    "                \n",
    "                Nk[m, col] += 1\n",
    "                N[m, col, x1] += 1\n",
    "                B[m, col, x0] += 1\n",
    "                \n",
    "                # why assign observations to randomly selected latent cause? \n",
    "                # because we believe that even if we do this, results will converge\n",
    "\n",
    "    # Remove unused particles\n",
    "    unused = np.mean(results['post'], axis=0) == 0\n",
    "    results['post'] = results['post'][:, ~unused]\n",
    "\n",
    "    return results\n",
    "\n",
    "def LCM_opts(opts):\n",
    "    \"\"\"Default options for the latent cause model.\"\"\"\n",
    "    default_opts = {\n",
    "        'M': 1,          # Number of particles\n",
    "        'a': 1.0,        # Parameter for likelihood\n",
    "        'b': 1.0,        # Parameter for likelihood\n",
    "        'alpha': 0.0,    # Concentration parameter for CRP\n",
    "        'stickiness': 0.0,  # Stickiness for CRP prior\n",
    "        'K': 5           # Initial number of latent causes\n",
    "    }\n",
    "    if opts is None:\n",
    "        return default_opts\n",
    "    default_opts.update(opts)\n",
    "    return default_opts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e51afbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.09090909 0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.03274878 0.05816031 0.         0.         0.        ]\n",
      "[0.36023663 0.63976337 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.09804454 0.01382606 0.         0.         0.        ]\n",
      "[0.87641024 0.12358976 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.0173569  0.04774812 0.01382606 0.         0.        ]\n",
      "[0.21989945 0.6049343  0.17516625 0.         0.        ]\n",
      "\n",
      "\n",
      "[0.09804454 0.01382606 0.         0.         0.        ]\n",
      "[0.87641024 0.12358976 0.         0.         0.        ]\n",
      "\n",
      "\n",
      "{'opts': {'M': 11, 'a': 1.0, 'b': 1.0, 'alpha': 0.5, 'stickiness': 0.0, 'K': 5}, 'post': array([[1.        , 0.        , 0.        ],\n",
      "       [0.36023663, 0.63976337, 0.        ],\n",
      "       [0.45863064, 0.42989992, 0.11146943]]), 'V': array([[0.5       ],\n",
      "       [0.55795892],\n",
      "       [0.33442764]])}\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])  # Example stimulus input\n",
    "opts = {'M': 11, 'alpha': 0.5}  # Example options\n",
    "results = LCM_infer(X, opts)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "945336c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8603552520681103"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab91b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.37209302, 0.62790698],\n",
       "       [0.8       , 0.2       ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b56f980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       ],\n",
       "       [0.57843137],\n",
       "       [0.5       ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e88f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:]==1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cscg",
   "language": "python",
   "name": "cscg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
