{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_modular_graph_varied_with_visualization(\n",
    "    num_nodes=15, num_observations=10000, num_aliased_states=10, num_modules=3\n",
    "):\n",
    "    if num_nodes < 4:\n",
    "        raise ValueError(\"num_nodes must be at least 4 to allow for meaningful connectivity.\")\n",
    "\n",
    "    if num_nodes < num_modules:\n",
    "        raise ValueError(\"Number of nodes must be at least equal to the number of modules to form a meaningful structure.\")\n",
    "\n",
    "    # Initialize the adjacency list for variable actions per node\n",
    "    adjacency_list = {node: [] for node in range(num_nodes)}\n",
    "\n",
    "    # Calculate the size of each module\n",
    "    module_size = num_nodes // num_modules\n",
    "\n",
    "    for module_index in range(num_modules):\n",
    "        module_start = module_index * module_size\n",
    "        # For the last module, extend to the end of the node list\n",
    "        module_end = module_start + module_size if module_index < num_modules - 1 else num_nodes\n",
    "\n",
    "        # Fully connect nodes within the module\n",
    "        for i in range(module_start, module_end):\n",
    "            for j in range(module_start, module_end):\n",
    "                if i != j:\n",
    "                    adjacency_list[i].append(j)\n",
    "\n",
    "    # Optionally, add sparse inter-module connections\n",
    "    for module_index in range(num_modules - 1):\n",
    "        module_end = (module_index + 1) * module_size - 1\n",
    "        next_module_start = (module_end + 1) % num_nodes\n",
    "        adjacency_list[module_end].append(next_module_start)\n",
    "        adjacency_list[next_module_start].append(module_end)\n",
    "\n",
    "    # Connect first and last module\n",
    "    adjacency_list[0].append(num_nodes - 1)\n",
    "    adjacency_list[num_nodes - 1].append(0)\n",
    "\n",
    "    # Generate observations based on random walks on the lattice graph\n",
    "    states = [np.random.choice(range(num_nodes))]  # Start from a random state\n",
    "    actions = []  # Keep track of the actions taken\n",
    "\n",
    "    for _ in range(1, num_observations):\n",
    "        current_state = states[-1]\n",
    "        # Actions correspond to outbound connections for the current node\n",
    "        outbound_connections = adjacency_list[current_state]\n",
    "        action = np.random.choice(len(outbound_connections))  # Choose an action\n",
    "        next_state = outbound_connections[action]  # Follow the chosen action\n",
    "        states.append(next_state)\n",
    "        actions.append(action)\n",
    "\n",
    "    # Map states to observations with aliasing\n",
    "    if num_aliased_states > num_nodes or num_aliased_states < 1:\n",
    "        raise ValueError(\"num_aliased_states must be between 1 and the number of nodes.\")\n",
    "\n",
    "    unique_obs = np.arange(num_nodes - num_aliased_states)\n",
    "    for n in range(num_aliased_states):\n",
    "        unique_obs = np.append(unique_obs, random.choice(unique_obs))\n",
    "    state_to_obs = unique_obs  # Aliasing version\n",
    "\n",
    "    # Create observation data\n",
    "    x = state_to_obs[states]\n",
    "\n",
    "    # Create a transition graph using networkx\n",
    "    G = nx.DiGraph()\n",
    "    for node, neighbors in adjacency_list.items():\n",
    "        for idx, neighbor in enumerate(neighbors):\n",
    "            G.add_edge(node, neighbor, action=idx)\n",
    "\n",
    "    return x, actions, G\n",
    "\n",
    "# Visualization function for the graph\n",
    "def visualize_transition_graph(G):\n",
    "    pos = nx.spring_layout(G)  # Position nodes for visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    edge_labels = nx.get_edge_attributes(G, \"action\")  # Get edge attributes\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw(\n",
    "        G,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_color=\"lightblue\",\n",
    "        edge_color=\"gray\",\n",
    "        node_size=500,\n",
    "        font_size=10,\n",
    "        arrowsize=20,\n",
    "    )\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "    plt.title(\"Transition Structure Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "x, actions, G = create_modular_graph_varied_with_visualization(\n",
    "    num_nodes=15, num_observations=1000, num_aliased_states=3, num_modules=3\n",
    ")\n",
    "visualize_transition_graph(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create observation data\n",
    "num_nodes=30\n",
    "num_observations=1000\n",
    "num_modules=3\n",
    "aliasing=3\n",
    "\n",
    "num_aliased_states = num_nodes//aliasing\n",
    "# observations = create_modular_graph_varied(num_nodes, num_observations, num_aliased_states, num_modules)\n",
    "observations, actions, G = create_modular_graph_varied_with_visualization(\n",
    "    num_nodes=15, num_observations=1000, num_aliased_states=3, num_modules=3\n",
    ")\n",
    "# actions = np.zeros(len(observations), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value\n",
    "unique, counts = np.unique(observations, return_counts=True)\n",
    "# Create a dictionary of the counts\n",
    "result = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Counts of 0, 1, 2, 3:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @nb.njit\n",
    "class Agent:\n",
    "    def __init__(self, num_obs=4, num_actions=4, pseudocount=1e-3):\n",
    "        self.num_obs = num_obs\n",
    "        self.groups_of_tables = {}\n",
    "        self.table_totals = {}  # Keep track of totals for each table separately\n",
    "        self.total_observations = 0  # Keep track of total observations across all tables\n",
    "        self.count = np.ones((num_obs,num_obs,num_actions))\n",
    "        self.TM = self.count.copy()\n",
    "        self.normalize_TM()\n",
    "        self.initialize_clones(num_obs)\n",
    "        self.pseudocount = pseudocount\n",
    "\n",
    "\n",
    "    def initialize_clones(self,num_obs):\n",
    "        for restaurant_id in range(num_obs):\n",
    "            self.groups_of_tables[restaurant_id] = {}\n",
    "            self.groups_of_tables[restaurant_id][0] = 1\n",
    "    \n",
    "    def normalize_TM(self):\n",
    "        num_obs = np.shape(self.count)[0]\n",
    "        num_actions = np.shape(self.count)[2]\n",
    "        count = self.count.copy()\n",
    "        for s in range(num_obs):\n",
    "            for a in range(num_actions):\n",
    "                self.TM[s, :, a] = count[s,:,a] / count[s,:,a].sum()\n",
    "                \n",
    "                \n",
    "    def add_clone(self, restaurant_id, table_id):\n",
    "        \"\"\"Add exactly one clone to a specified table, creating the table or group if necessary.\"\"\"\n",
    "        # Automatically create the group and table if they don't exist\n",
    "        if restaurant_id not in self.groups_of_tables:\n",
    "            self.groups_of_tables[restaurant_id] = {}\n",
    "        if table_id not in self.groups_of_tables[restaurant_id]:\n",
    "            self.groups_of_tables[restaurant_id][table_id] = 0  # Initialize clones count for the table\n",
    "\n",
    "        # Add one clone to the table count and update total observations\n",
    "        self.groups_of_tables[restaurant_id][table_id] += 1\n",
    "        self.table_totals[(restaurant_id, table_id)] = self.groups_of_tables[restaurant_id][table_id]  # Update table total\n",
    "        self.total_observations += 1\n",
    "\n",
    "    def get_total_observations(self):\n",
    "        \"\"\"Return the total number of observations.\"\"\"\n",
    "        return self.total_observations\n",
    "\n",
    "    def get_restaurant_total_customers(self, restaurant_id):\n",
    "        \"\"\"Return the total number of clones in all tables within a specific restaurant.\"\"\"\n",
    "        return sum(self.groups_of_tables.get(restaurant_id, {}).values())\n",
    "\n",
    "    def get_table_total_customers(self, restaurant_id, table_id):\n",
    "        \"\"\"Return the total number of clones for a specific table.\"\"\"\n",
    "        return self.groups_of_tables.get(restaurant_id, {}).get(table_id, 0)\n",
    "    \n",
    "#     def get_prior_distribution(self):\n",
    "\n",
    "#         return [self.get_restaurant_total_customers(restaurant_id for restaurant_id in range(self.num_obs))\n",
    "#                 /self.get_total_observations()]\n",
    "\n",
    "    def count_tables_in_restaurant(self, restaurant_id):\n",
    "        \"\"\"Returns the number of tables within the specified restaurant.\"\"\"\n",
    "        if restaurant_id in self.groups_of_tables:\n",
    "            return len(self.groups_of_tables[restaurant_id])\n",
    "        else:\n",
    "            # print(f\"Group {group_id} does not exist.\")\n",
    "            return 0\n",
    "        \n",
    "# agent.update_count((total_clones, prev_action, obs_ind))\n",
    "    def expand_split_TM(self):\n",
    "        # expand dimension\n",
    "        orig_count = self.count.copy()\n",
    "        orig_TM = self.TM.copy()\n",
    "        n_prev_clones = np.shape(orig_count)[0]\n",
    "        n_actions = np.shape(orig_count)[2]\n",
    "        expanded_count = np.zeros((n_prev_clones+1, n_prev_clones+1, n_actions)) + self.pseudocount\n",
    "        expanded_TM = np.zeros((n_prev_clones+1, n_prev_clones+1, n_actions)) + self.pseudocount\n",
    "                \n",
    "        # Copy the original matrix values into the top-left submatrix of the expanded matrix\n",
    "        expanded_count[:n_prev_clones, :n_prev_clones, :] = orig_count\n",
    "        expanded_TM[:n_prev_clones, :n_prev_clones, :] = orig_TM\n",
    "        \n",
    "        self.count = expanded_count\n",
    "        self.TM = expanded_TM\n",
    "        \n",
    "# #         self.count[state, state2]\n",
    "#         for ss in state2:\n",
    "#             self.count[state, ss, action] += 1\n",
    "            \n",
    "    def update_count(self, state, state2, action): # updating counts when not splitted\n",
    "#         for ss in state2: \n",
    "#         self.count[state, state2]\n",
    "#         for ss in state2:\n",
    "        self.count[state, state2, action] += 1\n",
    "\n",
    "        \n",
    "#         for next_state in state2: \n",
    "            \n",
    "#     def update_transition(self, state, state2, action): \n",
    "#         # update transition probability p(s'|s,a)\n",
    "#         self.TM[state, state2, action] += 1\n",
    "        \n",
    "#         # Normalize the probabilities for the current state and action\n",
    "#         self.TM[state, :, action] /= self.TM[state, :, action].sum()        \n",
    "    def merged_likelihood(self, clone_map):\n",
    "        \n",
    "        merged_TM = self.TM.copy()\n",
    "        \n",
    "        curr_obs, clone_num, unique_idx = random.choice(clone_map)\n",
    "        shared_obs_clones = [t for t in clone_map if t[0]==curr_obs and t[1] != clone_num] # all the clones that share the obs\n",
    "        \n",
    "        compare_clones = random.choice(shared_obs_clones) # pick a random clone that shares observation \n",
    "\n",
    "        state_a = unique_idx\n",
    "        state_b = compare_clones[2]\n",
    "        \n",
    "        for s in range(len(clone_map)):\n",
    "            if s != state_a and s != state_b:                \n",
    "                merged_TM[state_a][s] += merged_TM[state_b][s]\n",
    "                merged_TM[s][state_a] += merged_TM[s][state_b] \n",
    "                # Remove state_b by zeroing out probabilities (optional)\n",
    "                merged_TM[state_b][s] = 0\n",
    "                merged_TM[s][state_b] = 0\n",
    "                \n",
    "        \n",
    "def CRP(agent, curr_observation, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Simulates the Chinese Restaurant Process.\n",
    "\n",
    "    Parameters:\n",
    "    - history: int, the total number of customers to simulate.\n",
    "    - alpha: float, the concentration parameter.\n",
    "\n",
    "    Returns:\n",
    "    - A list where the i-th element represents the table number of the i-th customer.\n",
    "    \"\"\"\n",
    "\n",
    "    n = agent.get_restaurant_total_customers(curr_observation)\n",
    "\n",
    "\n",
    "    if curr_observation not in agent.groups_of_tables:\n",
    "        agent.add_clone(curr_observation,0)\n",
    "        table_choice = 0\n",
    "        assignments = 0\n",
    "        probs = 1\n",
    "    else:\n",
    "        \n",
    "        probs = [clone_count / (n + alpha) for table_id, clone_count in \n",
    "               agent.groups_of_tables[curr_observation].items()] + [alpha / (n + alpha)] # This is the prior\n",
    "#         print(probs)\n",
    "    \n",
    "#     probs = np.concatenate(probs)\n",
    "#     print(probs)\n",
    "    # Add an update rule (ref Nora's paper 1st equation)\n",
    "\n",
    "    # Choose a table based on the probabilities\n",
    "    table_choice = np.random.choice(len(probs), p=probs)\n",
    "\n",
    "\n",
    "    # update clone --> existing or new, same\n",
    "    agent.add_clone(curr_observation,table_choice)\n",
    "    assignments = table_choice\n",
    "\n",
    "    return assignments, probs\n",
    "\n",
    "\n",
    "def get_distribution(data):\n",
    "    \"\"\"\n",
    "    Compute the probability distribution from a nested dictionary structure.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A nested dictionary where the outer keys map to inner dictionaries\n",
    "                     containing counts for different elements.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array containing the probability distribution.\n",
    "    \"\"\"\n",
    "    # Flatten the dictionary and compute total count\n",
    "    flattened_counts = defaultdict(int)\n",
    "    total_count = 0\n",
    "    ct = 0\n",
    "    for outer_key, inner_dict in data.items():\n",
    "#         print(outer_key)\n",
    "        for inner_key, count in inner_dict.items():\n",
    "#             print(inner_key)\n",
    "            flattened_counts[ct] += count\n",
    "            total_count += count\n",
    "            ct += 1\n",
    "\n",
    "    # Compute probabilities as a sorted NumPy array\n",
    "    sorted_keys = sorted(flattened_counts.keys())\n",
    "    probabilities = np.array([flattened_counts[key] / total_count for key in sorted_keys])\n",
    "\n",
    "    return probabilities\n",
    "def forward_algorithm(agent, prior, observations, actions, clone_map, clone_history, TM):\n",
    "#     prior = get_distribution(agent.groups_of_tables)\n",
    "#     print(prior)\n",
    "#         TM = self.TM.copy()\n",
    "    num_steps = len(observations)\n",
    "    num_states = np.shape(TM)[0]\n",
    "\n",
    "    log_forward_probs = np.full((num_states, num_steps), 0)  # Use -inf for log(0)\n",
    "\n",
    "    initial_state = observations[0]\n",
    "    for s in range(num_states):\n",
    "        if s == initial_state:\n",
    "#             forward_probs[s, 0] = prior[s]\n",
    "            log_forward_probs[s,0] = prior[s]\n",
    "        else:\n",
    "#             forward_probs[s, 0] = 0  # Probability is zero if it doesn't match initial state\n",
    "            log_forward_probs[s,0] = 0\n",
    "\n",
    "\n",
    "    for t in range(1, num_steps):\n",
    "\n",
    "\n",
    "\n",
    "        current_clone = [clone[2] for clone in clone_map \n",
    "                         if clone[0]==clone_history[t][0] and clone[1]==clone_history[t][1]]\n",
    "    #     print(current_clone)\n",
    "    #         current_state = clone_history[t][1] #states_sequence[t]\n",
    "    #         current\n",
    "        prev_action = actions[t-1]\n",
    "\n",
    "        for s in range(num_states):\n",
    "#             if s == current_clone[0]:\n",
    "#                 forward_probs[s, t] = sum(\n",
    "#                     forward_probs[prev_s, t-1] * agent.TM[prev_s, s, prev_action]\n",
    "#                     for prev_s in range(num_states)\n",
    "#                 )\n",
    "#                 print(forward_probs[s,t])\n",
    "\n",
    "                log_forward_probs[s, t] = np.logaddexp.reduce([\n",
    "                    log_forward_probs[prev_s, t-1] + np.log(TM[prev_s, s, prev_action])\n",
    "                    for prev_s in range(num_states)\n",
    "                ])\n",
    "\n",
    "    return np.sum(log_forward_probs)\n",
    "# def clean_clone_map(data, delete_obs, delete_clone, delete_unique_id):\n",
    "def clean_clone_map(data, delete_items):\n",
    "#     delete_obs = []\n",
    "#     delete_clone = []\n",
    "#     delete_unique_id = []\n",
    "#     for items in delete_items:\n",
    "#         delete_obs.append(items[0])\n",
    "#         delete_clone.append(items[1])\n",
    "#         delete_unique_id.append(items[2])\n",
    "        \n",
    "#     updated_data = []\n",
    "#     offset = 0  # Track the number of removed IDs\n",
    "#     for x, y, uid in data:\n",
    "#         if uid in delete_unique_ids:\n",
    "#             offset += 1  # Increment offset for each deleted ID\n",
    "#         else:\n",
    "#             updated_data.append((x, y-1, uid - offset))  # Adjust the ID by the offset\n",
    "#     return updated_data\n",
    "\n",
    "\n",
    "#     updated_data = [\n",
    "#         (x, y-1, uid - 1 if uid > delete_unique_id else uid)\n",
    "#         for x, y, uid in data\n",
    "#         if uid != delete_unique_id\n",
    "#     ]\n",
    "#     return updated_data\n",
    "\n",
    "    updated_data=[]\n",
    "    for ids in delete_items: \n",
    "        for x,y,uid in data: \n",
    "            if uid < ids[2]:\n",
    "                updated_data.append((x,y,uid))\n",
    "            elif uid > ids[2]: \n",
    "                if x != ids[0]: \n",
    "                    updated_data.append((x,y,uid-1))\n",
    "                else:\n",
    "                    updated_data.append((x,y-1,uid-1))\n",
    "#             else: \n",
    "#                 if x==ids[0]: \n",
    "#                     updated_data.append((x,y-1,uid-1))\n",
    "    return updated_data\n",
    "\n",
    "def viterbi_decoding(agent):\n",
    "    TM = agent.TM.copy()\n",
    "    n_states = np.shape(TM)[0]\n",
    "    n_actions = np.shape(TM)[2]\n",
    "    for s1 in range(n_states):\n",
    "    #         for s2 in range(n_states):\n",
    "        TM_t = np.transpose(TM[s1])\n",
    "        for action in range(n_actions):\n",
    "            currTM = TM_t[action]\n",
    "\n",
    "            for e,element in enumerate(currTM): \n",
    "                if element == np.max(currTM):\n",
    "                    TM[s1][e][action]=1\n",
    "                else:\n",
    "                    TM[s1][e][action]=0\n",
    "    #         sns.heatmap(TM[s1][e])\n",
    "    #         plt.show()\n",
    "#             print(TM[s1][e])\n",
    "    return TM\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: A scalar, vector, or matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - Sigmoid of x.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocount=1e-3\n",
    "np.random.seed(42)\n",
    "\n",
    "alpha = 1\n",
    "n_iter = 10 #100\n",
    "n_particles = 5\n",
    "n_actions = len(np.unique(actions))\n",
    "n_obs = len(np.unique(observations))\n",
    "scaling_factor = 3\n",
    "pes = []\n",
    "agent = Agent(num_obs=n_obs, num_actions=n_actions,pseudocount=pseudocount)\n",
    "for i in range(n_iter): \n",
    "    \n",
    "    # 1. SPLITTING\n",
    "    print('Splitting: iteration {}'.format(i))\n",
    "    print('TM size: {}'.format(np.shape(agent.TM)))\n",
    "    clone_history = []\n",
    "    if i==0: # first iteration        \n",
    "        clone_map= []   \n",
    "        # initialize clones for each obs\n",
    "        for obs in range(n_obs):\n",
    "            clone_map.append((obs,0,obs))    \n",
    "        unique_clone_id = n_obs-1\n",
    "    else: \n",
    "        unique_clone_id = len(clone_map)-1\n",
    "    lik_orig = 0\n",
    "    for o, obs in enumerate(observations): \n",
    "        if o < 2:\n",
    "            clone_history.append((obs,0, obs))\n",
    "            # update transition\n",
    "            if o ==1:\n",
    "                agent.update_count(observations[o-1], observations[o], actions[o]) # update transition between t-1 and t-2\n",
    "                agent.normalize_TM()                        \n",
    "            continue # skip 1st observation\n",
    "            \n",
    "            \n",
    "        splitted = False\n",
    "        \n",
    "        prev_prev_action = actions[o-2]\n",
    "        prev_action = actions[o-1]\n",
    "        prev_obs = observations[o-1]\n",
    "        prev_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "        # get the PE using the transition matrix\n",
    "        \n",
    "        prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "        # Get all state, actions that lead to current obs:\n",
    "        obs_ind = [t[2] for t in clone_map if t[0]==obs] # this is list of curr obs clone id\n",
    "        PE = 0\n",
    "        for oi in obs_ind: # all possible clones related to current observation\n",
    "            for pi in prev_ind: # all possible clones related to t-1 observation\n",
    "            \n",
    "                PE += agent.TM[pi, oi, prev_action] # just sum up PEs for now\n",
    "#         PE = \n",
    "        pes.append(PE)\n",
    "        assignment, _ =  CRP(agent, prev_obs, alpha=scaling_factor/PE)\n",
    "        \n",
    "        post_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "        if prev_tables != post_tables: # a new clone has been created for this observation\n",
    "\n",
    "            splitted = True\n",
    "            unique_clone_id += 1\n",
    "            clone_map.append((prev_obs, assignment, unique_clone_id))\n",
    "            agent.expand_split_TM()\n",
    "            prev_clone = unique_clone_id # clone id of t-1 : unique\n",
    "            \n",
    "        else:\n",
    "\n",
    "            prev_clone = [t[2] for t in clone_map if t[0]==obs and t[1]==assignment] # clone id of t-1 revealed\n",
    "        prev_prev_obs, prev_prev_assignment, prev_prev_clone = clone_history[-1]\n",
    "#         if i > 0: \n",
    "#             print(clone_history)\n",
    "        agent.update_count(prev_prev_clone, prev_clone, prev_prev_action) # update transition between t-1 and t-2\n",
    "        agent.normalize_TM()\n",
    "\n",
    "        clone_history.append((prev_obs, assignment, prev_clone)) # this keeps track of observation & clone history\n",
    "\n",
    "    # 2. FORWARD ALGORITHM TO GET LIKELIHOOD\n",
    "    TM = agent.TM.copy()\n",
    "    prior = get_distribution(agent.groups_of_tables)\n",
    "    log_lik = forward_algorithm(agent, prior, observations, actions, clone_map, clone_history, TM)\n",
    "#     print(log_lik)\n",
    "    print('Log lik for split iteration {}: {}'.format(i, log_lik))\n",
    "    \n",
    "            \n",
    "    # 3. MERGING\n",
    "\n",
    "    for particle in range(n_particles): # many hypotheses // can this be parallelized?\n",
    "        print(\"Particle {}\".format(particle))\n",
    "        curr_obs, clone_num, unique_idx = random.choice(clone_map)\n",
    "        shared_obs_clones = [t for t in clone_map if t[0]==curr_obs and t[1] != clone_num] # all the clones that share the obs\n",
    "    #     prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "    #     _, clone_id = clone_list[idx]\n",
    "        # do the merging by iterating over other clones\n",
    "        # in the sequence, find other clones that could be merged with the current\n",
    "        \n",
    "#         compare_clones = random.choice(shared_obs_clones) # pick a random clone that shares observation \n",
    "        #### make this selection anti-proportional to number of assigned observations\n",
    "        distribution = [agent.groups_of_tables[curr_obs][v[1]] for v in shared_obs_clones]\n",
    "        distribution = np.array(distribution).astype(np.float64)\n",
    "        inverted_probs = 1 / distribution\n",
    "        inverted_probs[distribution == 0] = 0  # Handle zero probabilities if present\n",
    "        normalized_probs = inverted_probs / inverted_probs.sum()\n",
    "#         selected_element = np.random.choice(shared_obs_clones, p=normalized_probs)\n",
    "        \n",
    "        # Randomly select an element based on probabilities\n",
    "        compare_clones = shared_obs_clones[np.random.choice(len(shared_obs_clones), p=normalized_probs)]        \n",
    "\n",
    "        state_a = unique_idx\n",
    "        state_b = compare_clones[2]\n",
    "        merged_TM = agent.TM.copy()\n",
    "\n",
    "        # adjusting prior distribution\n",
    "        merged_prior = get_distribution(agent.groups_of_tables)\n",
    "        merged_prior[state_a] += merged_prior[state_b]\n",
    "        merged_prior[state_b] = 0 #pseudocount    \n",
    "        # adjusting TM\n",
    "        for s in range(np.shape(agent.TM)[0]):\n",
    "            if s != state_a and s != state_b:\n",
    "                for a in range(len(np.unique(actions))):\n",
    "                    merged_TM[state_a][s][a] += merged_TM[state_b][s][a]\n",
    "                    merged_TM[s][state_a][a] += merged_TM[s][state_b][a]\n",
    "                    merged_TM[s][state_b][a] = pseudocount\n",
    "                    merged_TM[state_b][s][a] = pseudocount\n",
    "\n",
    "\n",
    "        # Remove state_b by zeroing out probabilities (optional)\n",
    "    #     agent.TM[state_b] = np.zeros(self.num_states)    \n",
    "\n",
    "        # Get the new prior\n",
    "    #     new_prior = # GET THE NEW DISTRIBUTION\n",
    "\n",
    "        # Get the new likelihood by forward algorithm\n",
    "    #     new_lik = new_prior * new_Tm\n",
    "        new_lik = forward_algorithm(agent, merged_prior, observations, actions, clone_map, clone_history, merged_TM)\n",
    "    #     print(log_lik, new_lik)\n",
    "        if new_lik > log_lik: # if merging is better: \n",
    "            agent.TM = merged_TM\n",
    "            agent.groups_of_tables[curr_obs][clone_num] += agent.groups_of_tables[curr_obs][compare_clones[1]]\n",
    "            agent.groups_of_tables[curr_obs][compare_clones[1]] = 0\n",
    "            print(\"Merged Obs {} clone {} into clone {}, log lik: {}\".format(curr_obs, compare_clones[1], clone_num, new_lik))\n",
    "            \n",
    "            \n",
    "            # clean up clones (including values that are 0)\n",
    "            print(agent.groups_of_tables[curr_obs])\n",
    "            orig_table = agent.groups_of_tables[curr_obs].copy()\n",
    "#             print(orig_table)\n",
    "            orig_table[compare_clones[1]] = 0\n",
    "            delete_indices = [o for o,i in enumerate(orig_table) if orig_table[i]==0]\n",
    "            # Get unique IDs for the delete_indices from clone_map\n",
    "            delete_items = [\n",
    "                t for t in clone_map\n",
    "                for d in delete_indices\n",
    "                if t[0] == curr_obs and t[1] == d\n",
    "            ]\n",
    "#                     delete_uniqueids = [\n",
    "#                 t[2] for t in clone_map\n",
    "#                 for d in delete_indices\n",
    "#                 if t[0] == curr_obs and t[1] == d\n",
    "#             ]\n",
    "            new_table = {}\n",
    "            ni = 0\n",
    "            for i in range(len(orig_table)):\n",
    "#                 print(orig_table[i])\n",
    "                if orig_table[i] != 0: \n",
    "                    new_table[ni] = orig_table[i]\n",
    "                    ni+=1\n",
    "            agent.groups_of_tables[curr_obs] = new_table\n",
    "#             print(agent.groups_of_tables[curr_obs])\n",
    "            \n",
    "            # clean up TM\n",
    "            for d in delete_items:\n",
    "#                 if d >= np.shape(agent.TM)[0]: # MAYBE \n",
    "                agent.TM = np.delete(np.delete(agent.TM, d[2], axis=0), d[2], axis=1)\n",
    "\n",
    "            # clean up COUNT\n",
    "#             for d in delete_items: \n",
    "                agent.count = np.delete(np.delete(agent.count, d[2], axis=0), d[2], axis=1)     \n",
    "            agent.normalize_TM()\n",
    "                \n",
    "            # clean up clone_map\n",
    "#             print(clone_map)\n",
    "#             for ids in delete_uniqueids: \n",
    "#                 print(ids)\n",
    "#                 def clean_clone_map(data, delete_obs, delete_clone, delete_unique_id):\n",
    "            clone_map = clean_clone_map(clone_map, delete_items)\n",
    "#             print(clone_map)\n",
    "            print(np.shape(agent.TM))\n",
    "            \n",
    "                        \n",
    "            log_lik = new_lik    \n",
    "# ####### maybe instead of doing particle filter totally randomly, this can be rule-based too\n",
    "# ####### for example, proportional to clones with least number of tables will be more likely to be pruned\n",
    "# ####### probability for being selected == inverse of customers\n",
    "    \n",
    "    \n",
    "    \n",
    "#         # merge according to number of particles, and update them if they improved\n",
    "                \n",
    "# prev_prev_clone, prev_clone, prev_prev_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Batch updating (recommended to begin with this)\n",
    "\n",
    "\n",
    "room = np.array(\n",
    "    [\n",
    "        [1, 2, 3, 0, 3, 1, 1, 1],\n",
    "        [1, 1, 3, 2, 3, 2, 3, 1],\n",
    "        [1, 1, 2, 0, 1, 2, 1, 0],\n",
    "        [0, 2, 1, 1, 3, 0, 0, 2],\n",
    "        [3, 3, 1, 0, 1, 0, 3, 0],\n",
    "        [2, 1, 2, 3, 3, 3, 2, 0],\n",
    "    ]\n",
    ")\n",
    "actions, observations, rc = datagen_structured_obs_room(room, length=5000)     #Use length=50000 for bigger room\n",
    "pseudocount=1e-3\n",
    "np.random.seed(42)\n",
    "agent = Agent(pseudocount=pseudocount)\n",
    "alpha = 1\n",
    "n_iter = 1 #100\n",
    "n_particles = 50\n",
    "n_actions = len(np.unique(actions))\n",
    "n_obs = len(np.unique(observations))\n",
    "scaling_factor = 10\n",
    "pes = []\n",
    "for i in range(n_iter): \n",
    "    \n",
    "    # 1. SPLITTING\n",
    "    print('Splitting: iteration {}'.format(i))\n",
    "    print('TM size: {}'.format(np.shape(agent.TM)))\n",
    "    clone_history = []\n",
    "    if i==0: # first iteration        \n",
    "        clone_map= []   \n",
    "        # initialize clones for each obs\n",
    "        clone_map.append((0,0,0))\n",
    "        clone_map.append((1,0,1))\n",
    "        clone_map.append((2,0,2))\n",
    "        clone_map.append((3,0,3))\n",
    "    \n",
    "        unique_clone_id =3\n",
    "    else: \n",
    "        unique_clone_id = len(clone_map)-1\n",
    "    lik_orig = 0\n",
    "    for o, obs in enumerate(observations): \n",
    "        if o < 2:\n",
    "            clone_history.append((obs,0, obs))\n",
    "            # update transition\n",
    "            if o ==1:\n",
    "                agent.update_count(observations[o-1], observations[o], actions[o]) # update transition between t-1 and t-2\n",
    "                agent.normalize_TM()                        \n",
    "            continue # skip 1st observation\n",
    "        splitted = False\n",
    "        \n",
    "        prev_prev_action = actions[o-2]\n",
    "        prev_action = actions[o-1]\n",
    "        prev_obs = observations[o-1]\n",
    "        prev_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "        # get the PE using the transition matrix\n",
    "        \n",
    "        prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "        # Get all state, actions that lead to current obs:\n",
    "        obs_ind = [t[2] for t in clone_map if t[0]==obs] # this is list of curr obs clone id\n",
    "        PE = 0\n",
    "        for oi in obs_ind: # all possible clones related to current observation\n",
    "            for pi in prev_ind: # all possible clones related to t-1 observation\n",
    "            \n",
    "                PE += agent.TM[pi, oi, prev_action] # just sum up PEs for now\n",
    "#         PE = \n",
    "        pes.append(PE)\n",
    "        assignment, _ =  CRP(agent, prev_obs, alpha=scaling_factor/PE)\n",
    "        \n",
    "        post_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "        if prev_tables != post_tables: # a new clone has been created for this observation\n",
    "\n",
    "            splitted = True\n",
    "            unique_clone_id += 1\n",
    "            clone_map.append((prev_obs, assignment, unique_clone_id))\n",
    "            agent.expand_split_TM()\n",
    "            prev_clone = unique_clone_id # clone id of t-1 : unique\n",
    "            \n",
    "        else:\n",
    "\n",
    "            prev_clone = [t[2] for t in clone_map if t[0]==obs and t[1]==assignment] # clone id of t-1 revealed\n",
    "        prev_prev_obs, prev_prev_assignment, prev_prev_clone = clone_history[-1]\n",
    "#         if i > 0: \n",
    "#             print(clone_history)\n",
    "        agent.update_count(prev_prev_clone, prev_clone, prev_prev_action) # update transition between t-1 and t-2\n",
    "        agent.normalize_TM()\n",
    "\n",
    "        clone_history.append((prev_obs, assignment, prev_clone)) # this keeps track of observation & clone history\n",
    "\n",
    "    # 2. FORWARD ALGORITHM TO GET LIKELIHOOD\n",
    "    TM = agent.TM.copy()\n",
    "    prior = get_distribution(agent.groups_of_tables)\n",
    "    log_lik = forward_algorithm(agent, prior, observations, actions, clone_map, clone_history, TM)\n",
    "#     print(log_lik)\n",
    "    print('Log lik for split iteration {}: {}'.format(i, log_lik))\n",
    "    \n",
    "            \n",
    "    # 3. MERGING\n",
    "\n",
    "    for particle in range(n_particles): # many hypotheses // can this be parallelized?\n",
    "        print(\"Particle {}\".format(particle))\n",
    "        curr_obs, clone_num, unique_idx = random.choice(clone_map[n_obs:])\n",
    "        shared_obs_clones = [t for t in clone_map[n_obs:] if t[0]==curr_obs and t[1] != clone_num] # all the clones that share the obs\n",
    "    #     prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "    #     _, clone_id = clone_list[idx]\n",
    "        # do the merging by iterating over other clones\n",
    "        # in the sequence, find other clones that could be merged with the current\n",
    "\n",
    "        compare_clones = random.choice(shared_obs_clones) # pick a random clone that shares observation \n",
    "        #### make this selection anti-proportional to number of assigned observations???\n",
    "\n",
    "        state_a = unique_idx\n",
    "        state_b = compare_clones[2]\n",
    "        merged_TM = agent.TM.copy()\n",
    "\n",
    "        # adjusting prior distribution\n",
    "        merged_prior = get_distribution(agent.groups_of_tables)\n",
    "        merged_prior[state_a] += merged_prior[state_b]\n",
    "        merged_prior[state_b] = 0 #pseudocount    \n",
    "        # adjusting TM\n",
    "        for s in range(np.shape(agent.TM)[0]):\n",
    "            if s != state_a and s != state_b:\n",
    "                for a in range(len(np.unique(actions))):\n",
    "                    merged_TM[state_a][s][a] += merged_TM[state_b][s][a]\n",
    "                    merged_TM[s][state_a][a] += merged_TM[s][state_b][a]\n",
    "                    merged_TM[s][state_b][a] = pseudocount\n",
    "                    merged_TM[state_b][s][a] = pseudocount\n",
    "\n",
    "\n",
    "        # Remove state_b by zeroing out probabilities (optional)\n",
    "    #     agent.TM[state_b] = np.zeros(self.num_states)    \n",
    "\n",
    "        # Get the new prior\n",
    "    #     new_prior = # GET THE NEW DISTRIBUTION\n",
    "\n",
    "        # Get the new likelihood by forward algorithm\n",
    "    #     new_lik = new_prior * new_Tm\n",
    "        new_lik = forward_algorithm(agent, merged_prior, observations, actions, clone_map, clone_history, merged_TM)\n",
    "    #     print(log_lik, new_lik)\n",
    "        if new_lik > log_lik: # if merging is better: \n",
    "            agent.TM = merged_TM\n",
    "            agent.groups_of_tables[curr_obs][clone_num] += agent.groups_of_tables[curr_obs][compare_clones[1]]\n",
    "            agent.groups_of_tables[curr_obs][compare_clones[1]] = 0\n",
    "            print(\"Merged Obs {} clone {} into clone {}, log lik: {}\".format(curr_obs, compare_clones[1], clone_num, new_lik))\n",
    "            \n",
    "            \n",
    "            # clean up clones (including values that are 0)\n",
    "            print(agent.groups_of_tables[curr_obs])\n",
    "            orig_table = agent.groups_of_tables[curr_obs].copy()\n",
    "#             print(orig_table)\n",
    "            orig_table[compare_clones[1]] = 0\n",
    "            delete_indices = [o for o,i in enumerate(orig_table) if orig_table[i]==0]\n",
    "            # Get unique IDs for the delete_indices from clone_map\n",
    "            delete_items = [\n",
    "                t for t in clone_map\n",
    "                for d in delete_indices\n",
    "                if t[0] == curr_obs and t[1] == d\n",
    "            ]\n",
    "#                     delete_uniqueids = [\n",
    "#                 t[2] for t in clone_map\n",
    "#                 for d in delete_indices\n",
    "#                 if t[0] == curr_obs and t[1] == d\n",
    "#             ]\n",
    "            new_table = {}\n",
    "            ni = 0\n",
    "            for i in range(len(orig_table)):\n",
    "#                 print(orig_table[i])\n",
    "                if orig_table[i] != 0: \n",
    "                    new_table[ni] = orig_table[i]\n",
    "                    ni+=1\n",
    "            agent.groups_of_tables[curr_obs] = new_table\n",
    "#             print(agent.groups_of_tables[curr_obs])\n",
    "            \n",
    "            # clean up TM\n",
    "            for d in delete_items:\n",
    "#                 if d >= np.shape(agent.TM)[0]: # MAYBE \n",
    "                agent.TM = np.delete(np.delete(agent.TM, d[2], axis=0), d[2], axis=1)\n",
    "\n",
    "            # clean up COUNT\n",
    "            for d in delete_items: \n",
    "                agent.count = np.delete(np.delete(agent.count, d[2], axis=0), d[2], axis=1)     \n",
    "                \n",
    "            # clean up clone_map\n",
    "#             print(clone_map)\n",
    "#             for ids in delete_uniqueids: \n",
    "#                 print(ids)\n",
    "#                 def clean_clone_map(data, delete_obs, delete_clone, delete_unique_id):\n",
    "            clone_map = clean_clone_map(clone_map, delete_items)\n",
    "#             print(clone_map)\n",
    "            print(np.shape(agent.TM))\n",
    "            \n",
    "                        \n",
    "            log_lik = new_lik    \n",
    "# ####### maybe instead of doing particle filter totally randomly, this can be rule-based too\n",
    "# ####### for example, proportional to clones with least number of tables will be more likely to be pruned\n",
    "# ####### probability for being selected == inverse of customers\n",
    "    \n",
    "    \n",
    "    \n",
    "#         # merge according to number of particles, and update them if they improved\n",
    "                \n",
    "# prev_prev_clone, prev_clone, prev_prev_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(10/np.array(pes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example transition matrix (state, state, action)\n",
    "# transition_matrix = agent.TM\n",
    "TM=viterbi_decoding(agent)\n",
    "# TM = agent.TM\n",
    "num_states = np.shape(TM)[0]\n",
    "num_actions = np.shape(TM)[2]\n",
    "\n",
    "# Threshold to show only significant transitions\n",
    "threshold = 0.9\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each state\n",
    "for state in range(num_states):\n",
    "    G.add_node(state)\n",
    "\n",
    "# Add edges based on the transition matrix\n",
    "for action in range(num_actions):\n",
    "    for from_state in range(num_states):\n",
    "        for to_state in range(num_states):\n",
    "            weight = TM[from_state, to_state, action]\n",
    "            if weight > threshold:  # Filter out weak transitions\n",
    "                G.add_edge(\n",
    "                    from_state,\n",
    "                    to_state,\n",
    "                    weight=weight,\n",
    "                    label=f\"A{action}\"  # Label edges by action\n",
    "                )\n",
    "\n",
    "# Use Kamada-Kawai layout for a more spatially balanced layout\n",
    "pos = nx.kamada_kawai_layout(G)  # Alternative layouts: spring_layout, shell_layout\n",
    "\n",
    "# Node colors based on clone_map (or any observation data)\n",
    "node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "# color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "# node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "# node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,  # Use the color scheme for nodes\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edge_color='gray',\n",
    "    arrowsize=20\n",
    ")\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "plt.title(\"Spatial Transition Graph\")\n",
    "plt.axis('off')  # Turn off axes for cleaner visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action from one state to other should be more deterministic?\n",
    "# draw this graph thresholded \n",
    "# like viterbi decoding, only draw the highest prob. action (1 or 0) -- binarize the TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.groups_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(4):\n",
    "    print(len(agent.groups_of_tables[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique elements and their frequencies\n",
    "np.unique(room, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example transition matrix (state, state, action)\n",
    "transition_matrix = agent.TM\n",
    "num_states = np.shape(agent.TM)[0]\n",
    "num_actions = np.shape(agent.TM)[2]\n",
    "# transition_matrix = np.random.rand(num_states, num_states, num_actions)\n",
    "\n",
    "# Threshold to show only significant transitions\n",
    "threshold = 0.1\n",
    "\n",
    "# Example observations for each state (one of four categories)\n",
    "# Replace with your actual observations\n",
    "# observations = [0, 1, 2, 3, 1]  # Each state is assigned to an observation\n",
    "# color_map = {0: 'red', 1: 'blue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "\n",
    "# Assign colors to states based on observations\n",
    "# node_colors = [color_map[obs] for obs in observations]\n",
    "node_colors = [t[0] for t in clone_map]\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each state\n",
    "for state in range(num_states):\n",
    "    G.add_node(state)\n",
    "\n",
    "# Add edges based on the transition matrix\n",
    "for action in range(num_actions):\n",
    "    for from_state in range(num_states):\n",
    "        for to_state in range(num_states):\n",
    "            weight = transition_matrix[from_state, to_state, action]\n",
    "            if weight > threshold:  # Filter out weak transitions\n",
    "                G.add_edge(\n",
    "                    from_state,\n",
    "                    to_state,\n",
    "                    weight=weight,\n",
    "                    label=f\"A{action}\"  # Label edges by action\n",
    "                )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # Position nodes using a spring layout\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')  # Get edge labels\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,  # Use the color scheme for nodes\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edge_color='gray',\n",
    "    arrowsize=20\n",
    ")\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "# Add a legend for the color scheme\n",
    "legend_labels = [f\"Observation {obs}: {color}\" for obs, color in color_map.items()]\n",
    "legend_text = \"\\n\".join(legend_labels)\n",
    "plt.gcf().text(0.9, 0.5, legend_text, fontsize=10, va='center')\n",
    "\n",
    "plt.title(\"Transition Graph with Observations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(agent.TM[:][0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ground-truth environment\n",
    "room = np.array(\n",
    "    [\n",
    "        [1, 2, 3, 0, 3, 1, 1, 1],\n",
    "        [1, 1, 3, 2, 3, 2, 3, 1],\n",
    "        [1, 1, 2, 0, 1, 2, 1, 0],\n",
    "        [0, 2, 1, 1, 3, 0, 0, 2],\n",
    "        [3, 3, 1, 0, 1, 0, 3, 0],\n",
    "        [2, 1, 2, 3, 3, 3, 2, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the color map for observations\n",
    "color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "node_colors = [color_map[val] for val in room.flatten()]  # Flatten room to assign colors to each cell\n",
    "\n",
    "\n",
    "# Create a graph\n",
    "rows, cols = room.shape\n",
    "GT = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        node_id = (r, c)\n",
    "        GT.add_node(node_id, observation=room[r, c])\n",
    "\n",
    "# Add edges for neighbors (up, down, left, right)\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        if r > 0:  # Connect to the cell above\n",
    "            GT.add_edge((r, c), (r-1, c))\n",
    "        if r < rows - 1:  # Connect to the cell below\n",
    "            GT.add_edge((r, c), (r+1, c))\n",
    "        if c > 0:  # Connect to the cell to the left\n",
    "            GT.add_edge((r, c), (r, c-1))\n",
    "        if c < cols - 1:  # Connect to the cell to the right\n",
    "            GT.add_edge((r, c), (r, c+1))\n",
    "\n",
    "# Position nodes in a grid layout\n",
    "pos = {(r, c): (c, -r) for r in range(rows) for c in range(cols)}  # Flip y-axis for grid alignment\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    GT,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    labels={(r, c): f\"{room[r, c]}\" for r in range(rows) for c in range(cols)},  # Show observation value\n",
    "    node_color=node_colors,  # Use colors based on observations\n",
    "    node_size=500,\n",
    "    font_size=8,\n",
    "    edge_color='gray'\n",
    ")\n",
    "\n",
    "# Add a legend\n",
    "legend_labels = [f\"Observation {obs}: {color}\" for obs, color in color_map.items()]\n",
    "legend_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10)\n",
    "                  for color in color_map.values()]\n",
    "plt.legend(legend_patches, legend_labels, loc='upper left', title=\"Observation Key\")\n",
    "\n",
    "plt.title(\"Graph Representation of Spatial Environment\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Before putting it to agent.forward_algorithm #############    \n",
    "#     prior = get_distribution(agent.groups_of_tables)\n",
    "#     print(prior)\n",
    "\n",
    "#     num_steps = len(observations)\n",
    "#     num_states = len(prior)\n",
    "# #     forward_probs = np.zeros((num_states, num_steps)) # n total clone, sequence length\n",
    "#     log_forward_probs = np.full((num_states, num_steps), 0)  # Use -inf for log(0)\n",
    "\n",
    "#     initial_state = observations[0]\n",
    "#     for s in range(num_states):\n",
    "#         if s == initial_state:\n",
    "# #             forward_probs[s, 0] = prior[s]\n",
    "#             log_forward_probs[s,0] = prior[s]\n",
    "#         else:\n",
    "# #             forward_probs[s, 0] = 0  # Probability is zero if it doesn't match initial state\n",
    "#             log_forward_probs[s,0] = 0\n",
    "\n",
    "\n",
    "#     for t in range(1, num_steps):\n",
    "#         current_clone = [clone[2] for clone in clone_map \n",
    "#                          if clone[0]==clone_history[t][0] and clone[1]==clone_history[t][1]]\n",
    "\n",
    "#         prev_action = actions[t-1]\n",
    "\n",
    "#         for s in range(num_states):\n",
    "#                 log_forward_probs[s, t] = np.logaddexp.reduce([\n",
    "#                     log_forward_probs[prev_s, t-1] + np.log(agent.TM[prev_s, s, prev_action])\n",
    "#                     for prev_s in range(num_states)\n",
    "#                 ])\n",
    "#     print(np.sum(log_forward_probs))\n",
    "############################################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.groups_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clone_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(forward_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_total_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "#     curr_\n",
    "    print(agent.get_restaurant_total_customers(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "[agent.get_restaurant_total_customers(i)/agent.get_total_observations() for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_prior_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4): \n",
    "    print(agent.count_tables_in_restaurant(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Problems to consider: \n",
    "1. TM size is +4 the number of clones\n",
    "2. sum of probs (PE) doesn't necessarily have to be less than 1 but seems so. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= agent.groups_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(data):\n",
    "    \"\"\"\n",
    "    Compute the probability distribution from a nested dictionary structure.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A nested dictionary where the outer keys map to inner dictionaries\n",
    "                     containing counts for different elements.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array containing the probability distribution.\n",
    "    \"\"\"\n",
    "    # Flatten the dictionary and compute total count\n",
    "    flattened_counts = defaultdict(int)\n",
    "    total_count = 0\n",
    "\n",
    "    for outer_key, inner_dict in data.items():\n",
    "        for inner_key, count in inner_dict.items():\n",
    "            flattened_counts[inner_key] += count\n",
    "            total_count += count\n",
    "\n",
    "    # Compute probabilities as a sorted NumPy array\n",
    "    sorted_keys = sorted(flattened_counts.keys())\n",
    "    probabilities = np.array([flattened_counts[key] / total_count for key in sorted_keys])\n",
    "\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(agent.TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### General intuition\n",
    "At first, many clones will be splitted out due to vanilla transition matrix\n",
    "\n",
    "But this will be corrected with merging\n",
    "\n",
    "With more experience and more accurate transition matrices, splitting and merging will diminish\n",
    "\n",
    "we can end whenever the likelihood plataeus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample([1,2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.groups_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEs = np.array([np.sum(T_tr[a[t-1], old_ind, state_loc[i]:state_loc[i+1]]) for i in range(len(state_loc)-1)])\n",
    "if len(state_loc) < 5: ####CHECK THIS PART LATER\n",
    "    PEs = np.append(PEs, np.sum(T_tr[a[t-1], old_ind, state_loc[-1]:]))\n",
    "PEs_softmax = softmax(PEs)\n",
    "PE = PEs_softmax[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.get_group_total(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.get_total_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class splitMerge: \n",
    "    def __init__(self, alpha=1.0, depth=3):\n",
    "    # splitting\n",
    "    self.alpha = alpha   # Dispersion parameter for CRP\n",
    "    self.depth = depth   # Maximum depth of the tree\n",
    "    self.tree = defaultdict(lambda: defaultdict(int))  # Tracks customer counts at each node\n",
    "    # merging\n",
    "#         def __init__(self, num_states, transition_probs, emission_probs, alpha=1.0):\n",
    "    self.num_states = num_states\n",
    "    self.transition_probs = transition_probs\n",
    "    self.emission_probs = emission_probs\n",
    "    self.alpha = alpha  # Dirichlet prior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cscg",
   "language": "python",
   "name": "cscg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
