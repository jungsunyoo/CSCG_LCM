{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import trange\n",
    "import copy\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import igraph\n",
    "from matplotlib import cm, colors\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rargmax(x):\n",
    "    # return x.argmax()  # <- favors clustering towards smaller state numbers\n",
    "    return np.random.choice((x == x.max()).nonzero()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen_structured_obs_room(\n",
    "    room,\n",
    "    start_r=None,\n",
    "    start_c=None,\n",
    "    no_left=[],\n",
    "    no_right=[],\n",
    "    no_up=[],\n",
    "    no_down=[],\n",
    "    length=10000,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"room is a 2d numpy array. inaccessible locations are marked by -1.\n",
    "    start_r, start_c: starting locations\n",
    "\n",
    "    In addition, there are invisible obstructions in the room\n",
    "    which disallows certain actions from certain states.\n",
    "\n",
    "    no_left:\n",
    "    no_right:\n",
    "    no_up:\n",
    "    no_down:\n",
    "\n",
    "    Each of the above are list of states from which the corresponding action is not allowed.\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    H, W = room.shape\n",
    "    if start_r is None or start_c is None:\n",
    "        start_r, start_c = np.random.randint(H), np.random.randint(W)\n",
    "\n",
    "    actions = np.zeros(length, int)\n",
    "    x = np.zeros(length, int)  # observations\n",
    "    rc = np.zeros((length, 2), int)  # actual r&c\n",
    "\n",
    "    r, c = start_r, start_c\n",
    "    x[0] = room[r, c]\n",
    "    rc[0] = r, c\n",
    "\n",
    "    count = 0\n",
    "    while count < length - 1:\n",
    "\n",
    "        act_list = [0, 1, 2, 3]  # 0: left, 1: right, 2: up, 3: down\n",
    "        if (r, c) in no_left:\n",
    "            act_list.remove(0)\n",
    "        if (r, c) in no_right:\n",
    "            act_list.remove(1)\n",
    "        if (r, c) in no_up:\n",
    "            act_list.remove(2)\n",
    "        if (r, c) in no_down:\n",
    "            act_list.remove(3)\n",
    "\n",
    "        a = np.random.choice(act_list)\n",
    "\n",
    "        # Check for actions taking out of the matrix boundary.\n",
    "        prev_r = r\n",
    "        prev_c = c\n",
    "        if a == 0 and 0 < c:\n",
    "            c -= 1\n",
    "        elif a == 1 and c < W - 1:\n",
    "            c += 1\n",
    "        elif a == 2 and 0 < r:\n",
    "            r -= 1\n",
    "        elif a == 3 and r < H - 1:\n",
    "            r += 1\n",
    "\n",
    "        # Check whether action is taking to inaccessible states.\n",
    "        temp_x = room[r, c]\n",
    "        if temp_x == -1:\n",
    "            r = prev_r\n",
    "            c = prev_c\n",
    "            pass\n",
    "\n",
    "        actions[count] = a\n",
    "        x[count + 1] = room[r, c]\n",
    "        rc[count + 1] = r, c\n",
    "        count += 1\n",
    "\n",
    "    return actions, x, rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "room = np.array(\n",
    "    [\n",
    "        [1, 2, 3, 0, 3, 1, 1, 1],\n",
    "        [1, 1, 3, 2, 3, 2, 3, 1],\n",
    "        [1, 1, 2, 0, 1, 2, 1, 0],\n",
    "        [0, 2, 1, 1, 3, 0, 0, 2],\n",
    "        [3, 3, 1, 0, 1, 0, 3, 0],\n",
    "        [2, 1, 2, 3, 3, 3, 2, 0],\n",
    "    ]\n",
    ")\n",
    "actions, observations, rc = datagen_structured_obs_room(room, length=50)     #Use length=50000 for bigger room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value\n",
    "unique, counts = np.unique(room, return_counts=True)\n",
    "# Create a dictionary of the counts\n",
    "result = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Counts of 0, 1, 2, 3:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "c = []\n",
    "c = np.concatenate((c,a))\n",
    "c.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @nb.njit\n",
    "class Agent:\n",
    "    def __init__(self, num_obs=4, num_actions=4, pseudocount=1e-3):\n",
    "        self.num_obs = num_obs\n",
    "        self.groups_of_tables = {}\n",
    "        self.table_totals = {}  # Keep track of totals for each table separately\n",
    "        self.total_observations = 0  # Keep track of total observations across all tables\n",
    "        self.count = np.ones((num_obs,num_obs,num_actions))\n",
    "        self.TM = self.count.copy()\n",
    "        self.C = np.zeros((num_obs,num_obs,num_actions))\n",
    "        self.normalize_TM()\n",
    "        self.initialize_clones(num_obs)\n",
    "        self.pseudocount = pseudocount\n",
    "\n",
    "\n",
    "    def initialize_clones(self,num_obs):\n",
    "        for restaurant_id in range(num_obs):\n",
    "            self.groups_of_tables[restaurant_id] = {}\n",
    "            self.groups_of_tables[restaurant_id][0] = 1\n",
    "    \n",
    "    def normalize_TM(self):\n",
    "        num_obs = np.shape(self.count)[0]\n",
    "        num_actions = np.shape(self.count)[2]\n",
    "        count = self.count.copy()\n",
    "        for s in range(num_obs):\n",
    "            for a in range(num_actions):\n",
    "                self.TM[s, :, a] = count[s,:,a] / count[s,:,a].sum()\n",
    "                \n",
    "                \n",
    "    def add_clone(self, restaurant_id, table_id):\n",
    "        \"\"\"Add exactly one clone to a specified table, creating the table or group if necessary.\"\"\"\n",
    "        # Automatically create the group and table if they don't exist\n",
    "        if restaurant_id not in self.groups_of_tables:\n",
    "            self.groups_of_tables[restaurant_id] = {}\n",
    "        if table_id not in self.groups_of_tables[restaurant_id]:\n",
    "            self.groups_of_tables[restaurant_id][table_id] = 0  # Initialize clones count for the table\n",
    "\n",
    "        # Add one clone to the table count and update total observations\n",
    "        self.groups_of_tables[restaurant_id][table_id] += 1\n",
    "        self.table_totals[(restaurant_id, table_id)] = self.groups_of_tables[restaurant_id][table_id]  # Update table total\n",
    "        self.total_observations += 1\n",
    "\n",
    "    def get_total_observations(self):\n",
    "        \"\"\"Return the total number of observations.\"\"\"\n",
    "        return self.total_observations\n",
    "\n",
    "    def get_restaurant_total_customers(self, restaurant_id):\n",
    "        \"\"\"Return the total number of clones in all tables within a specific restaurant.\"\"\"\n",
    "        return sum(self.groups_of_tables.get(restaurant_id, {}).values())\n",
    "\n",
    "    def get_table_total_customers(self, restaurant_id, table_id):\n",
    "        \"\"\"Return the total number of clones for a specific table.\"\"\"\n",
    "        return self.groups_of_tables.get(restaurant_id, {}).get(table_id, 0)\n",
    "    \n",
    "#     def get_prior_distribution(self):\n",
    "\n",
    "#         return [self.get_restaurant_total_customers(restaurant_id for restaurant_id in range(self.num_obs))\n",
    "#                 /self.get_total_observations()]\n",
    "\n",
    "    def count_tables_in_restaurant(self, restaurant_id):\n",
    "        \"\"\"Returns the number of tables within the specified restaurant.\"\"\"\n",
    "        if restaurant_id in self.groups_of_tables:\n",
    "            return len(self.groups_of_tables[restaurant_id])\n",
    "        else:\n",
    "            # print(f\"Group {group_id} does not exist.\")\n",
    "            return 0\n",
    "        \n",
    "# agent.update_count((total_clones, prev_action, obs_ind))\n",
    "    def expand_split(self):\n",
    "        # expand dimension\n",
    "        orig_count = self.count.copy()\n",
    "        orig_TM = self.TM.copy()\n",
    "        orig_C = self.C.copy()\n",
    "        n_prev_clones = np.shape(orig_count)[0]\n",
    "        n_actions = np.shape(orig_count)[2]\n",
    "        expanded_count = np.zeros((n_prev_clones+1, n_prev_clones+1, n_actions)) + self.pseudocount\n",
    "        expanded_TM = np.zeros((n_prev_clones+1, n_prev_clones+1, n_actions)) + self.pseudocount\n",
    "        expanded_C = np.zeros((n_prev_clones+1, n_prev_clones+1, n_actions)) + self.pseudocount\n",
    "                \n",
    "        # Copy the original matrix values into the top-left submatrix of the expanded matrix\n",
    "        expanded_count[:n_prev_clones, :n_prev_clones, :] = orig_count\n",
    "        expanded_TM[:n_prev_clones, :n_prev_clones, :] = orig_TM\n",
    "        expanded_C[:n_prev_clones, :n_prev_clones, :] = orig_C\n",
    "        \n",
    "        self.count = expanded_count\n",
    "        self.TM = expanded_TM\n",
    "        self.C = expanded_C\n",
    "        \n",
    "# #         self.count[state, state2]\n",
    "#         for ss in state2:\n",
    "#             self.count[state, ss, action] += 1\n",
    "            \n",
    "    def update_count(self, state, state2, action): # updating counts when not splitted\n",
    "#         for ss in state2: \n",
    "#         self.count[state, state2]\n",
    "#         for ss in state2:\n",
    "        self.count[state, state2, action] += 1\n",
    "\n",
    "        \n",
    "#         for next_state in state2: \n",
    "            \n",
    "#     def update_transition(self, state, state2, action): \n",
    "#         # update transition probability p(s'|s,a)\n",
    "#         self.TM[state, state2, action] += 1\n",
    "        \n",
    "#         # Normalize the probabilities for the current state and action\n",
    "#         self.TM[state, :, action] /= self.TM[state, :, action].sum()        \n",
    "    def merged_likelihood(self, clone_map):\n",
    "        \n",
    "        merged_TM = self.TM.copy()\n",
    "        \n",
    "        curr_obs, clone_num, unique_idx = random.choice(clone_map)\n",
    "        shared_obs_clones = [t for t in clone_map if t[0]==curr_obs and t[1] != clone_num] # all the clones that share the obs\n",
    "        \n",
    "        compare_clones = random.choice(shared_obs_clones) # pick a random clone that shares observation \n",
    "\n",
    "        state_a = unique_idx\n",
    "        state_b = compare_clones[2]\n",
    "        \n",
    "        for s in range(len(clone_map)):\n",
    "            if s != state_a and s != state_b:                \n",
    "                merged_TM[state_a][s] += merged_TM[state_b][s]\n",
    "                merged_TM[s][state_a] += merged_TM[s][state_b] \n",
    "                # Remove state_b by zeroing out probabilities (optional)\n",
    "                merged_TM[state_b][s] = 0\n",
    "                merged_TM[s][state_b] = 0\n",
    "                \n",
    "# agent.update_C(mess_fwd, mess_bwd, observations[:o+1], actions[:o+1], clone_map)\n",
    "    def update_C(self, mess_fwd, mess_bwd, observations, actions, clone_map, clone_history):\n",
    "        timesteps = len(observations)\n",
    "        reversed_mess_bwd = mess_bwd[::-1]\n",
    "        for t in range(timesteps-1):\n",
    "            aij, i, j = (\n",
    "                actions[t],\n",
    "                observations[t],\n",
    "                observations[t+1],\n",
    "            )  # at time t-1 -> t we go from observation i to observation j\n",
    "            fwd = mess_fwd[t]\n",
    "            bwd = reversed_mess_bwd[t+1]\n",
    "\n",
    "            current_clones = [clone[2] for clone in clone_map \n",
    "                             if clone[0]==clone_history[t+1][0]]\n",
    "            previous_clones = [clone[2] for clone in clone_map \n",
    "                             if clone[0]==clone_history[t][0]]\n",
    "#             print(\"current clones: {}\".format(current_clones))\n",
    "#             print(\"previous clones: {}\".format(previous_clones))\n",
    "            for c, curr in enumerate(current_clones):\n",
    "                for p, prev in enumerate(previous_clones):\n",
    "#                     print('\\n')\n",
    "#                     print(\"fwd: {}\".format(fwd))\n",
    "#                     print(\"bwd: {}\".format(bwd))\n",
    "#                     print(\"c: {}\".format(c))\n",
    "#                     print(\"p: {}\".format(p))\n",
    "#                     print(\"prev: {}\".format(prev))\n",
    "#                     print(\"curr: {}\".format(curr))\n",
    "                    \n",
    "                    q = (\n",
    "                    fwd[p] * self.TM[prev, curr, aij] * bwd[c])\n",
    "                    self.C[prev, curr, aij] += q\n",
    "#         prev_action = actions[t-1]\n",
    "#         prev_message = message\n",
    "#         message = np.zeros(len(current_clones))\n",
    "#         for c, curr in enumerate(current_clones): # loop over indices for currently activated clones\n",
    "#             message_ = 0\n",
    "#             for p, prev in enumerate(previous_clones): # loop over indices for previously activated clones           \n",
    "#                 if prev != curr: \n",
    "#                     message_ += (TM[prev, curr, prev_action] * prev_message[p])\n",
    "#             message[c] = message_\n",
    "#         p_obs = message.sum()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "def CRP(agent, curr_observation, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Simulates the Chinese Restaurant Process.\n",
    "\n",
    "    Parameters:\n",
    "    - history: int, the total number of customers to simulate.\n",
    "    - alpha: float, the concentration parameter.\n",
    "\n",
    "    Returns:\n",
    "    - A list where the i-th element represents the table number of the i-th customer.\n",
    "    \"\"\"\n",
    "\n",
    "    n = agent.get_restaurant_total_customers(curr_observation)\n",
    "\n",
    "\n",
    "    if curr_observation not in agent.groups_of_tables:\n",
    "        agent.add_clone(curr_observation,0)\n",
    "        table_choice = 0\n",
    "        assignments = 0\n",
    "        probs = 1\n",
    "    else:\n",
    "        \n",
    "        probs = [clone_count / (n + alpha) for table_id, clone_count in \n",
    "               agent.groups_of_tables[curr_observation].items()] + [alpha / (n + alpha)] # This is the prior\n",
    "#         print(probs)\n",
    "    \n",
    "#     probs = np.concatenate(probs)\n",
    "#     print(probs)\n",
    "    # Add an update rule (ref Nora's paper 1st equation)\n",
    "\n",
    "    # Choose a table based on the probabilities\n",
    "    table_choice = np.random.choice(len(probs), p=probs)\n",
    "\n",
    "\n",
    "    # update clone --> existing or new, same\n",
    "    agent.add_clone(curr_observation,table_choice)\n",
    "    assignments = table_choice\n",
    "\n",
    "    return assignments, probs\n",
    "\n",
    "\n",
    "def get_distribution(data):\n",
    "    \"\"\"\n",
    "    Compute the probability distribution from a nested dictionary structure.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A nested dictionary where the outer keys map to inner dictionaries\n",
    "                     containing counts for different elements.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array containing the probability distribution.\n",
    "    \"\"\"\n",
    "    # Flatten the dictionary and compute total count\n",
    "    flattened_counts = defaultdict(int)\n",
    "    total_count = 0\n",
    "    ct = 0\n",
    "    for outer_key, inner_dict in data.items():\n",
    "#         print(outer_key)\n",
    "        for inner_key, count in inner_dict.items():\n",
    "#             print(inner_key)\n",
    "            flattened_counts[ct] += count\n",
    "            total_count += count\n",
    "            ct += 1\n",
    "\n",
    "    # Compute probabilities as a sorted NumPy array\n",
    "    sorted_keys = sorted(flattened_counts.keys())\n",
    "    probabilities = np.array([flattened_counts[key] / total_count for key in sorted_keys])\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\n",
    "def forward_algorithm(agent, prior, observations, actions, clone_map, clone_history, TM, store_messages=False):\n",
    "    num_steps = len(observations)\n",
    "    num_states = np.shape(TM)[0]\n",
    "    log_forward_probs = []\n",
    "    log2_lik = np.zeros(num_steps)\n",
    "    previous_clones = []\n",
    "    initial_state = observations[0]\n",
    "    mess_fwd = []\n",
    "    for c, clone in enumerate(clone_map):\n",
    "        if clone[0] == initial_state: # if this observation\n",
    "            log_forward_probs.append(prior[c])\n",
    "            previous_clones.append(clone[2])\n",
    "    message = log_forward_probs\n",
    "    p_obs = np.sum(message)   \n",
    "    assert p_obs > 0\n",
    "    message /= p_obs\n",
    "    log2_lik[0] = np.log2(p_obs)\n",
    "    if store_messages: \n",
    "        mess_fwd.append(message)\n",
    "    for t in range(1, num_steps):        \n",
    "        # 2024-11-26: following the cscg-orig code, calculate the likelihood based on obs (not clone)\n",
    "        current_clones = [clone[2] for clone in clone_map \n",
    "                         if clone[0]==clone_history[t][0]]\n",
    "        prev_action = actions[t-1]\n",
    "        prev_message = message\n",
    "        message = np.zeros(len(current_clones))\n",
    "        for c, curr in enumerate(current_clones): # loop over indices for currently activated clones\n",
    "            message_ = 0\n",
    "            for p, prev in enumerate(previous_clones): # loop over indices for previously activated clones           \n",
    "                if prev != curr: \n",
    "                    message_ += (TM[prev, curr, prev_action] * prev_message[p])\n",
    "            message[c] = message_\n",
    "        p_obs = message.sum()\n",
    "        assert p_obs > 0\n",
    "        message /= p_obs\n",
    "        log2_lik[t] = np.log2(p_obs)        \n",
    "        if store_messages:\n",
    "            mess_fwd.append(message)        \n",
    "        previous_clones = current_clones\n",
    "    return log2_lik, mess_fwd\n",
    "\n",
    "def forward_mp(agent, prior, observations, actions, clone_map, clone_history, TM, store_messages=False):\n",
    "    num_steps = len(observations)\n",
    "    num_states = np.shape(TM)[0]\n",
    "    log_forward_probs = []\n",
    "    log2_lik = np.zeros(num_steps)\n",
    "    previous_clones = []\n",
    "    initial_state = observations[0]\n",
    "    mess_fwd = []\n",
    "    for c, clone in enumerate(clone_map):\n",
    "        if clone[0] == initial_state: # if this observation\n",
    "            log_forward_probs.append(prior[c])\n",
    "            previous_clones.append(clone[2])\n",
    "    message = log_forward_probs\n",
    "    p_obs = np.sum(message)   \n",
    "    assert p_obs > 0\n",
    "    message /= p_obs\n",
    "    log2_lik[0] = np.log2(p_obs)\n",
    "    if store_messages: \n",
    "        mess_fwd.append(message)\n",
    "    for t in range(1, num_steps):        \n",
    "        # 2024-11-26: following the cscg-orig code, calculate the likelihood based on obs (not clone)\n",
    "        current_clones = [clone[2] for clone in clone_map \n",
    "                         if clone[0]==clone_history[t][0]]\n",
    "        prev_action = actions[t-1]\n",
    "        prev_message = message\n",
    "#         message = np.zeros(len(current_clones))\n",
    "        new_message = np.zeros(len(current_clones))\n",
    "        for c, curr in enumerate(current_clones): # loop over indices for currently activated clones\n",
    "#             message_ = 0\n",
    "            new_message_ = []\n",
    "            for p, prev in enumerate(previous_clones): # loop over indices for previously activated clones           \n",
    "                if prev != curr: \n",
    "#                     message_ += (TM[prev, curr, prev_action] * prev_message[p])\n",
    "                    new_message_.append(TM[prev, curr, prev_action] * prev_message[p])\n",
    "#             message[c] = message_\n",
    "            new_message[c] = max(new_message_)\n",
    "        message = new_message\n",
    "        p_obs = message.max()\n",
    "        assert p_obs > 0\n",
    "        message /= p_obs\n",
    "        log2_lik[t] = np.log2(p_obs)        \n",
    "        if store_messages:\n",
    "            mess_fwd.append(message)        \n",
    "        previous_clones = current_clones\n",
    "    return log2_lik, mess_fwd\n",
    "\n",
    "def backward_algorithm(agent, observations, actions, clone_map, TM):\n",
    "    mess_bwd = []\n",
    "    t = observations.shape[0] - 1\n",
    "#     i = observations[t]\n",
    "    current_clones = [clone[2] for clone in clone_map \n",
    "                     if clone[0]==clone_history[t][0]]    \n",
    "    message = np.ones(len(current_clones)) / len(current_clones)\n",
    "    message /= message.sum()\n",
    "    mess_bwd.append(message)\n",
    "    for t in range(observations.shape[0]-2, -1, -1):\n",
    "        previous_clones = [clone[2] for clone in clone_map \n",
    "                 if clone[0]==clone_history[t][0]]\n",
    "        prev_action = actions[t]\n",
    "        prev_message = message\n",
    "        \n",
    "        message = np.zeros(len(previous_clones))\n",
    "            \n",
    "        for p, prev in enumerate(previous_clones):\n",
    "            message_ = 0\n",
    "            for c, curr in enumerate(current_clones):\n",
    "                if prev != curr: \n",
    "                    message_ += (TM[prev, curr, prev_action] * prev_message[c])\n",
    "            message[p] = message_\n",
    "        p_obs = message.sum()\n",
    "        assert p_obs > 0\n",
    "        message /= p_obs\n",
    "        mess_bwd.append(message)\n",
    "        \n",
    "        current_clones = previous_clones\n",
    "        \n",
    "    return mess_bwd\n",
    "\n",
    "# states = backtrace(agent, observations, actions, mess_fwd)\n",
    "def backtrace(agent, observations, actions, mess_fwd, clone_map):\n",
    "    \"\"\"Computes backward messages\"\"\"\n",
    "    code = np.zeros(observations.shape[0], dtype=np.int64)\n",
    "    TM = agent.TM.copy()\n",
    "    # backward pass\n",
    "    t = observations.shape[0] - 1\n",
    "    i = observations[t]\n",
    "#     t_start, t_stop = mess_loc[t : t + 2]\n",
    "#     rev_mess_fwd = mess_fwd[::-1]\n",
    "#     belief = rev_mess_fwd[0]\n",
    "    belief = mess_fwd[t]\n",
    "    curr_clone = rargmax(belief)\n",
    "    \n",
    "    \n",
    "# [clone[2] for clone in clone_map \n",
    "#                  if clone[0]==clone_history[t][0]]    \n",
    "    for clone in clone_map: \n",
    "        if clone[0] == i and clone[1] == curr_clone:\n",
    "            code[t] = clone[2]\n",
    "#     code[t] = [clone[2] for clone in clone_map if clone[0]==i and clone[1]==curr_clone]\n",
    "    \n",
    "#     code[t] = rargmax(belief) # this is which clone\n",
    "#     print(t)\n",
    "    for t in range(observations.shape[0] - 2, 1, -1):\n",
    "        print(t)\n",
    "        prev_action, prev_obs, curr_obs = (\n",
    "            actions[t-1],\n",
    "            observations[t-1],\n",
    "            observations[t],\n",
    "        )  # at time t -> t+1 we go from observation i to observation j\n",
    "#         (i_start, i_stop), j_start = state_loc[i : i + 2], state_loc[j]\n",
    "#         t_start, t_stop = mess_loc[t : t + 2]\n",
    "        previous_clones = [clone[2] for clone in clone_map \n",
    "                 if clone[0]==prev_obs]\n",
    "        current_clones = [clone[2] for clone in clone_map \n",
    "             if clone[0]==curr_obs]\n",
    "        belief = []\n",
    "\n",
    "#         print(prev_obs)\n",
    "        print(len(mess_fwd[t]))\n",
    "        print(len(current_clones))\n",
    "        print(len(previous_clones))\n",
    "#         print(observations[t])\n",
    "#         print(previous_clones)\n",
    "#         print(code[t+1])\n",
    "#         print('previous clones'.format(previous_clones))\n",
    "#         print('code'.format(code[t+1]))\n",
    "        for p, prev in enumerate(previous_clones):\n",
    "            belief.append(mess_fwd[t][p] * TM[prev, code[t+1], prev_action])\n",
    "#         belief = (\n",
    "#             mess_fwd[t] * T[aij, obser, code[t+1]]\n",
    "#         )\n",
    "        curr_clone = rargmax(np.array(belief))\n",
    "        for clone in clone_map: \n",
    "            if clone[0] == prev_obs and clone[1] == curr_clone:\n",
    "                code[t] = clone[2]\n",
    "#     states = state_loc[x] + code\n",
    "    return code  \n",
    "\n",
    "def decode(agent, observations, actions, clone_map, clone_history):\n",
    "    \"\"\"Compute the MAP assignment of latent variables using max-product message passing.\"\"\"\n",
    "    prior = get_distribution(agent.groups_of_tables)    \n",
    "    TM = agent.TM.copy()\n",
    "    log2_lik, mess_fwd = forward_mp(agent, prior, observations, actions, \n",
    "                                    clone_map, clone_history, TM, store_messages=True)\n",
    "    states = backtrace(agent, observations, actions, mess_fwd, clone_map)\n",
    "    \n",
    "#     log2_lik, mess_fwd = forward_mp(\n",
    "#         self.T.transpose(0, 2, 1),\n",
    "#         self.Pi_x,\n",
    "#         self.n_clones,\n",
    "#         x,\n",
    "#         a,\n",
    "#         store_messages=True,\n",
    "#     )\n",
    "#     states = backtrace(self.T, self.n_clones, x, a, mess_fwd)\n",
    "    return -log2_lik, states\n",
    "\n",
    "def clean_clone_map(data, delete_items):\n",
    "\n",
    "\n",
    "    updated_data=[]\n",
    "    for ids in delete_items: \n",
    "        for x,y,uid in data: \n",
    "            if uid < ids[2]:\n",
    "                updated_data.append((x,y,uid))\n",
    "            elif uid > ids[2]: \n",
    "                if x != ids[0]: \n",
    "                    updated_data.append((x,y,uid-1))\n",
    "                else:\n",
    "                    updated_data.append((x,y-1,uid-1))\n",
    "\n",
    "    return updated_data\n",
    "\n",
    "def binarizing_edges(agent):\n",
    "    TM = agent.TM.copy()\n",
    "    n_states = np.shape(TM)[0]\n",
    "    n_actions = np.shape(TM)[2]\n",
    "    for s1 in range(n_states):\n",
    "    #         for s2 in range(n_states):\n",
    "        TM_t = np.transpose(TM[s1])\n",
    "        for action in range(n_actions):\n",
    "            currTM = TM_t[action]\n",
    "\n",
    "            for e,element in enumerate(currTM): \n",
    "                if element == np.max(currTM):\n",
    "                    TM[s1][e][action]=1\n",
    "                else:\n",
    "                    TM[s1][e][action]=0\n",
    "\n",
    "    return TM\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: A scalar, vector, or matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - Sigmoid of x.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def normalize_TM_count(count):\n",
    "    num_obs = np.shape(count)[0]\n",
    "    num_actions = np.shape(count)[2]\n",
    "    new_count = count.copy()\n",
    "    for s in range(num_obs):\n",
    "        for a in range(num_actions):\n",
    "            new_count[s, :, a] = count[s,:,a] / count[s,:,a].sum()\n",
    "    return new_count\n",
    "\n",
    "\n",
    "\n",
    "def draw_graph(agent, clone_map, niter=0, threshold=0.2, binary_edges=True):\n",
    "    # Example transition matrix (state, state, action)\n",
    "    # transition_matrix = agent.TM\n",
    "    if binary_edges:\n",
    "        TM=binarizing_edges(agent)\n",
    "    else:         \n",
    "        TM = agent.TM\n",
    "    num_states = np.shape(TM)[0]\n",
    "    num_actions = np.shape(TM)[2]\n",
    "\n",
    "    # Threshold to show only significant transitions\n",
    "#     threshold = 0.2\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes for each state\n",
    "    for state in range(num_states):\n",
    "        G.add_node(state)\n",
    "\n",
    "    # Add edges based on the transition matrix\n",
    "    for action in range(num_actions):\n",
    "        for from_state in range(num_states):\n",
    "            for to_state in range(num_states):\n",
    "                weight = TM[from_state, to_state, action]\n",
    "                if weight > threshold:  # Filter out weak transitions\n",
    "                    G.add_edge(\n",
    "                        from_state,\n",
    "                        to_state,\n",
    "                        weight=weight,\n",
    "                        label=f\"A{action}\"  # Label edges by action\n",
    "                    )\n",
    "\n",
    "    # Use Kamada-Kawai layout for a more spatially balanced layout\n",
    "    pos = nx.kamada_kawai_layout(G)  # Alternative layouts: spring_layout, shell_layout\n",
    "\n",
    "    # Node colors based on clone_map (or any observation data)\n",
    "    node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "    color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "    node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(\n",
    "        G,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_color=node_colors,  # Use the color scheme for nodes\n",
    "        node_size=1000,\n",
    "        font_size=10,\n",
    "        edge_color='gray',\n",
    "        arrowsize=20\n",
    "    )\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        G,\n",
    "        pos,\n",
    "        edge_labels=edge_labels,\n",
    "        font_size=8\n",
    "    )\n",
    "\n",
    "    plt.title(\"Iteration {}\".format(niter))\n",
    "    plt.axis('off')  # Turn off axes for cleaner visualization\n",
    "    plt.show()\n",
    "    # \n",
    "    \n",
    "def draw_graph_TM(agent, clone_map, TM, niter=0, threshold=0.2, binary_edges=True):\n",
    "    # Example transition matrix (state, state, action)\n",
    "    # transition_matrix = agent.TM\n",
    "    # if binary_edges:\n",
    "    #     TM=binarizing_edges(agent)\n",
    "    # else:         \n",
    "    #     TM = agent.TM\n",
    "    num_states = np.shape(TM)[0]\n",
    "    num_actions = np.shape(TM)[2]\n",
    "\n",
    "    # Threshold to show only significant transitions\n",
    "#     threshold = 0.2\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes for each state\n",
    "    for state in range(num_states):\n",
    "        G.add_node(state)\n",
    "\n",
    "    # Add edges based on the transition matrix\n",
    "    for action in range(num_actions):\n",
    "        for from_state in range(num_states):\n",
    "            for to_state in range(num_states):\n",
    "                weight = TM[from_state, to_state, action]\n",
    "                if weight > threshold:  # Filter out weak transitions\n",
    "                    G.add_edge(\n",
    "                        from_state,\n",
    "                        to_state,\n",
    "                        weight=weight,\n",
    "                        label=f\"A{action}\"  # Label edges by action\n",
    "                    )\n",
    "\n",
    "    # Use Kamada-Kawai layout for a more spatially balanced layout\n",
    "    pos = nx.kamada_kawai_layout(G)  # Alternative layouts: spring_layout, shell_layout\n",
    "\n",
    "    # Node colors based on clone_map (or any observation data)\n",
    "    node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "    color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "    node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(\n",
    "        G,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_color=node_colors,  # Use the color scheme for nodes\n",
    "        node_size=1000,\n",
    "        font_size=10,\n",
    "        edge_color='gray',\n",
    "        arrowsize=20\n",
    "    )\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        G,\n",
    "        pos,\n",
    "        edge_labels=edge_labels,\n",
    "        font_size=8\n",
    "    )\n",
    "\n",
    "    plt.title(\"Iteration {}\".format(niter))\n",
    "    plt.axis('off')  # Turn off axes for cleaner visualization\n",
    "    plt.show()\n",
    "# def draw_graph_cscg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(agent, observations, actions, split_lik, clone_map, clone_history, n_particles=10, ): \n",
    "    log_lik = split_lik.copy()\n",
    "    for particle in range(n_particles): # many hypotheses // can this be parallelized?\n",
    "\n",
    "        curr_obs, clone_num, unique_idx = random.choice(clone_map[n_obs:])\n",
    "        shared_obs_clones = [t for t in clone_map[n_obs:] if t[0]==curr_obs and t[1] != clone_num] # all the clones that share the obs\n",
    "    #     prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "    #     _, clone_id = clone_list[idx]\n",
    "        # do the merging by iterating over other clones\n",
    "        # in the sequence, find other clones that could be merged with the current\n",
    "\n",
    "        compare_clones = random.choice(shared_obs_clones) # pick a random clone that shares observation \n",
    "        #### make this selection anti-proportional to number of assigned observations???\n",
    "\n",
    "        state_a = unique_idx\n",
    "        state_b = compare_clones[2]\n",
    "#         merged_TM = agent.TM.copy()\n",
    "        merged_count = agent.count.copy()\n",
    "\n",
    "        # adjusting prior distribution\n",
    "        merged_prior = get_distribution(agent.groups_of_tables)\n",
    "        merged_prior[state_a] += merged_prior[state_b]\n",
    "        merged_prior[state_b] = 0 #pseudocount    \n",
    "        # adjusting TM\n",
    "        for s in range(np.shape(agent.TM)[0]):\n",
    "            if s != state_a and s != state_b:\n",
    "                for a in range(len(np.unique(actions))):\n",
    "#                     merged_TM[state_a][s][a] += merged_TM[state_b][s][a]\n",
    "#                     merged_TM[s][state_a][a] += merged_TM[s][state_b][a]\n",
    "#                     merged_TM[s][state_b][a] = pseudocount\n",
    "#                     merged_TM[state_b][s][a] = pseudocount\n",
    "                    merged_count[state_a][s][a] += merged_count[state_b][s][a]\n",
    "                    merged_count[s][state_a][a] += merged_count[s][state_b][a]\n",
    "                    merged_count[s][state_b][a] = pseudocount\n",
    "                    merged_count[state_b][s][a] = pseudocount\n",
    "        merged_TM = normalize_TM_count(merged_count)\n",
    "        # Remove state_b by zeroing out probabilities (optional)\n",
    "    #     agent.TM[state_b] = np.zeros(self.num_states)    \n",
    "\n",
    "        # Get the new prior\n",
    "    #     new_prior = # GET THE NEW DISTRIBUTION\n",
    "\n",
    "        # Get the new likelihood by forward algorithm\n",
    "    #     new_lik = new_prior * new_Tm\n",
    "\n",
    "        new_lik_, mess_fwd = forward_algorithm(agent, merged_prior, observations, actions, clone_map, clone_history, merged_TM)\n",
    "#         print(len(mess_fwd))\n",
    "        #     print(log_lik, new_lik)\n",
    "        new_lik = new_lik_.mean()\n",
    "        if new_lik > log_lik: # if merging is better: \n",
    "            total_lls.append(new_lik)\n",
    "            total_indices.append(1)\n",
    "            agent.TM = merged_TM\n",
    "            agent.groups_of_tables[curr_obs][clone_num] += agent.groups_of_tables[curr_obs][compare_clones[1]]\n",
    "            agent.groups_of_tables[curr_obs][compare_clones[1]] = 0\n",
    "            print(\"Particle {}:\".format(particle))\n",
    "            print(\"Merged Obs {} clone {} into clone {}, log lik: {}\".format(curr_obs, compare_clones[1], clone_num, new_lik))\n",
    "#             pbar.set_postfix(train_bps=new_lik)\n",
    "\n",
    "            # clean up clones (including values that are 0)\n",
    "#             print(agent.groups_of_tables[curr_obs])\n",
    "            orig_table = agent.groups_of_tables[curr_obs].copy()\n",
    "#             print(orig_table)\n",
    "            orig_table[compare_clones[1]] = 0\n",
    "            delete_indices = [o for o,i in enumerate(orig_table) if orig_table[i]==0]\n",
    "            # Get unique IDs for the delete_indices from clone_map\n",
    "            delete_items = [\n",
    "                t for t in clone_map\n",
    "                for d in delete_indices\n",
    "                if t[0] == curr_obs and t[1] == d\n",
    "            ]\n",
    "#                     delete_uniqueids = [\n",
    "#                 t[2] for t in clone_map\n",
    "#                 for d in delete_indices\n",
    "#                 if t[0] == curr_obs and t[1] == d\n",
    "#             ]\n",
    "            new_table = {}\n",
    "            ni = 0\n",
    "            for i in range(len(orig_table)):\n",
    "#                 print(orig_table[i])\n",
    "                if orig_table[i] != 0: \n",
    "                    new_table[ni] = orig_table[i]\n",
    "                    ni+=1\n",
    "            agent.groups_of_tables[curr_obs] = new_table\n",
    "            \n",
    "#             print(agent.groups_of_tables[curr_obs])\n",
    "\n",
    "            # clean up TM\n",
    "#             for d in delete_items:\n",
    "# #                 if d >= np.shape(agent.TM)[0]: # MAYBE \n",
    "#                 agent.TM = np.delete(np.delete(agent.TM, d[2], axis=0), d[2], axis=1)\n",
    "\n",
    "            # clean up COUNT\n",
    "            for d in delete_items: \n",
    "                agent.count = np.delete(np.delete(agent.count, d[2], axis=0), d[2], axis=1)\n",
    "                agent.TM = np.delete(np.delete(agent.TM, d[2], axis=0), d[2], axis=1)\n",
    "                agent.C = np.delete(np.delete(agent.C, d[2], axis=0), d[2], axis=1)\n",
    "            agent.normalize_TM()\n",
    "\n",
    "            # clean up clone_map\n",
    "#             print(clone_map)\n",
    "#             for ids in delete_uniqueids: \n",
    "#                 print(ids)\n",
    "#                 def clean_clone_map(data, delete_obs, delete_clone, delete_unique_id):\n",
    "            clone_map = clean_clone_map(clone_map, delete_items)\n",
    "#             print(clone_map)\n",
    "            print(np.shape(agent.TM))\n",
    "\n",
    "\n",
    "            log_lik = new_lik\n",
    "    merge_lik = log_lik\n",
    "    return agent, merge_lik\n",
    "\n",
    "\n",
    "\n",
    "def learn_viterbi(agent, observations, actions, clone_map, clone_history,TM,n_iter=100):\n",
    "    sys.stdout.flush()\n",
    "    convergence = []\n",
    "    pbar = trange(n_iter, position=0)\n",
    "    log2_lik_old = -np.inf\n",
    "    prior = get_distribution(agent.groups_of_tables)\n",
    "    for it in pbar: \n",
    "        log2_lik, mess_fwd = forward_mp(agent, prior, observations, actions, clone_map, clone_history, TM, store_messages=True)\n",
    "        states = backtrace(agent, observations, actions, mess_fwd, clone_map)\n",
    "        agent.C[:] = 0\n",
    "        for t in range(1, len(observations)):\n",
    "            prev_action, prev_state, curr_state = (\n",
    "                actions[t - 1],\n",
    "                states[t - 1],\n",
    "                states[t],\n",
    "            )  # at time t-1 -> t we go from observation i to observation j\n",
    "            agent.C[prev_state,curr_state,prev_action] += 1.0\n",
    "        agent.normalize_TM()\n",
    "#   def backtrace(agent, observations, actions, mess_fwd, clone_map):  \n",
    "\n",
    "        convergence.append(-log2_lik.mean())\n",
    "        pbar.set_postfix(train_bps=convergence[-1])\n",
    "        if log2_lik.mean() <= log2_lik_old:\n",
    "            break\n",
    "        log2_lik_old = log2_lik.mean()\n",
    "\n",
    "def plot_graph(\n",
    "    agent, observations, actions, \n",
    "    clone_map, clone_history, output_file, cmap=cm.Spectral, multiple_episodes=False, vertex_size=30\n",
    "):\n",
    "    # pdb.set_trace()\n",
    "#     states = chmm.decode(x, a)[1]\n",
    "    _,states = decode(agent, observations, actions, clone_map, clone_history)\n",
    "\n",
    "    v = np.unique(states)\n",
    "#     print(v)\n",
    "    if multiple_episodes:\n",
    "        T = agent.C[:, v][:, :, v][:-1, 1:, 1:]\n",
    "        v = v[1:]\n",
    "    else:\n",
    "        T = agent.C[v, :][:, v, :]\n",
    "#     print(np.shape(T))\n",
    "    A = T.sum(2)\n",
    "#     print(A)\n",
    "    A /= A.sum(1, keepdims=True)\n",
    "\n",
    "    g = igraph.Graph.Adjacency((A > 0).tolist())\n",
    "    node_labels = [clone[2] for clone in clone_map]\n",
    "    node_labels = np.array(node_labels)\n",
    "    \n",
    "    # Node colors based on clone_map (or any observation data)\n",
    "    node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "    color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "    node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell    \n",
    "#     node_labels = np.arange(x.max() + 1).repeat(chmm.n_clones)[v]\n",
    "    if multiple_episodes:\n",
    "        node_labels -= 1\n",
    "#     colors = [cmap(nl)[:3] for nl in node_labels / node_labels.max()]\n",
    "    # out=[]\n",
    "    out = igraph.plot(\n",
    "        g,\n",
    "        output_file,\n",
    "        layout=g.layout(\"kamada_kawai\"),\n",
    "        vertex_color=node_colors,\n",
    "        vertex_label=v,\n",
    "        vertex_size=vertex_size,\n",
    "        margin=50,\n",
    "    )\n",
    "\n",
    "    return out, v, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Batch updating (recommended to begin with this)\n",
    "\n",
    "\n",
    "# room = np.array(\n",
    "#     [\n",
    "#         [1, 2, 3, 0, 3, 1, 1, 1],\n",
    "#         [1, 1, 3, 2, 3, 2, 3, 1],\n",
    "#         [1, 1, 2, 0, 1, 2, 1, 0],\n",
    "#         [0, 2, 1, 1, 3, 0, 0, 2],\n",
    "#         [3, 3, 1, 0, 1, 0, 3, 0],\n",
    "#         [2, 1, 2, 3, 3, 3, 2, 0],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "room = np.array(\n",
    "    [\n",
    "        [1, 1, 1, 1, 3, 3, 3, 3],\n",
    "        [1, 1, 1, 1, 3, 3, 3, 3],\n",
    "        [1, 1, 1, 1, 3, 3, 3, 3],\n",
    "        [2, 2, 2, 2, 0, 0, 0, 0],\n",
    "        [2, 2, 2, 2, 0, 0, 0, 0],\n",
    "        [2, 2, 2, 2, 0, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "actions, observations, rc = datagen_structured_obs_room(room, length=10000)     #Use length=50000 for bigger room\n",
    "unique_obs = np.unique(observations)\n",
    "pseudocount=1e-3\n",
    "# np.random.seed(42)\n",
    "agent = Agent(pseudocount=pseudocount)\n",
    "alpha = 1\n",
    "n_iter = 1 #100\n",
    "n_particles = 10\n",
    "n_actions = len(np.unique(actions))\n",
    "n_obs = len(np.unique(observations))\n",
    "scaling_factor = 3\n",
    "pes = []\n",
    "merge_lik = -np.inf\n",
    "total_lls = []\n",
    "total_indices = []\n",
    "# indices = 0\n",
    "\n",
    "for i in range(n_iter): \n",
    "    \n",
    "    # 1. SPLITTING\n",
    "    print('Splitting: iteration {}'.format(i))\n",
    "    split_improved = False\n",
    "#     agent_ = agent.copy()\n",
    "#     agent_ = copy.deepcopy(agent)\n",
    "    clone_history = []\n",
    "    if i==0: # first iteration        \n",
    "        clone_map= []   \n",
    "        # initialize clones for each obs\n",
    "#         clone_map.append((0,0,0))\n",
    "#         clone_map.append((1,0,1))\n",
    "#         clone_map.append((2,0,2))\n",
    "#         clone_map.append((3,0,3))\n",
    "        for uo in unique_obs:\n",
    "            clone_map.append((uo,0,uo))\n",
    "    \n",
    "#         unique_clone_id =3\n",
    "        unique_clone_id = len(unique_obs)-1\n",
    "    else: \n",
    "        unique_clone_id = len(clone_map)-1\n",
    "    lik_orig = 0\n",
    "    for o, obs in enumerate(observations): \n",
    "        if o < 2:\n",
    "            clone_history.append((obs,0, obs))\n",
    "            # update transition\n",
    "            if o ==1:\n",
    "                agent.update_count(observations[o-1], observations[o], actions[o]) # update transition between t-1 and t-2\n",
    "                agent.normalize_TM()                        \n",
    "            continue # skip 1st observation\n",
    "        splitted = False\n",
    "        \n",
    "        prev_prev_action = actions[o-2]\n",
    "        prev_action = actions[o-1]\n",
    "        prev_obs = observations[o-1]\n",
    "        prev_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "        # get the PE using the transition matrix\n",
    "        \n",
    "        prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "        # Get all state, actions that lead to current obs:\n",
    "        obs_ind = [t[2] for t in clone_map if t[0]==obs] # this is list of curr obs clone id\n",
    "        PE = 0\n",
    "        for oi in obs_ind: # all possible clones related to current observation\n",
    "            for pi in prev_ind: # all possible clones related to t-1 observation\n",
    "            \n",
    "                PE += agent.TM[pi, oi, prev_action] # just sum up PEs for now\n",
    "#         PE = \n",
    "        pes.append(scaling_factor/PE)\n",
    "        assignment, _ =  CRP(agent, prev_obs, alpha=scaling_factor/PE)\n",
    "        \n",
    "        post_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "        if prev_tables != post_tables: # a new clone has been created for this observation\n",
    "\n",
    "            splitted = True\n",
    "            unique_clone_id += 1\n",
    "            clone_map.append((prev_obs, assignment, unique_clone_id))\n",
    "            agent.expand_split()\n",
    "            prev_clone = unique_clone_id # clone id of t-1 : unique\n",
    "            \n",
    "        else:\n",
    "\n",
    "            prev_clone = [t[2] for t in clone_map if t[0]==prev_obs and t[1]==assignment] # clone id of t-1 revealed\n",
    "        prev_prev_obs, prev_prev_assignment, prev_prev_clone = clone_history[-1]\n",
    "#         if i > 0: \n",
    "#             print(clone_history)\n",
    "\n",
    "\n",
    "        clone_history.append((prev_obs, assignment, prev_clone)) # this keeps track of observation & clone history\n",
    "    \n",
    "        ################### CHANGING THE FW BW ONLINE\n",
    "#         if o > 10: \n",
    "#             TM = agent.TM.copy()\n",
    "#             prior = get_distribution(agent.groups_of_tables)\n",
    "\n",
    "#             _, mess_fwd = forward_algorithm(agent, prior, observations[:o+1], actions[:o+1], clone_map, \n",
    "#                                             clone_history, TM, store_messages=True)\n",
    "#             print(mess_fwd)\n",
    "#             mess_bwd = backward_algorithm(agent, observations[:o+1], actions[:o+1], clone_map, TM)\n",
    "#             print(mess_bwd)\n",
    "\n",
    "#             agent.update_C(mess_fwd, mess_bwd, observations[:o+1], actions[:o+1], clone_map, clone_history)\n",
    "        \n",
    "        \n",
    "    \n",
    "        agent.update_count(prev_prev_clone, prev_clone, prev_prev_action) # update transition between t-1 and t-2\n",
    "        agent.normalize_TM()\n",
    "        ###################\n",
    "\n",
    "    \n",
    "\n",
    "    # 2. FORWARD ALGORITHM TO GET LIKELIHOOD\n",
    "    TM = agent.TM.copy()\n",
    "    prior = get_distribution(agent.groups_of_tables)\n",
    "    split_lik_, mess_fwd = forward_algorithm(agent, prior, observations, actions, clone_map, clone_history, TM, store_messages=True)\n",
    "    mess_bwd = backward_algorithm(agent, observations, actions, clone_map, TM)\n",
    "    rev_mess_bwd = mess_bwd[::-1]\n",
    "    forward_size = [len(f) for f in mess_fwd]\n",
    "    backward_size = [len(b) for b in rev_mess_bwd]\n",
    "#     print(forward_size)\n",
    "#     print(backward_size)\n",
    "    agent.update_C(mess_fwd, mess_bwd, observations, actions, clone_map, clone_history)\n",
    "    agent.normalize_TM()\n",
    "    \n",
    "    \n",
    "#     print(mess_bwd)\n",
    "    #     print(len(mess_fwd))\n",
    "    split_lik = split_lik_.mean()\n",
    "\n",
    "    print('Log lik for split iteration {}: {}'.format(i, split_lik))\n",
    "    print('TM size: {}'.format(np.shape(agent.TM)))\n",
    "    \n",
    "    total_lls.append(split_lik)\n",
    "    total_indices.append(0)\n",
    "    \n",
    "    \n",
    "#     draw_graph(agent, clone_map, niter=i)\n",
    "#     output_file = 'sample.png'\n",
    "#     graph, v, g = plot_graph(agent, observations, actions, \n",
    "#         clone_map, clone_history, output_file, cmap=cm.Spectral, multiple_episodes=False, vertex_size=30)\n",
    "#     graph\n",
    "# learn_viterbi(agent, observations, actions, clone_map, clone_history,TM,n_iter=100)\n",
    "    name = 'split' + ' ' + str(i)\n",
    "    draw_graph(agent, clone_map, niter=name)\n",
    "#     if split_lik > merge_lik:\n",
    "# #         agent = agent_\n",
    "#         agent = copy.deepcopy(agent_)\n",
    "        \n",
    "#         print('split improved')\n",
    "#         split_improved = True\n",
    "\n",
    "    \n",
    "            \n",
    "    # 3. MERGING\n",
    "#     pbar = trange(n_particles, position=0)\n",
    "# if split_improved: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     log_lik = split_lik.copy()\n",
    "#     for particle in range(n_particles): # many hypotheses // can this be parallelized?\n",
    "\n",
    "#         curr_obs, clone_num, unique_idx = random.choice(clone_map[n_obs:])\n",
    "#         shared_obs_clones = [t for t in clone_map[n_obs:] if t[0]==curr_obs and t[1] != clone_num] # all the clones that share the obs\n",
    "#     #     prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "#     #     _, clone_id = clone_list[idx]\n",
    "#         # do the merging by iterating over other clones\n",
    "#         # in the sequence, find other clones that could be merged with the current\n",
    "\n",
    "#         compare_clones = random.choice(shared_obs_clones) # pick a random clone that shares observation \n",
    "#         #### make this selection anti-proportional to number of assigned observations???\n",
    "\n",
    "#         state_a = unique_idx\n",
    "#         state_b = compare_clones[2]\n",
    "# #         merged_TM = agent.TM.copy()\n",
    "#         merged_count = agent.count.copy()\n",
    "\n",
    "#         # adjusting prior distribution\n",
    "#         merged_prior = get_distribution(agent.groups_of_tables)\n",
    "#         merged_prior[state_a] += merged_prior[state_b]\n",
    "#         merged_prior[state_b] = 0 #pseudocount    \n",
    "#         # adjusting TM\n",
    "#         for s in range(np.shape(agent.TM)[0]):\n",
    "#             if s != state_a and s != state_b:\n",
    "#                 for a in range(len(np.unique(actions))):\n",
    "# #                     merged_TM[state_a][s][a] += merged_TM[state_b][s][a]\n",
    "# #                     merged_TM[s][state_a][a] += merged_TM[s][state_b][a]\n",
    "# #                     merged_TM[s][state_b][a] = pseudocount\n",
    "# #                     merged_TM[state_b][s][a] = pseudocount\n",
    "#                     merged_count[state_a][s][a] += merged_count[state_b][s][a]\n",
    "#                     merged_count[s][state_a][a] += merged_count[s][state_b][a]\n",
    "#                     merged_count[s][state_b][a] = pseudocount\n",
    "#                     merged_count[state_b][s][a] = pseudocount\n",
    "#         merged_TM = normalize_TM_count(merged_count)\n",
    "#         # Remove state_b by zeroing out probabilities (optional)\n",
    "#     #     agent.TM[state_b] = np.zeros(self.num_states)    \n",
    "\n",
    "#         # Get the new prior\n",
    "#     #     new_prior = # GET THE NEW DISTRIBUTION\n",
    "\n",
    "#         # Get the new likelihood by forward algorithm\n",
    "#     #     new_lik = new_prior * new_Tm\n",
    "\n",
    "#         new_lik_, mess_fwd = forward_algorithm(agent, merged_prior, observations, actions, clone_map, clone_history, merged_TM)\n",
    "# #         print(len(mess_fwd))\n",
    "#         #     print(log_lik, new_lik)\n",
    "#         new_lik = new_lik_.mean()\n",
    "#         if new_lik > log_lik: # if merging is better: \n",
    "#             total_lls.append(new_lik)\n",
    "#             total_indices.append(1)\n",
    "#             agent.TM = merged_TM\n",
    "#             agent.groups_of_tables[curr_obs][clone_num] += agent.groups_of_tables[curr_obs][compare_clones[1]]\n",
    "#             agent.groups_of_tables[curr_obs][compare_clones[1]] = 0\n",
    "#             print(\"Particle {}:\".format(particle))\n",
    "#             print(\"Merged Obs {} clone {} into clone {}, log lik: {}\".format(curr_obs, compare_clones[1], clone_num, new_lik))\n",
    "# #             pbar.set_postfix(train_bps=new_lik)\n",
    "\n",
    "#             # clean up clones (including values that are 0)\n",
    "# #             print(agent.groups_of_tables[curr_obs])\n",
    "#             orig_table = agent.groups_of_tables[curr_obs].copy()\n",
    "# #             print(orig_table)\n",
    "#             orig_table[compare_clones[1]] = 0\n",
    "#             delete_indices = [o for o,i in enumerate(orig_table) if orig_table[i]==0]\n",
    "#             # Get unique IDs for the delete_indices from clone_map\n",
    "#             delete_items = [\n",
    "#                 t for t in clone_map\n",
    "#                 for d in delete_indices\n",
    "#                 if t[0] == curr_obs and t[1] == d\n",
    "#             ]\n",
    "# #                     delete_uniqueids = [\n",
    "# #                 t[2] for t in clone_map\n",
    "# #                 for d in delete_indices\n",
    "# #                 if t[0] == curr_obs and t[1] == d\n",
    "# #             ]\n",
    "#             new_table = {}\n",
    "#             ni = 0\n",
    "#             for ii in range(len(orig_table)):\n",
    "# #                 print(orig_table[i])\n",
    "#                 if orig_table[ii] != 0: \n",
    "#                     new_table[ni] = orig_table[ii]\n",
    "#                     ni+=1\n",
    "#             agent.groups_of_tables[curr_obs] = new_table\n",
    "            \n",
    "# #             print(agent.groups_of_tables[curr_obs])\n",
    "\n",
    "#             # clean up TM\n",
    "# #             for d in delete_items:\n",
    "# # #                 if d >= np.shape(agent.TM)[0]: # MAYBE \n",
    "# #                 agent.TM = np.delete(np.delete(agent.TM, d[2], axis=0), d[2], axis=1)\n",
    "\n",
    "#             # clean up COUNT\n",
    "#             for d in delete_items: \n",
    "#                 agent.count = np.delete(np.delete(agent.count, d[2], axis=0), d[2], axis=1)\n",
    "#                 agent.TM = np.delete(np.delete(agent.TM, d[2], axis=0), d[2], axis=1)\n",
    "#                 agent.C = np.delete(np.delete(agent.C, d[2], axis=0), d[2], axis=1)\n",
    "#             agent.normalize_TM()\n",
    "\n",
    "#             # clean up clone_map\n",
    "# #             print(clone_map)\n",
    "# #             for ids in delete_uniqueids: \n",
    "# #                 print(ids)\n",
    "# #                 def clean_clone_map(data, delete_obs, delete_clone, delete_unique_id):\n",
    "#             clone_map = clean_clone_map(clone_map, delete_items)\n",
    "# #             print(clone_map)\n",
    "#             print(np.shape(agent.TM))\n",
    "\n",
    "\n",
    "#             log_lik = new_lik\n",
    "#     merge_lik = log_lik\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #     print('\\n')\n",
    "# #     agent, merge_lik = merge(agent, observations, actions, split_lik, clone_map, clone_history, n_particles=10, )\n",
    "#     name = 'merge '+ str(i)\n",
    "#     draw_graph(agent, clone_map, niter=name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ####### maybe instead of doing particle filter totally randomly, this can be rule-based too\n",
    "    # ####### for example, proportional to clones with least number of tables will be more likely to be pruned\n",
    "    # ####### probability for being selected == inverse of customers\n",
    "\n",
    "\n",
    "\n",
    "    #         # merge according to number of particles, and update them if they improved\n",
    "\n",
    "    # prev_prev_clone, prev_clone, prev_prev_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.TM[18,11,0])\n",
    "print(agent.TM[18,11,1])\n",
    "print(agent.TM[18,11,2])\n",
    "print(agent.TM[18,11,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph_TM(agent, clone_map, TM_greedy, niter=0, threshold=0, binary_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_greedy = np.zeros(np.shape(agent.TM))\n",
    "for s_ in range(np.shape(agent.TM)[0]):\n",
    "    for a_ in range(np.shape(agent.TM)[2]):\n",
    "        \n",
    "        maxtrans = -np.inf\n",
    "        maxind = -np.inf\n",
    "        for s in range(np.shape(agent.TM)[0]):\n",
    "            if maxtrans < agent.TM[s_,s,a_]: \n",
    "                maxtrans = agent.TM[s_,s,a_]\n",
    "                maxind = s\n",
    "        # print(s_, maxtrans, maxind)\n",
    "        TM_greedy[s_, maxind, a_] += 1\n",
    "        print(\"action {} in state {} leads to {} with prob {}\".format(a_, s_, maxind, maxtrans))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_history[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pes[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_ in range(5):\n",
    "    for a_ in range(np.shape(agent.TM)[2]):\n",
    "        \n",
    "        maxtrans = -np.inf\n",
    "        maxind = -np.inf\n",
    "        for s in range(np.shape(agent.TM)[0]):\n",
    "            # if maxtrans < agent.TM[s_,s,a_]: \n",
    "            #     maxtrans = agent.TM[s_,s,a_]\n",
    "            #     maxind = s\n",
    "        # print(s_, maxtrans, maxind)\n",
    "            print(\"action {} in state {} leads to {} with prob {}\".format(a_, s_,s, agent.TM[s_,s,a_]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(agent.TM[0,:,0\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(agent.TM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "total_indices = np.array(total_indices)\n",
    "split_indices = np.where(total_indices==0)[0]\n",
    "merge_indices = np.where(total_indices==1)[0]\n",
    "total_lls = -np.array(total_lls)\n",
    "split_ll = [total_lls[i] for i in split_indices]\n",
    "merge_ll = [total_lls[i] for i in merge_indices]\n",
    "# plt.scatter()\n",
    "# Plot splits\n",
    "plt.scatter(split_indices, split_ll, color='red', label='Splits', s=100,zorder=2)\n",
    "\n",
    "# Label each split a little above the dot\n",
    "y_offset = 0.01  # Adjust this value as needed\n",
    "for i, idx in enumerate(split_indices):\n",
    "    plt.text(\n",
    "        idx, \n",
    "        split_ll[i] + y_offset,  # Add offset to y-coordinate\n",
    "        f\"Split {i+1}\", \n",
    "        fontsize=10, \n",
    "        color='red', \n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "# Plot merges\n",
    "plt.scatter(merge_indices, merge_ll, color='blue', label='Merges', s=50, zorder=1)\n",
    "\n",
    "plt.title(\"Training Curve: Splits and Merges\", size=20)\n",
    "# plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Negative log Likelihood\", size=20)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(agent.TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_indices = np.array(total_indices)\n",
    "split_indices = np.where(total_indices==0)[0]\n",
    "split_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_indices = np.where(total_indices==1)[0]\n",
    "merge_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #2. Batch updating (recommended to begin with this)\n",
    "# ############################################## don't touch this!###########################################\n",
    "\n",
    "# room = np.array(\n",
    "#     [\n",
    "#         [1, 2, 3, 0, 3, 1, 1, 1],\n",
    "#         [1, 1, 3, 2, 3, 2, 3, 1],\n",
    "#         [1, 1, 2, 0, 1, 2, 1, 0],\n",
    "#         [0, 2, 1, 1, 3, 0, 0, 2],\n",
    "#         [3, 3, 1, 0, 1, 0, 3, 0],\n",
    "#         [2, 1, 2, 3, 3, 3, 2, 0],\n",
    "#     ]\n",
    "# )\n",
    "# actions, observations, rc = datagen_structured_obs_room(room, length=5000)     #Use length=50000 for bigger room\n",
    "# pseudocount=1e-3\n",
    "# np.random.seed(42)\n",
    "# agent = Agent(pseudocount=pseudocount)\n",
    "# alpha = 1\n",
    "# n_iter = 1 #100\n",
    "# n_particles = 50\n",
    "# n_actions = len(np.unique(actions))\n",
    "# n_obs = len(np.unique(observations))\n",
    "# scaling_factor = 10\n",
    "# pes = []\n",
    "# for i in range(n_iter): \n",
    "    \n",
    "#     # 1. SPLITTING\n",
    "#     print('Splitting: iteration {}'.format(i))\n",
    "#     print('TM size: {}'.format(np.shape(agent.TM)))\n",
    "#     clone_history = []\n",
    "#     if i==0: # first iteration        \n",
    "#         clone_map= []   \n",
    "#         # initialize clones for each obs\n",
    "#         for obs in range(n_obs):\n",
    "#             clone_map.append((obs,0,obs))    \n",
    "#         unique_clone_id = n_obs-1\n",
    "#     else: \n",
    "#         unique_clone_id = len(clone_map)-1\n",
    "#     lik_orig = 0\n",
    "#     for o, obs in enumerate(observations): \n",
    "#         if o < 2:\n",
    "#             clone_history.append((obs,0, obs))\n",
    "#             # update transition\n",
    "#             if o ==1:\n",
    "#                 agent.update_count(observations[o-1], observations[o], actions[o]) # update transition between t-1 and t-2\n",
    "#                 agent.normalize_TM()                        \n",
    "#             continue # skip 1st observation\n",
    "#         splitted = False\n",
    "        \n",
    "#         prev_prev_action = actions[o-2]\n",
    "#         prev_action = actions[o-1]\n",
    "#         prev_obs = observations[o-1]\n",
    "#         prev_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "#         # get the PE using the transition matrix\n",
    "        \n",
    "#         prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "#         # Get all state, actions that lead to current obs:\n",
    "#         obs_ind = [t[2] for t in clone_map if t[0]==obs] # this is list of curr obs clone id\n",
    "#         PE = 0\n",
    "#         for oi in obs_ind: # all possible clones related to current observation\n",
    "#             for pi in prev_ind: # all possible clones related to t-1 observation\n",
    "            \n",
    "#                 PE += agent.TM[pi, oi, prev_action] # just sum up PEs for now\n",
    "# #         PE = \n",
    "#         pes.append(PE)\n",
    "#         assignment, _ =  CRP(agent, prev_obs, alpha=scaling_factor/PE)\n",
    "        \n",
    "#         post_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "#         if prev_tables != post_tables: # a new clone has been created for this observation\n",
    "\n",
    "#             splitted = True\n",
    "#             unique_clone_id += 1\n",
    "#             clone_map.append((prev_obs, assignment, unique_clone_id))\n",
    "#             agent.expand_split_TM()\n",
    "#             prev_clone = unique_clone_id # clone id of t-1 : unique\n",
    "            \n",
    "#         else:\n",
    "\n",
    "#             prev_clone = [t[2] for t in clone_map if t[0]==obs and t[1]==assignment] # clone id of t-1 revealed\n",
    "#         prev_prev_obs, prev_prev_assignment, prev_prev_clone = clone_history[-1]\n",
    "# #         if i > 0: \n",
    "# #             print(clone_history)\n",
    "#         agent.update_count(prev_prev_clone, prev_clone, prev_prev_action) # update transition between t-1 and t-2\n",
    "#         agent.normalize_TM()\n",
    "\n",
    "#         clone_history.append((prev_obs, assignment, prev_clone)) # this keeps track of observation & clone history\n",
    "\n",
    "#     # 2. FORWARD ALGORITHM TO GET LIKELIHOOD\n",
    "#     TM = agent.TM.copy()\n",
    "#     prior = get_distribution(agent.groups_of_tables)\n",
    "#     log_lik = forward_algorithm(agent, prior, observations, actions, clone_map, clone_history, TM)\n",
    "# #     print(log_lik)\n",
    "#     print('Log lik for split iteration {}: {}'.format(i, log_lik))\n",
    "    \n",
    "            \n",
    "#     # 3. MERGING\n",
    "\n",
    "#     for particle in range(n_particles): # many hypotheses // can this be parallelized?\n",
    "#         print(\"Particle {}\".format(particle))\n",
    "#         curr_obs, clone_num, unique_idx = random.choice(clone_map[n_obs:])\n",
    "#         shared_obs_clones = [t for t in clone_map[n_obs:] if t[0]==curr_obs and t[1] != clone_num] # all the clones that share the obs\n",
    "#     #     prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "#     #     _, clone_id = clone_list[idx]\n",
    "#         # do the merging by iterating over other clones\n",
    "#         # in the sequence, find other clones that could be merged with the current\n",
    "\n",
    "#         compare_clones = random.choice(shared_obs_clones) # pick a random clone that shares observation \n",
    "#         #### make this selection anti-proportional to number of assigned observations???\n",
    "\n",
    "#         state_a = unique_idx\n",
    "#         state_b = compare_clones[2]\n",
    "#         merged_TM = agent.TM.copy()\n",
    "\n",
    "#         # adjusting prior distribution\n",
    "#         merged_prior = get_distribution(agent.groups_of_tables)\n",
    "#         merged_prior[state_a] += merged_prior[state_b]\n",
    "#         merged_prior[state_b] = 0 #pseudocount    \n",
    "#         # adjusting TM\n",
    "#         for s in range(np.shape(agent.TM)[0]):\n",
    "#             if s != state_a and s != state_b:\n",
    "#                 for a in range(len(np.unique(actions))):\n",
    "#                     merged_TM[state_a][s][a] += merged_TM[state_b][s][a]\n",
    "#                     merged_TM[s][state_a][a] += merged_TM[s][state_b][a]\n",
    "#                     merged_TM[s][state_b][a] = pseudocount\n",
    "#                     merged_TM[state_b][s][a] = pseudocount\n",
    "\n",
    "\n",
    "#         # Remove state_b by zeroing out probabilities (optional)\n",
    "#     #     agent.TM[state_b] = np.zeros(self.num_states)    \n",
    "\n",
    "#         # Get the new prior\n",
    "#     #     new_prior = # GET THE NEW DISTRIBUTION\n",
    "\n",
    "#         # Get the new likelihood by forward algorithm\n",
    "#     #     new_lik = new_prior * new_Tm\n",
    "#         new_lik = forward_algorithm(agent, merged_prior, observations, actions, clone_map, clone_history, merged_TM)\n",
    "#     #     print(log_lik, new_lik)\n",
    "#         if new_lik > log_lik: # if merging is better: \n",
    "#             agent.TM = merged_TM\n",
    "#             agent.groups_of_tables[curr_obs][clone_num] += agent.groups_of_tables[curr_obs][compare_clones[1]]\n",
    "#             agent.groups_of_tables[curr_obs][compare_clones[1]] = 0\n",
    "#             print(\"Merged Obs {} clone {} into clone {}, log lik: {}\".format(curr_obs, compare_clones[1], clone_num, new_lik))\n",
    "            \n",
    "            \n",
    "#             # clean up clones (including values that are 0)\n",
    "#             print(agent.groups_of_tables[curr_obs])\n",
    "#             orig_table = agent.groups_of_tables[curr_obs].copy()\n",
    "# #             print(orig_table)\n",
    "#             orig_table[compare_clones[1]] = 0\n",
    "#             delete_indices = [o for o,i in enumerate(orig_table) if orig_table[i]==0]\n",
    "#             # Get unique IDs for the delete_indices from clone_map\n",
    "#             delete_items = [\n",
    "#                 t for t in clone_map\n",
    "#                 for d in delete_indices\n",
    "#                 if t[0] == curr_obs and t[1] == d\n",
    "#             ]\n",
    "# #                     delete_uniqueids = [\n",
    "# #                 t[2] for t in clone_map\n",
    "# #                 for d in delete_indices\n",
    "# #                 if t[0] == curr_obs and t[1] == d\n",
    "# #             ]\n",
    "#             new_table = {}\n",
    "#             ni = 0\n",
    "#             for i in range(len(orig_table)):\n",
    "# #                 print(orig_table[i])\n",
    "#                 if orig_table[i] != 0: \n",
    "#                     new_table[ni] = orig_table[i]\n",
    "#                     ni+=1\n",
    "#             agent.groups_of_tables[curr_obs] = new_table\n",
    "# #             print(agent.groups_of_tables[curr_obs])\n",
    "            \n",
    "#             # clean up TM\n",
    "#             for d in delete_items:\n",
    "# #                 if d >= np.shape(agent.TM)[0]: # MAYBE \n",
    "#                 agent.TM = np.delete(np.delete(agent.TM, d[2], axis=0), d[2], axis=1)\n",
    "\n",
    "#             # clean up COUNT\n",
    "#             for d in delete_items: \n",
    "#                 agent.count = np.delete(np.delete(agent.count, d[2], axis=0), d[2], axis=1)     \n",
    "                \n",
    "#             # clean up clone_map\n",
    "# #             print(clone_map)\n",
    "# #             for ids in delete_uniqueids: \n",
    "# #                 print(ids)\n",
    "# #                 def clean_clone_map(data, delete_obs, delete_clone, delete_unique_id):\n",
    "#             clone_map = clean_clone_map(clone_map, delete_items)\n",
    "# #             print(clone_map)\n",
    "#             print(np.shape(agent.TM))\n",
    "            \n",
    "                        \n",
    "#             log_lik = new_lik    \n",
    "# # ####### maybe instead of doing particle filter totally randomly, this can be rule-based too\n",
    "# # ####### for example, proportional to clones with least number of tables will be more likely to be pruned\n",
    "# # ####### probability for being selected == inverse of customers\n",
    "    \n",
    "    \n",
    "    \n",
    "# #         # merge according to number of particles, and update them if they improved\n",
    "                \n",
    "# # prev_prev_clone, prev_clone, prev_prev_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Doing a more interactive process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Batch updating (recommended to begin with this)\n",
    "\n",
    "\n",
    "room = np.array(\n",
    "    [\n",
    "        [1, 2, 3, 0, 3, 1, 1, 1],\n",
    "        [1, 1, 3, 2, 3, 2, 3, 1],\n",
    "        [1, 1, 2, 0, 1, 2, 1, 0],\n",
    "        [0, 2, 1, 1, 3, 0, 0, 2],\n",
    "        [3, 3, 1, 0, 1, 0, 3, 0],\n",
    "        [2, 1, 2, 3, 3, 3, 2, 0],\n",
    "    ]\n",
    ")\n",
    "actions, observations, rc = datagen_structured_obs_room(room, length=5000)     #Use length=50000 for bigger room\n",
    "pseudocount=1e-3\n",
    "np.random.seed(42)\n",
    "# agent = Agent(pseudocount=pseudocount)\n",
    "alpha = 1\n",
    "n_iter = 2 #100\n",
    "n_particles = 30\n",
    "n_actions = len(np.unique(actions))\n",
    "n_obs = len(np.unique(observations))\n",
    "scaling_factor = 5\n",
    "pes = []\n",
    "\n",
    "agent = Agent(num_obs=n_obs, num_actions=n_actions,pseudocount=pseudocount)\n",
    "\n",
    "for i in range(n_iter): \n",
    "    \n",
    "    # 1. SPLITTING\n",
    "    print('Splitting: iteration {}'.format(i))\n",
    "    print('TM size: {}'.format(np.shape(agent.TM)))\n",
    "    clone_history = []\n",
    "    if i==0: # first iteration        \n",
    "        clone_map= []   \n",
    "        # initialize clones for each obs\n",
    "        for obs in range(n_obs):\n",
    "            clone_map.append((obs,0,obs))    \n",
    "        unique_clone_id = n_obs-1\n",
    "    else: \n",
    "        unique_clone_id = len(clone_map)-1\n",
    "    lik_orig = 0\n",
    "    for o, obs in enumerate(observations): \n",
    "        if o < 2:\n",
    "            clone_history.append((obs,0, obs))\n",
    "            # update transition\n",
    "            if o ==1:\n",
    "                agent.update_count(observations[o-1], observations[o], actions[o]) # update transition between t-1 and t-2\n",
    "                agent.normalize_TM()                        \n",
    "            continue # skip 1st observation\n",
    "            \n",
    "            \n",
    "        splitted = False\n",
    "        \n",
    "        prev_prev_action = actions[o-2]\n",
    "        prev_action = actions[o-1]\n",
    "        prev_obs = observations[o-1]\n",
    "        prev_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "        # get the PE using the transition matrix\n",
    "        \n",
    "        prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "        # Get all state, actions that lead to current obs:\n",
    "        obs_ind = [t[2] for t in clone_map if t[0]==obs] # this is list of curr obs clone id\n",
    "        PE = 0\n",
    "        for oi in obs_ind: # all possible clones related to current observation\n",
    "            for pi in prev_ind: # all possible clones related to t-1 observation\n",
    "            \n",
    "                PE += agent.TM[pi, oi, prev_action] # just sum up PEs for now\n",
    "#         PE = \n",
    "        pes.append(PE)\n",
    "        assignment, _ =  CRP(agent, prev_obs, alpha=scaling_factor/PE)\n",
    "        \n",
    "        post_tables = agent.count_tables_in_restaurant(prev_obs)\n",
    "        if prev_tables != post_tables: # a new clone has been created for this observation\n",
    "\n",
    "            splitted = True\n",
    "            unique_clone_id += 1\n",
    "            clone_map.append((prev_obs, assignment, unique_clone_id))\n",
    "            agent.expand_split_TM()\n",
    "            prev_clone = unique_clone_id # clone id of t-1 : unique\n",
    "            \n",
    "        else:\n",
    "\n",
    "            prev_clone = [t[2] for t in clone_map if t[0]==obs and t[1]==assignment] # clone id of t-1 revealed\n",
    "        prev_prev_obs, prev_prev_assignment, prev_prev_clone = clone_history[-1]\n",
    "#         if i > 0: \n",
    "#             print(clone_history)\n",
    "        agent.update_count(prev_prev_clone, prev_clone, prev_prev_action) # update transition between t-1 and t-2\n",
    "        agent.normalize_TM()\n",
    "\n",
    "        clone_history.append((prev_obs, assignment, prev_clone)) # this keeps track of observation & clone history\n",
    "\n",
    "    # 2. FORWARD ALGORITHM TO GET LIKELIHOOD\n",
    "    TM = agent.TM.copy()\n",
    "    prior = get_distribution(agent.groups_of_tables)\n",
    "    log_lik = forward_algorithm(agent, prior, observations, actions, clone_map, clone_history, TM)\n",
    "#     print(log_lik)\n",
    "    print('Log lik for split iteration {}: {}'.format(i, log_lik))\n",
    "    \n",
    "            \n",
    "    # 3. MERGING\n",
    "\n",
    "    for particle in range(n_particles): # many hypotheses // can this be parallelized?\n",
    "        print(\"Particle {}\".format(particle))\n",
    "        curr_obs, clone_num, unique_idx = random.choice(clone_map[n_obs:])\n",
    "        shared_obs_clones = [t for t in clone_map[n_obs:] if t[0]==curr_obs and t[1] != clone_num] # all the clones that share the obs\n",
    "    #     prev_ind = [t[2] for t in clone_map if t[0]==prev_obs]\n",
    "    #     _, clone_id = clone_list[idx]\n",
    "        # do the merging by iterating over other clones\n",
    "        # in the sequence, find other clones that could be merged with the current\n",
    "        \n",
    "        compare_clones = random.choice(shared_obs_clones) # pick a random clone that shares observation \n",
    "#         ############ make this selection anti-proportional to number of assigned observations\n",
    "#         distribution = [agent.groups_of_tables[curr_obs][v[1]] for v in shared_obs_clones]\n",
    "#         distribution = np.array(distribution).astype(np.float64)\n",
    "#         inverted_probs = 1 / distribution\n",
    "#         inverted_probs[distribution == 0] = 0  # Handle zero probabilities if present\n",
    "#         normalized_probs = inverted_probs / inverted_probs.sum()\n",
    "#         # Randomly select an element based on probabilities\n",
    "#         compare_clones = shared_obs_clones[np.random.choice(len(shared_obs_clones), p=normalized_probs)]\n",
    "#         ################################\n",
    "\n",
    "        state_a = unique_idx\n",
    "        state_b = compare_clones[2]\n",
    "        merged_TM = agent.TM.copy()\n",
    "\n",
    "        # adjusting prior distribution\n",
    "        merged_prior = get_distribution(agent.groups_of_tables)\n",
    "        merged_prior[state_a] += merged_prior[state_b]\n",
    "        merged_prior[state_b] = 0 #pseudocount    \n",
    "        # adjusting TM\n",
    "        for s in range(np.shape(agent.TM)[0]):\n",
    "            if s != state_a and s != state_b:\n",
    "                for a in range(len(np.unique(actions))):\n",
    "                    merged_TM[state_a][s][a] += merged_TM[state_b][s][a]\n",
    "                    merged_TM[s][state_a][a] += merged_TM[s][state_b][a]\n",
    "                    merged_TM[s][state_b][a] = pseudocount\n",
    "                    merged_TM[state_b][s][a] = pseudocount\n",
    "\n",
    "\n",
    "        # Remove state_b by zeroing out probabilities (optional)\n",
    "    #     agent.TM[state_b] = np.zeros(self.num_states)    \n",
    "\n",
    "        # Get the new prior\n",
    "    #     new_prior = # GET THE NEW DISTRIBUTION\n",
    "\n",
    "        # Get the new likelihood by forward algorithm\n",
    "    #     new_lik = new_prior * new_Tm\n",
    "        merged_TM\n",
    "        new_lik = forward_algorithm(agent, merged_prior, observations, actions, clone_map, clone_history, merged_TM)\n",
    "    #     print(log_lik, new_lik)\n",
    "        if new_lik > log_lik: # if merging is better: \n",
    "            agent.TM = merged_TM\n",
    "            agent.groups_of_tables[curr_obs][clone_num] += agent.groups_of_tables[curr_obs][compare_clones[1]]\n",
    "            agent.groups_of_tables[curr_obs][compare_clones[1]] = 0\n",
    "            print(\"Merged Obs {} clone {} into clone {}, log lik: {}\".format(curr_obs, compare_clones[1], clone_num, new_lik))\n",
    "            \n",
    "            \n",
    "            # clean up clones (including values that are 0)\n",
    "            print(agent.groups_of_tables[curr_obs])\n",
    "            orig_table = agent.groups_of_tables[curr_obs].copy()\n",
    "#             print(orig_table)\n",
    "            orig_table[compare_clones[1]] = 0\n",
    "            delete_indices = [o for o,i in enumerate(orig_table) if orig_table[i]==0]\n",
    "            # Get unique IDs for the delete_indices from clone_map\n",
    "            delete_items = [\n",
    "                t for t in clone_map\n",
    "                for d in delete_indices\n",
    "                if t[0] == curr_obs and t[1] == d\n",
    "            ]\n",
    "#                     delete_uniqueids = [\n",
    "#                 t[2] for t in clone_map\n",
    "#                 for d in delete_indices\n",
    "#                 if t[0] == curr_obs and t[1] == d\n",
    "#             ]\n",
    "            new_table = {}\n",
    "            ni = 0\n",
    "            for i in range(len(orig_table)):\n",
    "#                 print(orig_table[i])\n",
    "                if orig_table[i] != 0: \n",
    "                    new_table[ni] = orig_table[i]\n",
    "                    ni+=1\n",
    "            agent.groups_of_tables[curr_obs] = new_table\n",
    "#             print(agent.groups_of_tables[curr_obs])\n",
    "            \n",
    "            # clean up TM\n",
    "            for d in delete_items:\n",
    "#                 if d >= np.shape(agent.TM)[0]: # MAYBE \n",
    "                agent.TM = np.delete(np.delete(agent.TM, d[2], axis=0), d[2], axis=1)\n",
    "\n",
    "            # clean up COUNT\n",
    "#             for d in delete_items: \n",
    "                agent.count = np.delete(np.delete(agent.count, d[2], axis=0), d[2], axis=1)     \n",
    "            agent.normalize_TM()\n",
    "                \n",
    "            # clean up clone_map\n",
    "#             print(clone_map)\n",
    "#             for ids in delete_uniqueids: \n",
    "#                 print(ids)\n",
    "#                 def clean_clone_map(data, delete_obs, delete_clone, delete_unique_id):\n",
    "            clone_map = clean_clone_map(clone_map, delete_items)\n",
    "#             print(clone_map)\n",
    "            print(np.shape(agent.TM))\n",
    "            \n",
    "                        \n",
    "            log_lik = new_lik    \n",
    "# ####### maybe instead of doing particle filter totally randomly, this can be rule-based too\n",
    "# ####### for example, proportional to clones with least number of tables will be more likely to be pruned\n",
    "# ####### probability for being selected == inverse of customers\n",
    "    \n",
    "    \n",
    "    \n",
    "#         # merge according to number of particles, and update them if they improved\n",
    "                \n",
    "# prev_prev_clone, prev_clone, prev_prev_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_obs_clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_obs_clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.groups_of_tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "[v for v in range(len(agent.groups_of_tables[curr_obs]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = [agent.groups_of_tables[curr_obs][v[1]] for v in shared_obs_clones]\n",
    "distribution = np.array(distribution).astype(np.float64)\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution /= distribution.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(10/np.array(pes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example transition matrix (state, state, action)\n",
    "# transition_matrix = agent.TM\n",
    "TM=viterbi_decoding(agent)\n",
    "# TM = agent.TM\n",
    "num_states = np.shape(TM)[0]\n",
    "num_actions = np.shape(TM)[2]\n",
    "\n",
    "# Threshold to show only significant transitions\n",
    "threshold = 0.9\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each state\n",
    "for state in range(num_states):\n",
    "    G.add_node(state)\n",
    "\n",
    "# Add edges based on the transition matrix\n",
    "for action in range(num_actions):\n",
    "    for from_state in range(num_states):\n",
    "        for to_state in range(num_states):\n",
    "            weight = TM[from_state, to_state, action]\n",
    "            if weight > threshold:  # Filter out weak transitions\n",
    "                G.add_edge(\n",
    "                    from_state,\n",
    "                    to_state,\n",
    "                    weight=weight,\n",
    "                    label=f\"A{action}\"  # Label edges by action\n",
    "                )\n",
    "\n",
    "# Use Kamada-Kawai layout for a more spatially balanced layout\n",
    "pos = nx.kamada_kawai_layout(G)  # Alternative layouts: spring_layout, shell_layout\n",
    "\n",
    "# Node colors based on clone_map (or any observation data)\n",
    "node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,  # Use the color scheme for nodes\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edge_color='gray',\n",
    "    arrowsize=20\n",
    ")\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "plt.title(\"Spatial Transition Graph\")\n",
    "plt.axis('off')  # Turn off axes for cleaner visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example transition matrix (state, state, action)\n",
    "# transition_matrix = agent.TM\n",
    "TM=viterbi_decoding(agent)\n",
    "# TM = agent.TM\n",
    "num_states = np.shape(TM)[0]\n",
    "num_actions = np.shape(TM)[2]\n",
    "\n",
    "# Threshold to show only significant transitions\n",
    "threshold = 0.2\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each state\n",
    "for state in range(num_states):\n",
    "    G.add_node(state)\n",
    "\n",
    "# Add edges based on the transition matrix\n",
    "for action in range(num_actions):\n",
    "    for from_state in range(num_states):\n",
    "        for to_state in range(num_states):\n",
    "            weight = TM[from_state, to_state, action]\n",
    "            if weight > threshold:  # Filter out weak transitions\n",
    "                G.add_edge(\n",
    "                    from_state,\n",
    "                    to_state,\n",
    "                    weight=weight,\n",
    "                    label=f\"A{action}\"  # Label edges by action\n",
    "                )\n",
    "\n",
    "# Use Kamada-Kawai layout for a more spatially balanced layout\n",
    "pos = nx.kamada_kawai_layout(G)  # Alternative layouts: spring_layout, shell_layout\n",
    "\n",
    "# Node colors based on clone_map (or any observation data)\n",
    "node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,  # Use the color scheme for nodes\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edge_color='gray',\n",
    "    arrowsize=20\n",
    ")\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "plt.title(\"Spatial Transition Graph\")\n",
    "plt.axis('off')  # Turn off axes for cleaner visualization\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example transition matrix (state, state, action)\n",
    "# transition_matrix = agent.TM\n",
    "TM=viterbi_decoding(agent)\n",
    "# TM = agent.TM\n",
    "num_states = np.shape(TM)[0]\n",
    "num_actions = np.shape(TM)[2]\n",
    "\n",
    "# Threshold to show only significant transitions\n",
    "threshold = 0.2\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each state\n",
    "for state in range(num_states):\n",
    "    G.add_node(state)\n",
    "\n",
    "# Add edges based on the transition matrix\n",
    "for action in range(num_actions):\n",
    "    for from_state in range(num_states):\n",
    "        for to_state in range(num_states):\n",
    "            weight = TM[from_state, to_state, action]\n",
    "            if weight > threshold:  # Filter out weak transitions\n",
    "                G.add_edge(\n",
    "                    from_state,\n",
    "                    to_state,\n",
    "                    weight=weight,\n",
    "                    label=f\"A{action}\"  # Label edges by action\n",
    "                )\n",
    "\n",
    "# Use Kamada-Kawai layout for a more spatially balanced layout\n",
    "pos = nx.kamada_kawai_layout(G)  # Alternative layouts: spring_layout, shell_layout\n",
    "\n",
    "# Node colors based on clone_map (or any observation data)\n",
    "node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,  # Use the color scheme for nodes\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edge_color='gray',\n",
    "    arrowsize=20\n",
    ")\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "plt.title(\"Spatial Transition Graph\")\n",
    "plt.axis('off')  # Turn off axes for cleaner visualization\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example transition matrix (state, state, action)\n",
    "# transition_matrix = agent.TM\n",
    "TM=viterbi_decoding(agent)\n",
    "# TM = agent.TM\n",
    "num_states = np.shape(TM)[0]\n",
    "num_actions = np.shape(TM)[2]\n",
    "\n",
    "# Threshold to show only significant transitions\n",
    "threshold = 0.1\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each state\n",
    "for state in range(num_states):\n",
    "    G.add_node(state)\n",
    "\n",
    "# Add edges based on the transition matrix\n",
    "for action in range(num_actions):\n",
    "    for from_state in range(num_states):\n",
    "        for to_state in range(num_states):\n",
    "            weight = TM[from_state, to_state, action]\n",
    "            if weight > threshold:  # Filter out weak transitions\n",
    "                G.add_edge(\n",
    "                    from_state,\n",
    "                    to_state,\n",
    "                    weight=weight,\n",
    "                    label=f\"A{action}\"  # Label edges by action\n",
    "                )\n",
    "\n",
    "# Use Kamada-Kawai layout for a more spatially balanced layout\n",
    "pos = nx.kamada_kawai_layout(G)  # Alternative layouts: spring_layout, shell_layout\n",
    "\n",
    "# Node colors based on clone_map (or any observation data)\n",
    "node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,  # Use the color scheme for nodes\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edge_color='gray',\n",
    "    arrowsize=20\n",
    ")\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "plt.title(\"Spatial Transition Graph\")\n",
    "plt.axis('off')  # Turn off axes for cleaner visualization\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example transition matrix (state, state, action)\n",
    "# transition_matrix = agent.TM\n",
    "TM=viterbi_decoding(agent)\n",
    "# TM = agent.TM\n",
    "num_states = np.shape(TM)[0]\n",
    "num_actions = np.shape(TM)[2]\n",
    "\n",
    "# Threshold to show only significant transitions\n",
    "threshold = 0.2\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each state\n",
    "for state in range(num_states):\n",
    "    G.add_node(state)\n",
    "\n",
    "# Add edges based on the transition matrix\n",
    "for action in range(num_actions):\n",
    "    for from_state in range(num_states):\n",
    "        for to_state in range(num_states):\n",
    "            weight = TM[from_state, to_state, action]\n",
    "            if weight > threshold:  # Filter out weak transitions\n",
    "                G.add_edge(\n",
    "                    from_state,\n",
    "                    to_state,\n",
    "                    weight=weight,\n",
    "                    label=f\"A{action}\"  # Label edges by action\n",
    "                )\n",
    "\n",
    "# Use Kamada-Kawai layout for a more spatially balanced layout\n",
    "pos = nx.kamada_kawai_layout(G)  # Alternative layouts: spring_layout, shell_layout\n",
    "\n",
    "# Node colors based on clone_map (or any observation data)\n",
    "node_colors = [t[0] for t in clone_map]  # Replace with your actual node color logic\n",
    "\n",
    "color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "node_colors = [color_map[val] for val in node_colors]  # Flatten room to assign colors to each cell\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,  # Use the color scheme for nodes\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edge_color='gray',\n",
    "    arrowsize=20\n",
    ")\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "plt.title(\"Spatial Transition Graph\")\n",
    "plt.axis('off')  # Turn off axes for cleaner visualization\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action from one state to other should be more deterministic?\n",
    "# draw this graph thresholded \n",
    "# like viterbi decoding, only draw the highest prob. action (1 or 0) -- binarize the TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.groups_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(4):\n",
    "    print(len(agent.groups_of_tables[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique elements and their frequencies\n",
    "np.unique(room, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example transition matrix (state, state, action)\n",
    "transition_matrix = agent.TM\n",
    "num_states = np.shape(agent.TM)[0]\n",
    "num_actions = np.shape(agent.TM)[2]\n",
    "# transition_matrix = np.random.rand(num_states, num_states, num_actions)\n",
    "\n",
    "# Threshold to show only significant transitions\n",
    "threshold = 0.1\n",
    "\n",
    "# Example observations for each state (one of four categories)\n",
    "# Replace with your actual observations\n",
    "# observations = [0, 1, 2, 3, 1]  # Each state is assigned to an observation\n",
    "# color_map = {0: 'red', 1: 'blue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "\n",
    "# Assign colors to states based on observations\n",
    "# node_colors = [color_map[obs] for obs in observations]\n",
    "node_colors = [t[0] for t in clone_map]\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each state\n",
    "for state in range(num_states):\n",
    "    G.add_node(state)\n",
    "\n",
    "# Add edges based on the transition matrix\n",
    "for action in range(num_actions):\n",
    "    for from_state in range(num_states):\n",
    "        for to_state in range(num_states):\n",
    "            weight = transition_matrix[from_state, to_state, action]\n",
    "            if weight > threshold:  # Filter out weak transitions\n",
    "                G.add_edge(\n",
    "                    from_state,\n",
    "                    to_state,\n",
    "                    weight=weight,\n",
    "                    label=f\"A{action}\"  # Label edges by action\n",
    "                )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # Position nodes using a spring layout\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')  # Get edge labels\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,  # Use the color scheme for nodes\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edge_color='gray',\n",
    "    arrowsize=20\n",
    ")\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "# Add a legend for the color scheme\n",
    "legend_labels = [f\"Observation {obs}: {color}\" for obs, color in color_map.items()]\n",
    "legend_text = \"\\n\".join(legend_labels)\n",
    "plt.gcf().text(0.9, 0.5, legend_text, fontsize=10, va='center')\n",
    "\n",
    "plt.title(\"Transition Graph with Observations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(agent.TM[:][0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ground-truth environment\n",
    "room = np.array(\n",
    "    [\n",
    "        [1, 2, 3, 0, 3, 1, 1, 1],\n",
    "        [1, 1, 3, 2, 3, 2, 3, 1],\n",
    "        [1, 1, 2, 0, 1, 2, 1, 0],\n",
    "        [0, 2, 1, 1, 3, 0, 0, 2],\n",
    "        [3, 3, 1, 0, 1, 0, 3, 0],\n",
    "        [2, 1, 2, 3, 3, 3, 2, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the color map for observations\n",
    "color_map = {0: 'pink', 1: 'skyblue', 2: 'green', 3: 'orange'}  # Observation to color mapping\n",
    "node_colors = [color_map[val] for val in room.flatten()]  # Flatten room to assign colors to each cell\n",
    "\n",
    "\n",
    "# Create a graph\n",
    "rows, cols = room.shape\n",
    "GT = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        node_id = (r, c)\n",
    "        GT.add_node(node_id, observation=room[r, c])\n",
    "\n",
    "# Add edges for neighbors (up, down, left, right)\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        if r > 0:  # Connect to the cell above\n",
    "            GT.add_edge((r, c), (r-1, c))\n",
    "        if r < rows - 1:  # Connect to the cell below\n",
    "            GT.add_edge((r, c), (r+1, c))\n",
    "        if c > 0:  # Connect to the cell to the left\n",
    "            GT.add_edge((r, c), (r, c-1))\n",
    "        if c < cols - 1:  # Connect to the cell to the right\n",
    "            GT.add_edge((r, c), (r, c+1))\n",
    "\n",
    "# Position nodes in a grid layout\n",
    "pos = {(r, c): (c, -r) for r in range(rows) for c in range(cols)}  # Flip y-axis for grid alignment\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    GT,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    labels={(r, c): f\"{room[r, c]}\" for r in range(rows) for c in range(cols)},  # Show observation value\n",
    "    node_color=node_colors,  # Use colors based on observations\n",
    "    node_size=500,\n",
    "    font_size=8,\n",
    "    edge_color='gray'\n",
    ")\n",
    "\n",
    "# Add a legend\n",
    "legend_labels = [f\"Observation {obs}: {color}\" for obs, color in color_map.items()]\n",
    "legend_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10)\n",
    "                  for color in color_map.values()]\n",
    "plt.legend(legend_patches, legend_labels, loc='upper left', title=\"Observation Key\")\n",
    "\n",
    "plt.title(\"Graph Representation of Spatial Environment\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Before putting it to agent.forward_algorithm #############    \n",
    "#     prior = get_distribution(agent.groups_of_tables)\n",
    "#     print(prior)\n",
    "\n",
    "#     num_steps = len(observations)\n",
    "#     num_states = len(prior)\n",
    "# #     forward_probs = np.zeros((num_states, num_steps)) # n total clone, sequence length\n",
    "#     log_forward_probs = np.full((num_states, num_steps), 0)  # Use -inf for log(0)\n",
    "\n",
    "#     initial_state = observations[0]\n",
    "#     for s in range(num_states):\n",
    "#         if s == initial_state:\n",
    "# #             forward_probs[s, 0] = prior[s]\n",
    "#             log_forward_probs[s,0] = prior[s]\n",
    "#         else:\n",
    "# #             forward_probs[s, 0] = 0  # Probability is zero if it doesn't match initial state\n",
    "#             log_forward_probs[s,0] = 0\n",
    "\n",
    "\n",
    "#     for t in range(1, num_steps):\n",
    "#         current_clone = [clone[2] for clone in clone_map \n",
    "#                          if clone[0]==clone_history[t][0] and clone[1]==clone_history[t][1]]\n",
    "\n",
    "#         prev_action = actions[t-1]\n",
    "\n",
    "#         for s in range(num_states):\n",
    "#                 log_forward_probs[s, t] = np.logaddexp.reduce([\n",
    "#                     log_forward_probs[prev_s, t-1] + np.log(agent.TM[prev_s, s, prev_action])\n",
    "#                     for prev_s in range(num_states)\n",
    "#                 ])\n",
    "#     print(np.sum(log_forward_probs))\n",
    "############################################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.groups_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clone_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(forward_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_total_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "#     curr_\n",
    "    print(agent.get_restaurant_total_customers(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "[agent.get_restaurant_total_customers(i)/agent.get_total_observations() for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_prior_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4): \n",
    "    print(agent.count_tables_in_restaurant(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "# Problems to consider: \n",
    "1. TM size is +4 the number of clones\n",
    "2. sum of probs (PE) doesn't necessarily have to be less than 1 but seems so. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= agent.groups_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(data):\n",
    "    \"\"\"\n",
    "    Compute the probability distribution from a nested dictionary structure.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A nested dictionary where the outer keys map to inner dictionaries\n",
    "                     containing counts for different elements.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array containing the probability distribution.\n",
    "    \"\"\"\n",
    "    # Flatten the dictionary and compute total count\n",
    "    flattened_counts = defaultdict(int)\n",
    "    total_count = 0\n",
    "\n",
    "    for outer_key, inner_dict in data.items():\n",
    "        for inner_key, count in inner_dict.items():\n",
    "            flattened_counts[inner_key] += count\n",
    "            total_count += count\n",
    "\n",
    "    # Compute probabilities as a sorted NumPy array\n",
    "    sorted_keys = sorted(flattened_counts.keys())\n",
    "    probabilities = np.array([flattened_counts[key] / total_count for key in sorted_keys])\n",
    "\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(agent.TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### General intuition\n",
    "At first, many clones will be splitted out due to vanilla transition matrix\n",
    "\n",
    "But this will be corrected with merging\n",
    "\n",
    "With more experience and more accurate transition matrices, splitting and merging will diminish\n",
    "\n",
    "we can end whenever the likelihood plataeus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample([1,2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.groups_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEs = np.array([np.sum(T_tr[a[t-1], old_ind, state_loc[i]:state_loc[i+1]]) for i in range(len(state_loc)-1)])\n",
    "if len(state_loc) < 5: ####CHECK THIS PART LATER\n",
    "    PEs = np.append(PEs, np.sum(T_tr[a[t-1], old_ind, state_loc[-1]:]))\n",
    "PEs_softmax = softmax(PEs)\n",
    "PE = PEs_softmax[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.get_group_total(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.get_total_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class splitMerge: \n",
    "    def __init__(self, alpha=1.0, depth=3):\n",
    "    # splitting\n",
    "    self.alpha = alpha   # Dispersion parameter for CRP\n",
    "    self.depth = depth   # Maximum depth of the tree\n",
    "    self.tree = defaultdict(lambda: defaultdict(int))  # Tracks customer counts at each node\n",
    "    # merging\n",
    "#         def __init__(self, num_states, transition_probs, emission_probs, alpha=1.0):\n",
    "    self.num_states = num_states\n",
    "    self.transition_probs = transition_probs\n",
    "    self.emission_probs = emission_probs\n",
    "    self.alpha = alpha  # Dirichlet prior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cscg",
   "language": "python",
   "name": "cscg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
