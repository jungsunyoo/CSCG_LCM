{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoDA trial-by-trial with **uncertainty-aware** split & online merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single notebook runs acquisition \u2192 split and then extinction \u2192 merge, with diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import sys, numpy as np\n",
    "sys.path.append('/mnt/data')\n",
    "from coda_trial_by_trial import (\n",
    "    CoDAAgent, CoDAConfig,\n",
    "    GridEnvRightDownNoSelf, GridEnvRightDownNoCue,\n",
    "    generate_episode, posterior_prob_p_greater_than\n",
    ")\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Configure uncertainty-aware splitting\n",
    "cfg = CoDAConfig(\n",
    "    gamma=0.9, lam=0.8,\n",
    "    theta_split=0.9, theta_merge=0.5,\n",
    "    n_threshold=8,                 # min step-level evidence\n",
    "    min_presence_episodes=8,       # min episode-level presence\n",
    "    min_effective_exposure=25.0,   # min E_r + E_nr\n",
    "    confidence=0.95,               # require P(p>theta_split | data) >= 0.95\n",
    "    alpha0=0.5, beta0=0.5          # Jeffreys prior\n",
    ")\n",
    "\n",
    "env = GridEnvRightDownNoSelf(cue_states=[5], env_size=(4,4), rewarded_terminal=[15])\n",
    "agent = CoDAAgent(env, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Acquisition: run episodes and allow splitting when uncertainty gates are satisfied\n",
    "split_eps = []\n",
    "with_clones = False\n",
    "for ep in range(1, 400):\n",
    "    states, actions = generate_episode(env, T=agent.get_T() if with_clones else None, max_steps=20)\n",
    "    agent.update_with_episode(states, actions)\n",
    "    new = agent.maybe_split()\n",
    "    if new:\n",
    "        with_clones = True\n",
    "        split_eps.extend([ep]*len(new))\n",
    "        # optional: stop early once we see the first split\n",
    "        # break\n",
    "\n",
    "print(\"Split episodes:\", split_eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Diagnostics: PC, posterior tail prob, and RC\n",
    "pc = agent.prospective()\n",
    "rc = agent.retrospective()\n",
    "exposure = (agent.E_r + agent.E_nr).reshape(-1)\n",
    "post_tail = np.array([posterior_prob_p_greater_than(cfg.theta_split, agent.E_r[0,i], agent.E_nr[0,i], cfg.alpha0, cfg.beta0) for i in range(agent.n_states)])\n",
    "\n",
    "# Plot prospective contingency\n",
    "plt.figure()\n",
    "plt.title(\"Prospective contingency P(US|CS)\")\n",
    "plt.plot(pc, marker='o')\n",
    "plt.axhline(cfg.theta_split, linestyle='--')\n",
    "plt.xlabel(\"state index\"); plt.ylabel(\"PC\")\n",
    "plt.show()\n",
    "\n",
    "# Plot posterior tail probability\n",
    "plt.figure()\n",
    "plt.title(f\"Posterior tail Pr[P(US|CS)>={cfg.theta_split}]\")\n",
    "plt.plot(post_tail, marker='o')\n",
    "plt.axhline(cfg.confidence, linestyle='--')\n",
    "plt.xlabel(\"state index\"); plt.ylabel(\"tail prob\")\n",
    "plt.show()\n",
    "\n",
    "# Plot retrospective contingency\n",
    "plt.figure()\n",
    "plt.title(\"Retrospective contingency P(CS|US)\")\n",
    "plt.plot(rc, marker='o')\n",
    "plt.xlabel(\"state index\"); plt.ylabel(\"RC\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Extinction-like phase: reward no longer depends on cue, so informativeness drops and merges should occur\n",
    "env2 = GridEnvRightDownNoCue(cue_states=[5], env_size=(4,4), rewarded_terminal=[15])\n",
    "env2.clone_dict = dict(agent.env.clone_dict)\n",
    "env2.reverse_clone_dict = dict(agent.env.reverse_clone_dict)\n",
    "agent.env = env2\n",
    "\n",
    "merge_eps = []\n",
    "for ep in range(400, 900):\n",
    "    states, actions = generate_episode(env2, T=agent.get_T(), max_steps=20)\n",
    "    agent.update_with_episode(states, actions)\n",
    "    merged = agent.maybe_merge()\n",
    "    if merged:\n",
    "        merge_eps.extend([ep]*len(merged))\n",
    "\n",
    "print(\"Merge episodes:\", merge_eps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}