{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# CoDA — Graph snapshots **only when changed** (uncertainty-aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook runs acquisition → extinction with the uncertainty-aware split rule, but **only draws a graph snapshot when the graph changes** relative to the previous episode.\n",
    "\n",
    "**Change criterion** (fast, deterministic):\n",
    "- Any change in transition tensor shape (e.g., new clone added).\n",
    "- After aggregating over actions, any change in the **thresholded** adjacency (same threshold as the plotter).\n",
    "- Any change in `clone_dict` mapping (detects clone re-wiring/merge).\n",
    "\n",
    "This matches the visual “Graph at episode XX” you use, but avoids redundant frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, numpy as np\n",
    "# sys.path.append('/mnt/data')\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional\n",
    "from spatial_environments import GridEnvRightDownNoSelf, GridEnvRightDownNoCue, GridEnvRightDownExtinction\n",
    "from coda_trial_by_trial_util import CoDAAgent, CoDAConfig\n",
    "# from spatial_environments import GridEnvRightDownNoSelf, GridEnvRightDownNoCue\n",
    "from util import generate_dataset, generate_dataset_post_augmentation, compute_transition_entropies, find_stochastic_state_actions_by_entropy, get_successor_states\n",
    "\n",
    "# Rewritten metrics functions to ensure correct handling of fixed reference shapes\n",
    "EPS = 1e-12\n",
    "\n",
    "def _safe_row_norm(x: np.ndarray, axis: int = -1, eps: float = EPS) -> np.ndarray:\n",
    "    y = x.astype(float, copy=True)\n",
    "    s = y.sum(axis=axis, keepdims=True)\n",
    "    s[s < eps] = 1.0\n",
    "    y /= s\n",
    "    return y\n",
    "\n",
    "def _pad_to_shape(A: np.ndarray, shape: tuple) -> np.ndarray:\n",
    "    S, A_, S2 = A.shape\n",
    "    Sg, Ag, S2g = shape\n",
    "    out = np.zeros(shape, dtype=float)\n",
    "    out[:min(S,Sg), :min(A_,Ag), :min(S2,S2g)] = A[:min(S,Sg), :min(A_,Ag), :min(S2,S2g)]\n",
    "    return out\n",
    "\n",
    "def _aggregate_actions(T: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Aggregate over actions to get P(s'|s) as [S,S] row-stochastic.\"\"\"\n",
    "    return _safe_row_norm(T.sum(axis=1), axis=1)\n",
    "\n",
    "def _kl_row(p: np.ndarray, q: np.ndarray, eps: float = EPS) -> float:\n",
    "    p = np.clip(p, eps, 1.0); p /= p.sum()\n",
    "    q = np.clip(q, eps, 1.0); q /= q.sum()\n",
    "    return float(np.sum(p * (np.log(p) - np.log(q))))\n",
    "\n",
    "def _js_row(p: np.ndarray, q: np.ndarray, eps: float = EPS) -> float:\n",
    "    p = np.clip(p, eps, 1.0); p /= p.sum()\n",
    "    q = np.clip(q, eps, 1.0); q /= q.sum()\n",
    "    m = 0.5*(p+q)\n",
    "    return 0.5*_kl_row(p, m, eps) + 0.5*_kl_row(q, m, eps)\n",
    "\n",
    "def _entropy_row(p: np.ndarray, eps: float = EPS) -> float:\n",
    "    p = np.clip(p, eps, 1.0); p /= p.sum()\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "def kl_over_time(T_series: List[np.ndarray],\n",
    "                 T_ref_fn,\n",
    "                 weights: Optional[np.ndarray] = None,\n",
    "                 use_js: bool = False,\n",
    "                 base_states_only: bool = False,\n",
    "                 threshold: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute KL (or JS) divergence over time between learned T and reference.\n",
    "    The reference function should return a fixed-shape reference tensor.\n",
    "    Both T and T_ref are padded to a consistent comparison shape for fair comparison.\n",
    "    \n",
    "    If base_states_only=True, only compute KL over base states (first num_unique_states),\n",
    "    excluding clone states. This is useful for extinction where reference has zeros for clones.\n",
    "    \n",
    "    If threshold > 0, only consider transitions above threshold in the adjacency matrix.\n",
    "    This focuses the comparison on significant transitions and ignores small/noise transitions.\n",
    "    \"\"\"\n",
    "    if len(T_series) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Get reference shape from first call - this is our fixed comparison shape\n",
    "    T_ref_sample = T_ref_fn(T_series[0])\n",
    "    ref_shape = T_ref_sample.shape\n",
    "    \n",
    "    # Always use reference shape for comparison (not max shape)\n",
    "    # This ensures consistent comparison throughout extinction\n",
    "    comparison_shape = ref_shape\n",
    "    \n",
    "    # If base_states_only, determine base state count from reference\n",
    "    # (assuming reference has base states in first num_unique_states positions)\n",
    "    if base_states_only:\n",
    "        # Find base state count by looking for first row with all zeros in reference\n",
    "        T_ref_check = _pad_to_shape(T_ref_sample, comparison_shape)\n",
    "        Q_check = _aggregate_actions(T_ref_check)\n",
    "        # Base states are those that have non-zero transitions in reference\n",
    "        # (clone states in extinction reference are all zeros)\n",
    "        base_mask = Q_check.sum(axis=1) > EPS\n",
    "        if base_mask.any():\n",
    "            n_base = int(np.where(~base_mask)[0][0]) if (~base_mask).any() else Q_check.shape[0]\n",
    "        else:\n",
    "            n_base = Q_check.shape[0]\n",
    "    else:\n",
    "        n_base = None\n",
    "    \n",
    "    scores = []\n",
    "    for T in T_series:\n",
    "        T_ref = T_ref_fn(T)\n",
    "        # Ensure reference is at its fixed shape\n",
    "        assert T_ref.shape == ref_shape, f\"Reference shape changed: {T_ref.shape} != {ref_shape}\"\n",
    "        \n",
    "        # Pad both to the reference shape for consistent comparison\n",
    "        T_padded = _pad_to_shape(T, comparison_shape)\n",
    "        T_ref_padded = _pad_to_shape(T_ref, comparison_shape)\n",
    "        \n",
    "        # Normalize T_padded to ensure it's a proper probability distribution\n",
    "        # (padding might have introduced zeros, so renormalize)\n",
    "        for s in range(T_padded.shape[0]):\n",
    "            for a in range(T_padded.shape[1]):\n",
    "                row_sum = T_padded[s, a, :].sum()\n",
    "                if row_sum > EPS:\n",
    "                    T_padded[s, a, :] /= row_sum\n",
    "        \n",
    "        # Aggregate over actions\n",
    "        P = _aggregate_actions(T_padded)\n",
    "        Q = _aggregate_actions(T_ref_padded)\n",
    "        \n",
    "        # If base_states_only, only compare base states\n",
    "        if base_states_only and n_base is not None:\n",
    "            P = P[:n_base, :n_base]\n",
    "            Q = Q[:n_base, :n_base]\n",
    "            S = n_base\n",
    "        else:\n",
    "            S = P.shape[0]\n",
    "        \n",
    "        # Apply threshold if specified: only consider transitions above threshold\n",
    "        if threshold > 0:\n",
    "            # Create mask for significant transitions (above threshold in either P or Q)\n",
    "            significant_mask = (P >= threshold) | (Q >= threshold)\n",
    "            \n",
    "            # For each row, only consider transitions that are significant\n",
    "            # Mask out non-significant transitions before computing KL\n",
    "            P_masked = P.copy()\n",
    "            Q_masked = Q.copy()\n",
    "            for i in range(S):\n",
    "                # Zero out non-significant transitions\n",
    "                P_masked[i, ~significant_mask[i, :]] = 0.0\n",
    "                Q_masked[i, ~significant_mask[i, :]] = 0.0\n",
    "                # Renormalize\n",
    "                p_sum = P_masked[i, :].sum()\n",
    "                q_sum = Q_masked[i, :].sum()\n",
    "                if p_sum > EPS:\n",
    "                    P_masked[i, :] /= p_sum\n",
    "                if q_sum > EPS:\n",
    "                    Q_masked[i, :] /= q_sum\n",
    "            P = P_masked\n",
    "            Q = Q_masked\n",
    "        \n",
    "        if weights is None:\n",
    "            w = np.ones(S, dtype=float)/S\n",
    "        else:\n",
    "            w = np.zeros(S, dtype=float)\n",
    "            w[:min(S, weights.shape[0])] = weights[:min(S, weights.shape[0])]\n",
    "            w = w / max(w.sum(), EPS)\n",
    "        \n",
    "        if use_js:\n",
    "            row_scores = np.array([_js_row(P[i], Q[i]) for i in range(S)])\n",
    "        else:\n",
    "            row_scores = np.array([_kl_row(P[i], Q[i]) for i in range(S)])\n",
    "        scores.append(float(np.sum(w * row_scores)))\n",
    "    return np.array(scores)\n",
    "\n",
    "def entropy_over_time(T_series: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Average next-state entropy H(S'|S) per episode.\"\"\"\n",
    "    out = []\n",
    "    for T in T_series:\n",
    "        P = _aggregate_actions(T)\n",
    "        H = np.array([_entropy_row(P[i]) for i in range(P.shape[0])])\n",
    "        out.append(float(np.mean(H)))\n",
    "    return np.array(out)\n",
    "\n",
    "def markovization_score(T: np.ndarray, eps: float = EPS) -> float:\n",
    "    \"\"\"1 - normalized conditional entropy averaged across states.\"\"\"\n",
    "    P = _aggregate_actions(T)\n",
    "    H = np.array([_entropy_row(P[i], eps) for i in range(P.shape[0])])\n",
    "    Hmax = np.log(max(2, P.shape[1]))\n",
    "    return float(1.0 - np.mean(H)/Hmax)\n",
    "\n",
    "# Keep these imports for other uses\n",
    "from coda_metrics import ref_empirical_from_rollouts, greedy_right_down_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create fixed reference function\n",
    "def _ref_fn_fixed_from_T(T_fixed: np.ndarray):\n",
    "    \"\"\"Create a reference function from a fixed transition tensor.\"\"\"\n",
    "    def _fn(T_learned):\n",
    "        return T_fixed.copy()\n",
    "    return _fn\n",
    "\n",
    "# Helper function for degradation GT (uses _build_base_T which is defined later)\n",
    "def build_gt_degradation_no_clones(env_always_reward) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Contingency degradation GT: reward is given regardless of cue/history.\n",
    "    Structural GT is simply the base right/down graph with terminals absorbing (no clones).\n",
    "    \"\"\"\n",
    "    return _build_base_T(env_always_reward)\n",
    "\n",
    "# -------- Latent inhibition: seed-level --------\n",
    "def run_latent_inhibition_seed(seed:int,\n",
    "                               cfg:CoDAConfig,\n",
    "                               pre_episodes:int = 500,\n",
    "                               acq_episodes:int = 1000,\n",
    "                               max_steps:int = 20,\n",
    "                               cue:int = 5,\n",
    "                               threshold: float = 0.3):\n",
    "    \"\"\"\n",
    "    Phase 1 (pre-exposure):  no reward (extinction-like) to inflate P(US & ~CS)\n",
    "    Phase 2 (acquisition):   normal cued task; splitting should be delayed\n",
    "    Metrics are computed across the entire run vs. the acquisition GT-with-clones.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # -------- Pre-exposure (no reward) --------\n",
    "    env_pre = GridEnvRightDownNoCue(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent   = CoDAAgent(env_pre, cfg)\n",
    "\n",
    "    T_series = []\n",
    "\n",
    "    for ep in range(1, pre_episodes+1):\n",
    "        (states, actions) = generate_dataset(env_pre, n_episodes=1, max_steps=max_steps)[0]\n",
    "        agent.update_with_episode(states, actions)\n",
    "        # No splitting expected (no US); still collect T\n",
    "        T_series.append(agent.get_T().copy())\n",
    "\n",
    "    # -------- Acquisition (normal cued task) --------\n",
    "    env_acq = GridEnvRightDownNoSelf(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent.env = env_acq  # keep uncertainty/accumulators; switch to cued env\n",
    "\n",
    "    with_clones = False\n",
    "    for ep in range(1, acq_episodes+1):\n",
    "        if with_clones:\n",
    "            (states, actions) = generate_dataset_post_augmentation(env_acq, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        else:\n",
    "            (states, actions) = generate_dataset(env_acq, n_episodes=1, max_steps=max_steps)[0]\n",
    "\n",
    "        agent.update_with_episode(states, actions)\n",
    "        new = agent.maybe_split()\n",
    "        if new:\n",
    "            with_clones = True\n",
    "\n",
    "        T_series.append(agent.get_T().copy())\n",
    "\n",
    "    # GT for latent inhibition evaluation: acquisition graph with clones\n",
    "    T_ref = build_gt_acquisition_with_clones(env_acq, cue_state=cue)\n",
    "    ref_fn = _ref_fn_fixed_from_T(T_ref)\n",
    "\n",
    "    # Metrics over time (full series vs acquisition GT) with threshold\n",
    "    KL = kl_over_time(T_series, ref_fn, use_js=False, threshold=threshold)\n",
    "    JS = kl_over_time(T_series, ref_fn, use_js=True, threshold=threshold)\n",
    "    H  = entropy_over_time(T_series)\n",
    "    MS = np.array([markovization_score(T) for T in T_series])\n",
    "\n",
    "    return dict(T_series=T_series, T_ref=T_ref, KL=KL, JS=JS, H=H, MS=MS)\n",
    "\n",
    "# -------- Normal acquisition (for comparison with latent inhibition) --------\n",
    "def run_normal_acquisition_seed(seed:int,\n",
    "                                cfg:CoDAConfig,\n",
    "                                acq_episodes:int = 1000,\n",
    "                                max_steps:int = 20,\n",
    "                                cue:int = 5,\n",
    "                                threshold: float = 0.3):\n",
    "    \"\"\"\n",
    "    Normal acquisition without pre-exposure, for comparison with latent inhibition.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env_acq = GridEnvRightDownNoSelf(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent   = CoDAAgent(env_acq, cfg)\n",
    "\n",
    "    T_series = []\n",
    "    with_clones = False\n",
    "\n",
    "    for ep in range(1, acq_episodes+1):\n",
    "        if with_clones:\n",
    "            (states, actions) = generate_dataset_post_augmentation(env_acq, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        else:\n",
    "            (states, actions) = generate_dataset(env_acq, n_episodes=1, max_steps=max_steps)[0]\n",
    "\n",
    "        agent.update_with_episode(states, actions)\n",
    "        new = agent.maybe_split()\n",
    "        if new:\n",
    "            with_clones = True\n",
    "\n",
    "        T_series.append(agent.get_T().copy())\n",
    "\n",
    "    # GT: acquisition graph with clones\n",
    "    T_ref = build_gt_acquisition_with_clones(env_acq, cue_state=cue)\n",
    "    ref_fn = _ref_fn_fixed_from_T(T_ref)\n",
    "\n",
    "    # Metrics over time with threshold\n",
    "    KL = kl_over_time(T_series, ref_fn, use_js=False, threshold=threshold)\n",
    "    JS = kl_over_time(T_series, ref_fn, use_js=True, threshold=threshold)\n",
    "    H  = entropy_over_time(T_series)\n",
    "    MS = np.array([markovization_score(T) for T in T_series])\n",
    "\n",
    "    return dict(T_series=T_series, T_ref=T_ref, KL=KL, JS=JS, H=H, MS=MS)\n",
    "\n",
    "# -------- Contingency degradation: seed-level --------\n",
    "def run_contingency_degradation_seed(seed:int,\n",
    "                                     cfg:CoDAConfig,\n",
    "                                     acq_episodes:int = 1000,\n",
    "                                     degr_episodes:int = 1000,\n",
    "                                     max_steps:int = 20,\n",
    "                                     cue:int = 5,\n",
    "                                     threshold: float = 0.3,\n",
    "                                     wash_in:int = 50,\n",
    "                                     edge_eps_early:float = 1e-4,\n",
    "                                     edge_eps_late:float  = 1e-6):\n",
    "    \"\"\"\n",
    "    Phase 1 (acquisition):      normal cued task (splits form).\n",
    "    Phase 2 (degradation):      reward at terminal regardless of cue/history (always reward).\n",
    "                                RC falls while PC~1, so clones should merge.\n",
    "    We report metrics separately for acq (vs acq GT-with-clones) and degr (vs degr GT no-clones).\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # -------- Acquisition --------\n",
    "    env_acq = GridEnvRightDownNoSelf(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent   = CoDAAgent(env_acq, cfg)\n",
    "\n",
    "    T_series_acq = []\n",
    "    with_clones = False\n",
    "\n",
    "    for ep in range(1, acq_episodes+1):\n",
    "        if with_clones:\n",
    "            (states, actions) = generate_dataset_post_augmentation(env_acq, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        else:\n",
    "            (states, actions) = generate_dataset(env_acq, n_episodes=1, max_steps=max_steps)[0]\n",
    "\n",
    "        agent.update_with_episode(states, actions)\n",
    "        if agent.maybe_split():\n",
    "            with_clones = True\n",
    "\n",
    "        T_series_acq.append(agent.get_T().copy())\n",
    "\n",
    "    T_ref_acq = build_gt_acquisition_with_clones(env_acq, cue_state=cue)\n",
    "    ref_fn_acq = _ref_fn_fixed_from_T(T_ref_acq)\n",
    "\n",
    "    # -------- Degradation (always reward; no reset) --------\n",
    "    env_deg = GridEnvRightDownAlwaysReward(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    # carry learned clones into degradation env so merges are meaningful\n",
    "    env_deg.clone_dict = dict(getattr(env_acq, \"clone_dict\", {}))\n",
    "    env_deg.reverse_clone_dict = dict(getattr(env_acq, \"reverse_clone_dict\", {}))\n",
    "    agent.env = env_deg\n",
    "\n",
    "    # short wash-in to encourage structural merges early (optional)\n",
    "    orig = dict(count_decay=agent.cfg.count_decay, trace_decay=agent.cfg.trace_decay, retro_decay=agent.cfg.retro_decay,\n",
    "                theta_merge=agent.cfg.theta_merge, confidence=agent.cfg.confidence,\n",
    "                min_presence_episodes=agent.cfg.min_presence_episodes,\n",
    "                min_effective_exposure=agent.cfg.min_effective_exposure)\n",
    "\n",
    "    agent.cfg.count_decay = 0.98\n",
    "    agent.cfg.trace_decay = 0.98\n",
    "    agent.cfg.retro_decay = 0.98\n",
    "    agent.cfg.theta_merge = 0.60\n",
    "    agent.cfg.confidence  = 0.99\n",
    "    agent.cfg.min_presence_episodes += 3\n",
    "    agent.cfg.min_effective_exposure = int(agent.cfg.min_effective_exposure * 1.5)\n",
    "\n",
    "    T_series_deg = []\n",
    "    for k in range(degr_episodes):\n",
    "        (states, actions) = generate_dataset_post_augmentation(env_deg, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        agent.update_with_episode(states, actions)\n",
    "        agent._edge_eps_override = edge_eps_early if k < wash_in else edge_eps_late\n",
    "        agent.maybe_merge()\n",
    "        T_series_deg.append(agent.get_T().copy())\n",
    "\n",
    "        if k == wash_in - 1:\n",
    "            # restore original cfg after wash-in\n",
    "            for key, val in orig.items():\n",
    "                setattr(agent.cfg, key, val)\n",
    "\n",
    "    T_ref_deg = build_gt_degradation_no_clones(env_deg)\n",
    "    ref_fn_deg = _ref_fn_fixed_from_T(T_ref_deg)\n",
    "\n",
    "    # Metrics (separate for each phase) with threshold\n",
    "    KL_acq = kl_over_time(T_series_acq, ref_fn_acq, use_js=False, threshold=threshold)\n",
    "    JS_acq = kl_over_time(T_series_acq, ref_fn_acq, use_js=True, threshold=threshold)\n",
    "    H_acq  = entropy_over_time(T_series_acq)\n",
    "    MS_acq = np.array([markovization_score(T) for T in T_series_acq])\n",
    "\n",
    "    KL_deg = kl_over_time(T_series_deg, ref_fn_deg, use_js=False, threshold=threshold)\n",
    "    JS_deg = kl_over_time(T_series_deg, ref_fn_deg, use_js=True, threshold=threshold)\n",
    "    H_deg  = entropy_over_time(T_series_deg)\n",
    "    MS_deg = np.array([markovization_score(T) for T in T_series_deg])\n",
    "\n",
    "    return dict(\n",
    "        env_acq=env_acq, env_deg=env_deg,\n",
    "        T_series_acq=T_series_acq, T_series_deg=T_series_deg,\n",
    "        T_ref_acq=T_ref_acq, T_ref_deg=T_ref_deg,\n",
    "        KL_acq=KL_acq, JS_acq=JS_acq, H_acq=H_acq, MS_acq=MS_acq,\n",
    "        KL_deg=KL_deg, JS_deg=JS_deg, H_deg=H_deg, MS_deg=MS_deg\n",
    "    )\n",
    "\n",
    "# -------- Latent inhibition: multi-seed --------\n",
    "def run_latent_inhibition_many(cfg:CoDAConfig,\n",
    "                               seeds:list,\n",
    "                               pre_episodes:int=500,\n",
    "                               acq_episodes:int=1000,\n",
    "                               max_steps:int=20,\n",
    "                               cue:int=5,\n",
    "                               threshold: float = 0.3):\n",
    "    runs = [run_latent_inhibition_seed(s, cfg, pre_episodes, acq_episodes, max_steps, cue, threshold) for s in seeds]\n",
    "    KL_runs = [r[\"KL\"] for r in runs]\n",
    "    JS_runs = [r[\"JS\"] for r in runs]\n",
    "    H_runs  = [r[\"H\"]  for r in runs]\n",
    "    MS_runs = [r[\"MS\"] for r in runs]\n",
    "    return dict(runs=runs, KL=KL_runs, JS=JS_runs, H=H_runs, MS=MS_runs)\n",
    "\n",
    "# -------- Normal acquisition: multi-seed (for comparison) --------\n",
    "def run_normal_acquisition_many(cfg:CoDAConfig,\n",
    "                                seeds:list,\n",
    "                                acq_episodes:int=1000,\n",
    "                                max_steps:int=20,\n",
    "                                cue:int=5,\n",
    "                                threshold: float = 0.3):\n",
    "    runs = [run_normal_acquisition_seed(s, cfg, acq_episodes, max_steps, cue, threshold) for s in seeds]\n",
    "    KL_runs = [r[\"KL\"] for r in runs]\n",
    "    JS_runs = [r[\"JS\"] for r in runs]\n",
    "    H_runs  = [r[\"H\"]  for r in runs]\n",
    "    MS_runs = [r[\"MS\"] for r in runs]\n",
    "    return dict(runs=runs, KL=KL_runs, JS=JS_runs, H=H_runs, MS=MS_runs)\n",
    "\n",
    "def plot_latent_inhibition_summary(res_li, pre_episodes:int, title_suffix=\"\"):\n",
    "    plot_band(res_li[\"KL\"], f\"KL (latent inhibition{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_li[\"JS\"], f\"JS (latent inhibition{title_suffix})\", \"JS\")\n",
    "    plot_band(res_li[\"H\"],  f\"Avg H(S'|S) (latent inhibition{title_suffix})\", \"nats\")\n",
    "    plot_band(res_li[\"MS\"], f\"Markovization (latent inhibition{title_suffix})\", \"[0,1]\")\n",
    "    # vertical line to show transition from pre-exposure to acquisition\n",
    "    for fig_num in plt.get_fignums()[-4:]:\n",
    "        plt.figure(fig_num)\n",
    "        plt.axvline(pre_episodes, ls=\"--\", alpha=0.4)\n",
    "\n",
    "def plot_latent_inhibition_comparison(res_li, res_normal, pre_episodes:int, acq_episodes:int, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Compare latent inhibition (with pre-exposure) vs normal acquisition (without pre-exposure).\n",
    "    Shows that latent inhibition slows down learning.\n",
    "    \"\"\"\n",
    "    # Align the series: LI starts at episode 0 (pre-exposure), normal starts at episode 0\n",
    "    # For comparison, we want to show LI acquisition phase vs normal acquisition\n",
    "    # So we'll plot LI from pre_episodes onwards, and normal from 0 onwards\n",
    "    # But offset LI x-axis by pre_episodes so they align at \"acquisition start\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot normal acquisition (starts immediately)\n",
    "    mean_normal, se_normal = mean_se(res_normal[\"KL\"])\n",
    "    x_normal = np.arange(len(mean_normal))\n",
    "    ax.plot(x_normal, mean_normal, 'b-', lw=2.2, label=f\"Normal acquisition (n={len(res_normal['KL'])})\")\n",
    "    ax.fill_between(x_normal, mean_normal - se_normal, mean_normal + se_normal, alpha=0.25, color='blue')\n",
    "    \n",
    "    # Plot latent inhibition (pre-exposure + acquisition)\n",
    "    # Extract only the acquisition phase (after pre_episodes)\n",
    "    KL_li_acq = []\n",
    "    for kl_series in res_li[\"KL\"]:\n",
    "        if len(kl_series) > pre_episodes:\n",
    "            KL_li_acq.append(kl_series[pre_episodes:])\n",
    "    \n",
    "    mean_li, se_li = mean_se(KL_li_acq)\n",
    "    x_li = np.arange(len(mean_li))\n",
    "    # Offset by pre_episodes to align with normal acquisition start\n",
    "    ax.plot(x_li + pre_episodes, mean_li, 'r-', lw=2.2, label=f\"Latent inhibition (n={len(res_li['KL'])})\")\n",
    "    ax.fill_between(x_li + pre_episodes, mean_li - se_li, mean_li + se_li, alpha=0.25, color='red')\n",
    "    \n",
    "    # Add vertical line at pre-exposure end\n",
    "    ax.axvline(pre_episodes, ls=\"--\", color='gray', alpha=0.5, label=\"Pre-exposure end\")\n",
    "    \n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"KL (nats)\")\n",
    "    ax.set_title(f\"Learning Speed Comparison: Latent Inhibition vs Normal Acquisition{title_suffix}\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "# -------- Degradation: multi-seed --------\n",
    "def run_degradation_many(cfg:CoDAConfig,\n",
    "                         seeds:list,\n",
    "                         acq_episodes:int=1000,\n",
    "                         degr_episodes:int=1000,\n",
    "                         max_steps:int=20,\n",
    "                         cue:int=5,\n",
    "                         threshold: float = 0.3):\n",
    "    runs = [run_contingency_degradation_seed(s, cfg, acq_episodes, degr_episodes, max_steps, cue, threshold) for s in seeds]\n",
    "    return dict(\n",
    "        runs=runs,\n",
    "        KL_acq=[r[\"KL_acq\"] for r in runs], JS_acq=[r[\"JS_acq\"] for r in runs],\n",
    "        H_acq=[r[\"H_acq\"] for r in runs],   MS_acq=[r[\"MS_acq\"] for r in runs],\n",
    "        KL_deg=[r[\"KL_deg\"] for r in runs], JS_deg=[r[\"JS_deg\"] for r in runs],\n",
    "        H_deg=[r[\"H_deg\"] for r in runs],   MS_deg=[r[\"MS_deg\"] for r in runs],\n",
    "    )\n",
    "\n",
    "def plot_degradation_summary(res_deg, acq_episodes:int, title_suffix=\"\"):\n",
    "    plot_band(res_deg[\"KL_acq\"], f\"KL (acquisition — degradation runs{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_deg[\"JS_acq\"], f\"JS (acquisition — degradation runs{title_suffix})\", \"JS\")\n",
    "    plot_band(res_deg[\"H_acq\"],  f\"Avg H(S'|S) (acquisition — degradation runs{title_suffix})\", \"nats\")\n",
    "    plot_band(res_deg[\"MS_acq\"], f\"Markovization (acquisition — degradation runs{title_suffix})\", \"[0,1]\")\n",
    "\n",
    "    plot_band(res_deg[\"KL_deg\"], f\"KL (contingency degradation{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_deg[\"JS_deg\"], f\"JS (contingency degradation{title_suffix})\", \"JS\")\n",
    "    plot_band(res_deg[\"H_deg\"],  f\"Avg H(S'|S) (contingency degradation{title_suffix})\", \"nats\")\n",
    "    plot_band(res_deg[\"MS_deg\"], f\"Markovization (contingency degradation{title_suffix})\", \"[0,1]\")\n",
    "    \n",
    "    # Add vertical line to show transition from acquisition to degradation\n",
    "    for fig_num in plt.get_fignums()[-4:]:\n",
    "        plt.figure(fig_num)\n",
    "        plt.axvline(acq_episodes, ls=\"--\", alpha=0.4)\n",
    "\n",
    "def plot_degradation_kl_over_time(res_deg, acq_episodes:int, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot KL over time for degradation, showing both acquisition and degradation phases.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot acquisition phase\n",
    "    mean_acq, se_acq = mean_se(res_deg[\"KL_acq\"])\n",
    "    x_acq = np.arange(len(mean_acq))\n",
    "    ax.plot(x_acq, mean_acq, 'b-', lw=2.2, label=f\"Acquisition (n={len(res_deg['KL_acq'])})\")\n",
    "    ax.fill_between(x_acq, mean_acq - se_acq, mean_acq + se_acq, alpha=0.25, color='blue')\n",
    "    \n",
    "    # Plot degradation phase\n",
    "    mean_deg, se_deg = mean_se(res_deg[\"KL_deg\"])\n",
    "    x_deg = np.arange(len(mean_deg)) + acq_episodes\n",
    "    ax.plot(x_deg, mean_deg, 'r-', lw=2.2, label=f\"Degradation (n={len(res_deg['KL_deg'])})\")\n",
    "    ax.fill_between(x_deg, mean_deg - se_deg, mean_deg + se_deg, alpha=0.25, color='red')\n",
    "    \n",
    "    # Add vertical line at transition\n",
    "    ax.axvline(acq_episodes, ls=\"--\", color='gray', alpha=0.5, label=\"Acquisition → Degradation\")\n",
    "    \n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"KL (nats)\")\n",
    "    ax.set_title(f\"KL Over Time: Contingency Degradation{title_suffix}\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "# Helper function for mean ± SE (if not already defined)\n",
    "def mean_se(arrs):\n",
    "    \"\"\"Compute mean and standard error across runs.\"\"\"\n",
    "    L = max(len(a) for a in arrs)\n",
    "    M = np.full((len(arrs), L), np.nan)\n",
    "    for i, a in enumerate(arrs):\n",
    "        M[i, :len(a)] = a\n",
    "    mean = np.nanmean(M, axis=0)\n",
    "    se   = np.nanstd(M, axis=0, ddof=max(1, min(len(arrs)-1, 1))) / np.sqrt(max(1, len(arrs)))\n",
    "    return mean, se\n",
    "\n",
    "def plot_band(y_runs, title, ylabel):\n",
    "    \"\"\"Plot mean ± SE band.\"\"\"\n",
    "    mean, se = mean_se(y_runs)\n",
    "    x = np.arange(len(mean))\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, mean, lw=2.2, label=f\"mean ({len(y_runs)} seeds)\")\n",
    "    plt.fill_between(x, mean-se, mean+se, alpha=0.25, label=\"±1 SE\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "def sanitize_for_plot(env, T, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Remove clones with ~zero inbound+outbound mass and rebuild reverse map.\n",
    "    Safe for growing/shrinking T during splits/merges.\n",
    "    \"\"\"\n",
    "    if T is None or getattr(T, \"ndim\", 0) != 3:\n",
    "        return\n",
    "\n",
    "    S = T.shape[0]\n",
    "\n",
    "    # Outbound mass from each state: sum over actions and next states\n",
    "    out_mass = T.sum(axis=(1, 2))   # shape [S]\n",
    "\n",
    "    # Inbound mass to each state: sum over sources and actions\n",
    "    in_mass  = T.sum(axis=(0, 1))   # shape [S]\n",
    "\n",
    "    active = (out_mass + in_mass) > eps\n",
    "\n",
    "    # Drop clone ids that are inactive or out of bounds\n",
    "    for cl in list(env.clone_dict.keys()):\n",
    "        if cl >= S or not active[cl]:\n",
    "            env.clone_dict.pop(cl, None)\n",
    "\n",
    "    # Rebuild reverse mapping (parent -> latest clone)\n",
    "    env.reverse_clone_dict = {parent: cl for cl, parent in env.clone_dict.items()}\n",
    "\n",
    "def make_terminals_absorbing_for_plot(T, terminals):\n",
    "    T = T.copy()\n",
    "    for t in terminals:\n",
    "        if t < T.shape[0]:\n",
    "            T[t, :, :] = 0.0\n",
    "    return T\n",
    "def thresh_adj(T, thr=0.3):\n",
    "    A = T.sum(axis=1)         # [S,S]\n",
    "    return (A >= thr).astype(np.uint8)\n",
    "\n",
    "def clone_dict_tuple(d):\n",
    "    return tuple(sorted(d.items()))\n",
    "\n",
    "def graph_changed(prev_T, prev_map, curr_T, curr_map, thr=0.3):\n",
    "    if prev_T is None or prev_T.shape != curr_T.shape:\n",
    "        return True\n",
    "    A_prev = thresh_adj(prev_T, thr)\n",
    "    A_curr = thresh_adj(curr_T, thr)\n",
    "    if A_prev.shape != A_curr.shape:\n",
    "        return True\n",
    "    if (A_prev != A_curr).any():\n",
    "        return True\n",
    "    return prev_map != curr_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _n_actions(env):\n",
    "    return max(a for acts in env.valid_actions.values() for a in acts) + 1\n",
    "\n",
    "def _base_successor(env, s, a):\n",
    "    i, j  = env.state_to_pos[s]\n",
    "    di, dj = env.base_actions[a]\n",
    "    ni, nj = i + di, j + dj\n",
    "    return env.pos_to_state.get((ni, nj), s)\n",
    "\n",
    "def _build_base_T(env):\n",
    "    S0 = env.num_unique_states\n",
    "    A  = _n_actions(env)\n",
    "    T  = np.zeros((S0, A, S0), dtype=float)\n",
    "    terminals = set(env.rewarded_terminals) | set(env.unrewarded_terminals)\n",
    "    for s, acts in env.valid_actions.items():\n",
    "        if s in terminals: \n",
    "            continue\n",
    "        for a in acts:\n",
    "            sp = _base_successor(env, s, a)\n",
    "            T[s, a, sp] = 1.0\n",
    "    return T\n",
    "\n",
    "def _descendants_until_terminal(env, start, terminals):\n",
    "    adj = {s: [_base_successor(env, s, a) for a in env.valid_actions.get(s, [])]\n",
    "           for s in range(env.num_unique_states)}\n",
    "    seen, Q = set(), [start]\n",
    "    while Q:\n",
    "        s = Q.pop(0)\n",
    "        for sp in adj.get(s, []):\n",
    "            if sp in terminals: \n",
    "                continue\n",
    "            if sp not in seen:\n",
    "                seen.add(sp); Q.append(sp)\n",
    "    return seen\n",
    "\n",
    "def build_gt_acquisition_with_clones(env, cue_state: int) -> np.ndarray:\n",
    "    S0 = env.num_unique_states\n",
    "    A  = _n_actions(env)\n",
    "    T0 = _build_base_T(env)\n",
    "\n",
    "    terminals = set(env.rewarded_terminals) | set(env.unrewarded_terminals)\n",
    "    rewT = env.rewarded_terminals\n",
    "    unrewT = env.unrewarded_terminals\n",
    "\n",
    "    D = _descendants_until_terminal(env, cue_state, terminals)\n",
    "    clone_of = {orig: S0 + k for k, orig in enumerate(sorted(D))}\n",
    "    S = S0 + len(clone_of)\n",
    "    T = np.zeros((S, A, S), dtype=float)\n",
    "    T[:S0, :, :S0] = T0\n",
    "\n",
    "    for a in env.valid_actions[cue_state]:\n",
    "        sp = _base_successor(env, cue_state, a)\n",
    "        if sp in D:\n",
    "            T[cue_state, a, sp] = 0.0\n",
    "            T[cue_state, a, clone_of[sp]] = 1.0\n",
    "\n",
    "    for orig, cl in clone_of.items():\n",
    "        for a in env.valid_actions[orig]:\n",
    "            sp = _base_successor(env, orig, a)\n",
    "            if sp in terminals:\n",
    "                for t in rewT:   T[cl, a, t] = 1.0\n",
    "                for t in unrewT: T[cl, a, t] = 0.0\n",
    "            else:\n",
    "                T[cl, a, clone_of[sp] if sp in clone_of else sp] = 1.0\n",
    "\n",
    "        for a in env.valid_actions[orig]:\n",
    "            sp = _base_successor(env, orig, a)\n",
    "            if sp in rewT:\n",
    "                T[orig, a, sp] = 0.0\n",
    "                idx = rewT.index(sp)\n",
    "                T[orig, a, unrewT[idx]] = 1.0\n",
    "    return T\n",
    "\n",
    "def build_gt_extinction_no_clones(env2) -> np.ndarray:\n",
    "    T = _build_base_T(env2)\n",
    "    for s in range(env2.num_unique_states):\n",
    "        for a in env2.valid_actions.get(s, []):\n",
    "            sp = _base_successor(env2, s, a)\n",
    "            if sp in env2.rewarded_terminals:\n",
    "                idx = env2.rewarded_terminals.index(sp)\n",
    "                T[s, a, sp] = 0.0\n",
    "                T[s, a, env2.unrewarded_terminals[idx]] = 1.0\n",
    "    return T\n",
    "\n",
    "def build_gt_extinction_with_clone_shape(env2, T_acq_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build extinction reference with same shape as acquisition (with clones).\n",
    "    Base states redirect to unrewarded terminals (extinction behavior).\n",
    "    Clone states have all zeros (no transitions, since extinction has no clones).\n",
    "    \"\"\"\n",
    "    S_acq, A_acq, S2_acq = T_acq_shape\n",
    "    S_base = env2.num_unique_states\n",
    "    \n",
    "    # Start with base extinction (no clones)\n",
    "    T_base = build_gt_extinction_no_clones(env2)\n",
    "    \n",
    "    # Expand to match acquisition shape\n",
    "    T_ext = np.zeros((S_acq, A_acq, S2_acq), dtype=float)\n",
    "    T_ext[:S_base, :, :S_base] = T_base\n",
    "    \n",
    "    # Clone states (S_base and above) remain zeros - no transitions\n",
    "    # This represents extinction where clones don't exist\n",
    "    \n",
    "    # Normalize rows to ensure proper probability distribution\n",
    "    for s in range(S_base):\n",
    "        for a in range(A_acq):\n",
    "            row_sum = T_ext[s, a, :].sum()\n",
    "            if row_sum > 0:\n",
    "                T_ext[s, a, :] /= row_sum\n",
    "    \n",
    "    return T_ext\n",
    "\n",
    "def _pad3(arr: np.ndarray, shape: tuple) -> np.ndarray:\n",
    "    S, A, S2 = arr.shape\n",
    "    Sg, Ag, S2g = shape\n",
    "    if (S, A, S2) == (Sg, Ag, S2g): \n",
    "        return arr\n",
    "    out = np.zeros((Sg, Ag, S2g), dtype=float)\n",
    "    out[:min(S,Sg), :min(A,Ag), :min(S2,S2g)] = arr[:min(S,Sg), :min(A,Ag), :min(S2,S2g)]\n",
    "    return out\n",
    "\n",
    "def _ref_fn_fixed(T_fixed: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns a function that always returns the fixed reference at its original shape.\n",
    "    kl_over_time will handle padding the learned T to match.\n",
    "    \"\"\"\n",
    "    def _fn(T_learned):\n",
    "        # Always return reference at its fixed shape - let kl_over_time handle padding\n",
    "        return T_fixed.copy()\n",
    "    return _fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ref_builder_factory(env, policy_fn, nroll=300, max_steps=20):\n",
    "    def _make_ref(T_learned):\n",
    "        return ref_empirical_from_rollouts(env, policy_fn, n_episodes=nroll, max_steps=max_steps)\n",
    "    return _make_ref\n",
    "\n",
    "def collapse_to_base_size(T, env):\n",
    "    \"\"\"\n",
    "    Collapse transition tensor to base size by removing clones.\n",
    "    Redirects any transitions to/from clones back to their parent states.\n",
    "    Returns tensor of shape (num_unique_states, A, num_unique_states).\n",
    "    \"\"\"\n",
    "    S_base = env.num_unique_states\n",
    "    A = T.shape[1]\n",
    "    T_collapsed = np.zeros((S_base, A, S_base), dtype=float)\n",
    "    \n",
    "    # Build mapping: clone_id -> parent_id (for all clones, including merged ones)\n",
    "    clone_to_parent = {}\n",
    "    for clone_id, parent_id in env.clone_dict.items():\n",
    "        if parent_id < S_base:\n",
    "            clone_to_parent[clone_id] = parent_id\n",
    "    \n",
    "    # Process all transitions\n",
    "    for s in range(T.shape[0]):\n",
    "        for a in range(A):\n",
    "            for sp in range(T.shape[2]):\n",
    "                mass = T[s, a, sp]\n",
    "                if mass <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # Map source state to base state\n",
    "                if s < S_base:\n",
    "                    s_base = s\n",
    "                else:\n",
    "                    # s is a clone, redirect to parent\n",
    "                    s_base = clone_to_parent.get(s, None)\n",
    "                    if s_base is None:\n",
    "                        continue  # Skip if clone has no parent mapping\n",
    "                \n",
    "                # Map target state to base state\n",
    "                if sp < S_base:\n",
    "                    sp_base = sp\n",
    "                else:\n",
    "                    # sp is a clone, redirect to parent\n",
    "                    sp_base = clone_to_parent.get(sp, None)\n",
    "                    if sp_base is None:\n",
    "                        continue  # Skip if clone has no parent mapping\n",
    "                \n",
    "                # Add mass to collapsed tensor\n",
    "                T_collapsed[s_base, a, sp_base] += mass\n",
    "    \n",
    "    # Normalize rows to maintain stochasticity\n",
    "    for s in range(S_base):\n",
    "        for a in range(A):\n",
    "            row_sum = T_collapsed[s, a, :].sum()\n",
    "            if row_sum > 0:\n",
    "                T_collapsed[s, a, :] /= row_sum\n",
    "    \n",
    "    return T_collapsed\n",
    "\n",
    "def run_one_seed(seed:int):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = GridEnvRightDownNoSelf(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "\n",
    "    T_ref_acq_fixed = build_gt_acquisition_with_clones(env, cue_state=CUE)\n",
    "    ref_fn_acq = _ref_fn_fixed(T_ref_acq_fixed)\n",
    "\n",
    "\n",
    "    agent = CoDAAgent(env, cfg)\n",
    "\n",
    "    T_series_acq: List[np.ndarray] = []\n",
    "    with_clones = False\n",
    "    for ep in range(1, N_ACQ+1):\n",
    "        if with_clones:\n",
    "            (states, actions) = generate_dataset_post_augmentation(env, agent.get_T(), n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "        else:\n",
    "            (states, actions) = generate_dataset(env, n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "        agent.update_with_episode(states, actions)\n",
    "        if agent.maybe_split():\n",
    "            with_clones = True\n",
    "        T_series_acq.append(agent.get_T().copy())\n",
    "\n",
    "    env2 = GridEnvRightDownNoCue(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "    env2.clone_dict = dict(getattr(env, \"clone_dict\", {}))\n",
    "    env2.reverse_clone_dict = dict(getattr(env, \"reverse_clone_dict\", {}))\n",
    "    agent.env = env2\n",
    "\n",
    "    # Build extinction reference with same shape as acquisition (with clones, but zeros for clones)\n",
    "    T_ref_ext_fixed = build_gt_extinction_with_clone_shape(env2, T_ref_acq_fixed.shape)\n",
    "    # Verify shapes match\n",
    "    assert T_ref_ext_fixed.shape == T_ref_acq_fixed.shape, f\"Extinction ref shape {T_ref_ext_fixed.shape} != acquisition ref shape {T_ref_acq_fixed.shape}\"\n",
    "    ref_fn_ext = _ref_fn_fixed(T_ref_ext_fixed)\n",
    "\n",
    "    T_series_ext: List[np.ndarray] = []\n",
    "    for ep in range(N_ACQ+1, N_ACQ+N_EXT+1):\n",
    "        (states, actions) = generate_dataset_post_augmentation(env2, agent.get_T(), n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "        agent.update_with_episode(states, actions)\n",
    "        agent.maybe_merge()\n",
    "        # Keep full size (same as acquisition) for fair KL comparison over time\n",
    "        T_series_ext.append(agent.get_T().copy())\n",
    "\n",
    "    # ref_fn_acq = ref_builder_factory(env,  greedy_right_down_policy, nroll=N_ROLL_REF, max_steps=MAX_STEPS)\n",
    "    # ref_fn_ext = ref_builder_factory(env2, greedy_right_down_policy, nroll=N_ROLL_REF, max_steps=MAX_STEPS)\n",
    "\n",
    "    # For acquisition, use threshold to focus on significant transitions (>0.3)\n",
    "    # This helps KL approach 0 by ignoring small/noise transitions\n",
    "    KL_acq = kl_over_time(T_series_acq, ref_fn_acq, use_js=False, threshold=THRESH)\n",
    "    JS_acq = kl_over_time(T_series_acq, ref_fn_acq, use_js=True, threshold=THRESH)\n",
    "    H_acq  = entropy_over_time(T_series_acq)\n",
    "    MS_acq = np.array([markovization_score(T) for T in T_series_acq])\n",
    "\n",
    "    # Verify extinction reference shape (should be same as acquisition: 24,2,24)\n",
    "    if len(T_series_ext) > 0:\n",
    "        test_ref = ref_fn_ext(T_series_ext[0])\n",
    "        assert test_ref.shape == T_ref_acq_fixed.shape, f\"Extinction ref function returns shape {test_ref.shape}, expected {T_ref_acq_fixed.shape}\"\n",
    "        # Debug: print shapes to verify\n",
    "        if seed == SEED0:  # Only print for first seed to avoid spam\n",
    "            print(f\"Debug (seed {seed}): T_ref_ext_fixed shape = {T_ref_ext_fixed.shape}\")\n",
    "            print(f\"Debug (seed {seed}): T_ref_acq_fixed shape = {T_ref_acq_fixed.shape}\")\n",
    "            print(f\"Debug (seed {seed}): First T_ext shape = {T_series_ext[0].shape}\")\n",
    "            print(f\"Debug (seed {seed}): Last T_ext shape = {T_series_ext[-1].shape}\")\n",
    "            print(f\"Debug (seed {seed}): ref_fn_ext returns shape = {test_ref.shape}\")\n",
    "    \n",
    "    # For extinction, compute KL only over base states (excluding clones)\n",
    "    # since extinction reference has zeros for clones\n",
    "    # This ensures we're comparing the actual transition structure, not penalizing for clone mass\n",
    "    KL_ext = kl_over_time(T_series_ext, ref_fn_ext, use_js=False, base_states_only=True)\n",
    "    JS_ext = kl_over_time(T_series_ext, ref_fn_ext, use_js=True, base_states_only=True)\n",
    "    H_ext  = entropy_over_time(T_series_ext)\n",
    "    MS_ext = np.array([markovization_score(T) for T in T_series_ext])\n",
    "    \n",
    "    # Debug: print KL values and inspect transitions\n",
    "    if seed == SEED0 and len(KL_ext) > 0:  # Only print for first seed\n",
    "        print(f\"Debug (seed {seed}): KL_ext[0] = {KL_ext[0]:.6f}, KL_ext[25] = {KL_ext[25] if len(KL_ext) > 25 else 'N/A':.6f}, KL_ext[-1] = {KL_ext[-1]:.6f}\")\n",
    "        print(f\"Debug (seed {seed}): KL_ext change (0->25) = {KL_ext[25] - KL_ext[0] if len(KL_ext) > 25 else 'N/A':.6f}, (0->end) = {KL_ext[-1] - KL_ext[0]:.6f}\")\n",
    "        \n",
    "        # Inspect transitions for cue state (state 5) to see what's happening\n",
    "        if len(T_series_ext) > 0:\n",
    "            T_start = T_series_ext[0]\n",
    "            T_ref = ref_fn_ext(T_start)\n",
    "            # Check transitions from cue state (5) to terminals\n",
    "            cue_state = CUE\n",
    "            if cue_state < T_start.shape[0] and cue_state < T_ref.shape[0]:\n",
    "                # Aggregate over actions\n",
    "                P_start = _aggregate_actions(_pad_to_shape(T_start, T_ref.shape))\n",
    "                Q_ref = _aggregate_actions(T_ref)\n",
    "                print(f\"Debug (seed {seed}): Start - P[{cue_state}, 15] (rewarded) = {P_start[cue_state, 15]:.4f}, P[{cue_state}, 11] (unrewarded) = {P_start[cue_state, 11]:.4f}\")\n",
    "                print(f\"Debug (seed {seed}): Ref   - Q[{cue_state}, 15] (rewarded) = {Q_ref[cue_state, 15]:.4f}, Q[{cue_state}, 11] (unrewarded) = {Q_ref[cue_state, 11]:.4f}\")\n",
    "            \n",
    "            if len(T_series_ext) > 25:\n",
    "                T_ep25 = T_series_ext[25]\n",
    "                P_ep25 = _aggregate_actions(_pad_to_shape(T_ep25, T_ref.shape))\n",
    "                print(f\"Debug (seed {seed}): Ep25  - P[{cue_state}, 15] (rewarded) = {P_ep25[cue_state, 15]:.4f}, P[{cue_state}, 11] (unrewarded) = {P_ep25[cue_state, 11]:.4f}\")\n",
    "\n",
    "    return dict(KL_acq=KL_acq, JS_acq=JS_acq, H_acq=H_acq, MS_acq=MS_acq,\n",
    "                KL_ext=KL_ext, JS_ext=JS_ext, H_ext=H_ext, MS_ext=MS_ext,\n",
    "                T_acq_final=T_series_acq[-1], T_ext_final=T_series_ext[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always-reward variant: terminal delivers reward regardless of cue/history\n",
    "class GridEnvRightDownAlwaysReward(GridEnvRightDownNoSelf):\n",
    "    def step(self, action):\n",
    "        # Use base dynamics to compute next state & done flag\n",
    "        ns, _, done = super().step(action)\n",
    "        # Force reward at terminal regardless of visited_cue\n",
    "        if done and hasattr(self, \"is_terminal\") and self.is_terminal(ns):\n",
    "            return ns, 1, True\n",
    "        return ns, 0, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Config ---\n",
    "# CUE = 5\n",
    "THRESH = 0.5               # must match env.plot_graph threshold\n",
    "cfg = CoDAConfig(\n",
    "    theta_split=0.6, theta_merge=0.3,\n",
    "    n_threshold=8, min_presence_episodes=3, min_effective_exposure=5.0,\n",
    "    confidence=0.8, \n",
    "    count_decay=0.9, \n",
    "    trace_decay=0.99,    # makes PC recent\n",
    "    # retro_decay=0.9     # makes RC recent\n",
    ")\n",
    "N_SEEDS   = 30\n",
    "SEED0     = 0\n",
    "# N_ACQ     = 250\n",
    "# N_EXT     = 300\n",
    "# MAX_STEPS = 20\n",
    "CUE       = 5\n",
    "\n",
    "# cfg.theta_split = 0.85\n",
    "N_ACQ, N_EXT = 250, 300\n",
    "MAX_STEPS = 20\n",
    "N_ROLL_REF = 300\n",
    "env = GridEnvRightDownNoSelf(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "agent = CoDAAgent(env, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def thresh_adj(T, thr=0.3):\n",
    "    \"\"\"Aggregate over actions and threshold to binary adjacency.\"\"\"\n",
    "    A = T.sum(axis=1)   # [S,S]\n",
    "    if A.ndim != 2:\n",
    "        # handle empty / malformed\n",
    "        return None\n",
    "    return (A >= thr).astype(np.uint8)\n",
    "\n",
    "def clone_dict_tuple(d):\n",
    "    \"\"\"Stable tuple view of clone mapping for change detection.\"\"\"\n",
    "    # sort by clone_id\n",
    "    return tuple(sorted(d.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_changed(prev_T, prev_clone_map, curr_T, curr_clone_map, thr=THRESH):\n",
    "    if prev_T is None:\n",
    "        return True\n",
    "    # shape change (e.g., clones added)\n",
    "    if prev_T.shape != curr_T.shape:\n",
    "        return True\n",
    "    # adjacency change\n",
    "    A_prev = thresh_adj(prev_T, thr=thr)\n",
    "    A_curr = thresh_adj(curr_T, thr=thr)\n",
    "    if A_prev is None or A_curr is None:\n",
    "        return True\n",
    "    if A_prev.shape != A_curr.shape:\n",
    "        return True\n",
    "    if np.any(A_prev != A_curr):\n",
    "        return True\n",
    "    # clone map change\n",
    "    if prev_clone_map != curr_clone_map:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run loops; only plot when changed ---\n",
    "with_clones = False\n",
    "prev_T = None\n",
    "prev_map = None\n",
    "changed_episodes = []\n",
    "\n",
    "# Acquisition\n",
    "for ep in range(1, N_ACQ+1):\n",
    "    if with_clones:\n",
    "        (states, actions) = generate_dataset_post_augmentation(env, agent.get_T(), n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "    else:\n",
    "        (states, actions) = generate_dataset(env, n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "    # maybe here stochastic pairs\n",
    "    transition_probs = agent.get_T()\n",
    "    entropies = compute_transition_entropies(transition_probs)\n",
    "    stochastic_pairs = find_stochastic_state_actions_by_entropy(entropies, eps=1e-9) # (s,a,sprime,sprime2)\n",
    "    agent.update_with_episode(states, actions)\n",
    "    if stochastic_pairs:\n",
    "        unique_outcomes = set()\n",
    "\n",
    "        for (s,a) in stochastic_pairs:\n",
    "            sprime, sprime2 = get_successor_states(agent.transition_counts, s, a)\n",
    "            unique_outcomes.add((sprime, sprime2))\n",
    "        for (sprime,sprime2) in unique_outcomes:\n",
    "            agent.update_eligibility_traces(states, sprime, sprime2)\n",
    "        new = agent.maybe_split()\n",
    "        if new:\n",
    "            with_clones = True\n",
    "            # if with_clones:\n",
    "            #     print(f\"Episode {ep}: Split occurred, now with clones.\")\n",
    "\n",
    "        T_curr = agent.get_T().copy()\n",
    "        map_curr = clone_dict_tuple(env.clone_dict)\n",
    "\n",
    "        if graph_changed(prev_T, prev_map, T_curr, map_curr, thr=THRESH):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            sanitize_for_plot(env, T_curr)\n",
    "            env.plot_graph(T_curr, niter=ep, threshold=THRESH, save=False, savename=f'graph_ep{ep}.png')\n",
    "            changed_episodes.append(ep)\n",
    "            prev_T, prev_map = T_curr, map_curr\n",
    "\n",
    "print(\"Changed episodes (acquisition):\", changed_episodes[:20], \"... total:\", len(changed_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_graph(T_curr, niter=ep, threshold=THRESH, save=False, savename=f'graph_ep{ep}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.salient_cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extinction / degradation\n",
    "THRESH = 0.5\n",
    "prev_T, prev_map = prev_T, prev_map  # reuse from acquisition if you kept them\n",
    "changed_ext = []\n",
    "\n",
    "\n",
    "# Use GridEnvRightDownNoSelf instead of GridEnvRightDownNoCue\n",
    "# env2 = GridEnvRightDownNoSelf(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "# env2.clone_dict = dict(agent.env.clone_dict)\n",
    "# env2.reverse_clone_dict = dict(agent.env.reverse_clone_dict)\n",
    "\n",
    "# # Manually override step to force extinction (no reward) right after creating env2\n",
    "# _orig_step = env2.step\n",
    "# def extinction_step(action):\n",
    "#     next_state, reward, done = _orig_step(action)\n",
    "#     if done:\n",
    "#         # Force extinction: no reward\n",
    "#         reward = -1\n",
    "#         # Swap to unrewarded terminal if needed\n",
    "#         if next_state in env2.rewarded_terminals:\n",
    "#             idx = env2.rewarded_terminals.index(next_state)\n",
    "#             next_state = env2.unrewarded_terminals[idx]\n",
    "#             env2.current_state = next_state\n",
    "#     return next_state, reward, done\n",
    "\n",
    "# env2.step = extinction_step\n",
    "# agent.env = env2\n",
    "\n",
    "\n",
    "env2 = GridEnvRightDownExtinction(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "env2.clone_dict = dict(agent.env.clone_dict)\n",
    "env2.reverse_clone_dict = dict(agent.env.reverse_clone_dict)\n",
    "agent.env = env2\n",
    "\n",
    "for ep in range(N_ACQ+1, N_ACQ+N_EXT+1):\n",
    "    (states, actions) = generate_dataset_post_augmentation(env2, agent.get_T(), n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "    agent.update_with_episode(states, actions)\n",
    "    agent.maybe_merge()\n",
    "\n",
    "    T_curr  = agent.get_T().copy()\n",
    "    map_curr = clone_dict_tuple(env2.clone_dict)\n",
    "\n",
    "    # if graph_changed(prev_T, prev_map, T_curr, map_curr, thr=THRESH):\n",
    "    # (optional) clean terminals/clones just for the figure:\n",
    "    T_vis = make_terminals_absorbing_for_plot(T_curr, env2.rewarded_terminals + env2.unrewarded_terminals)\n",
    "    sanitize_for_plot(env2, T_vis)\n",
    "    env2.plot_graph(T_vis, niter=ep, threshold=THRESH, save=False, savename=f'graph_ep{ep}.png')\n",
    "\n",
    "    changed_ext.append(ep)\n",
    "    prev_T, prev_map = T_curr, map_curr\n",
    "\n",
    "print(\"Extinction changed episodes:\", changed_ext[:30], \"... total:\", len(changed_ext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.retrospective()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.prospective()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Run all experiments and plot KL over time: Acquisition, Extinction, \n",
    "# Latent Inhibition, and Contingency Degradation\n",
    "# ============================================================================\n",
    "\n",
    "# Parameters\n",
    "N_LI_PRE = 300\n",
    "N_LI_ACQ = 300\n",
    "N_CD_ACQ = 300\n",
    "N_CD_DEG = 300\n",
    "N_SEEDS_POSTER = 100 #100  # Use 100 seeds for poster\n",
    "SEEDS_ALL = list(range(N_SEEDS_POSTER))\n",
    "\n",
    "print(\"Running all experiments...\")\n",
    "print(f\"Seeds: {len(SEEDS_ALL)}\")\n",
    "\n",
    "# 1. Acquisition & Extinction (already computed in run_one_seed)\n",
    "print(\"\\n1. Running Acquisition & Extinction...\")\n",
    "results_acq_ext = []\n",
    "for k in range(N_SEEDS_POSTER):\n",
    "    res = run_one_seed(SEED0 + k)\n",
    "    results_acq_ext.append(res)\n",
    "\n",
    "KL_acq_runs = [r[\"KL_acq\"] for r in results_acq_ext]\n",
    "KL_ext_runs = [r[\"KL_ext\"] for r in results_acq_ext]\n",
    "\n",
    "# 2. Latent Inhibition\n",
    "print(\"2. Running Latent Inhibition...\")\n",
    "res_li = run_latent_inhibition_many(cfg, SEEDS_ALL, \n",
    "                                     pre_episodes=N_LI_PRE, \n",
    "                                     acq_episodes=N_LI_ACQ, \n",
    "                                     max_steps=MAX_STEPS, \n",
    "                                     cue=CUE, \n",
    "                                     threshold=THRESH)\n",
    "\n",
    "# 3. Contingency Degradation\n",
    "print(\"3. Running Contingency Degradation...\")\n",
    "res_deg = run_degradation_many(cfg, SEEDS_ALL,\n",
    "                                acq_episodes=N_CD_ACQ,\n",
    "                                degr_episodes=N_CD_DEG,\n",
    "                                max_steps=MAX_STEPS,\n",
    "                                cue=CUE,\n",
    "                                threshold=THRESH)\n",
    "\n",
    "print(\"\\nAll experiments complete! Plotting KL curves...\")\n",
    "\n",
    "# ============================================================================\n",
    "# Plot all KL curves - 4 separate plots for poster\n",
    "# ============================================================================\n",
    "\n",
    "# Set poster-style font sizes - much larger for poster\n",
    "plt.rcParams.update({'font.size': 26, 'axes.titlesize': 36, 'axes.labelsize': 30, \n",
    "                     'xtick.labelsize': 26, 'ytick.labelsize': 26, 'legend.fontsize': 26,\n",
    "                     'figure.titlesize': 36})\n",
    "\n",
    "# 1. Acquisition\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "mean_acq, se_acq = mean_se(KL_acq_runs)\n",
    "x_acq = np.arange(len(mean_acq))\n",
    "ax.plot(x_acq, mean_acq, 'b-', lw=5, label=f\"Acquisition (n={len(KL_acq_runs)})\")\n",
    "ax.fill_between(x_acq, mean_acq - se_acq, mean_acq + se_acq, alpha=0.3, color='blue')\n",
    "ax.set_xlabel(\"Episode\", fontsize=30, fontweight='bold')\n",
    "ax.set_ylabel(\"KL Divergence (nats)\", fontsize=30, fontweight='bold')\n",
    "ax.set_title(\"Acquisition\", fontsize=36, fontweight='bold', pad=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=26)\n",
    "ax.legend(fontsize=26, frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Extinction\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# Acquisition phase\n",
    "mean_acq, se_acq = mean_se(KL_acq_runs)\n",
    "x_acq = np.arange(len(mean_acq))\n",
    "ax.plot(x_acq, mean_acq, 'b-', lw=5, label=f\"Acquisition Phase (n={len(KL_acq_runs)})\")\n",
    "ax.fill_between(x_acq, mean_acq - se_acq, mean_acq + se_acq, alpha=0.3, color='blue')\n",
    "\n",
    "# Extinction phase\n",
    "mean_ext, se_ext = mean_se(KL_ext_runs)\n",
    "x_ext = np.arange(len(mean_ext)) + N_ACQ\n",
    "ax.plot(x_ext, mean_ext, 'r-', lw=5, label=f\"Extinction Phase (n={len(KL_ext_runs)})\")\n",
    "ax.fill_between(x_ext, mean_ext - se_ext, mean_ext + se_ext, alpha=0.3, color='red')\n",
    "ax.axvline(N_ACQ, ls=\"--\", color='gray', alpha=0.6, linewidth=4, label=\"Acquisition → Extinction\")\n",
    "ax.set_xlabel(\"Episode\", fontsize=30, fontweight='bold')\n",
    "ax.set_ylabel(\"KL Divergence (nats)\", fontsize=30, fontweight='bold')\n",
    "ax.set_title(\"Extinction\", fontsize=36, fontweight='bold', pad=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=26)\n",
    "ax.legend(fontsize=26, frameon=True, fancybox=True, shadow=True, loc='best')\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Latent Inhibition\n",
    "# Plot only the acquisition phase, starting from episode 0 (treating acquisition start as episode 0)\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Extract acquisition phase only (after pre-exposure)\n",
    "KL_li_acq = []\n",
    "for kl_series in res_li[\"KL\"]:\n",
    "    if len(kl_series) > N_LI_PRE:\n",
    "        KL_li_acq.append(kl_series[N_LI_PRE:])\n",
    "\n",
    "mean_li, se_li = mean_se(KL_li_acq)\n",
    "x_li = np.arange(len(mean_li))\n",
    "ax.plot(x_li, mean_li, 'r-', lw=5, label=f\"Latent Inhibition (n={len(KL_li_acq)})\")\n",
    "ax.fill_between(x_li, mean_li - se_li, mean_li + se_li, alpha=0.3, color='red')\n",
    "ax.set_xlabel(\"Episode\", fontsize=30, fontweight='bold')\n",
    "ax.set_ylabel(\"KL Divergence (nats)\", fontsize=30, fontweight='bold')\n",
    "ax.set_title(\"Latent Inhibition\", fontsize=36, fontweight='bold', pad=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=26)\n",
    "ax.legend(fontsize=26, frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Contingency Degradation\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# Acquisition phase\n",
    "mean_deg_acq, se_deg_acq = mean_se(res_deg[\"KL_acq\"])\n",
    "x_deg_acq = np.arange(len(mean_deg_acq))\n",
    "ax.plot(x_deg_acq, mean_deg_acq, 'b-', lw=5, label=f\"Acquisition Phase (n={len(res_deg['KL_acq'])})\")\n",
    "ax.fill_between(x_deg_acq, mean_deg_acq - se_deg_acq, mean_deg_acq + se_deg_acq, alpha=0.3, color='blue')\n",
    "\n",
    "# Degradation phase\n",
    "mean_deg, se_deg = mean_se(res_deg[\"KL_deg\"])\n",
    "x_deg = np.arange(len(mean_deg)) + N_CD_ACQ\n",
    "ax.plot(x_deg, mean_deg, 'r-', lw=5, label=f\"Degradation Phase (n={len(res_deg['KL_deg'])})\")\n",
    "ax.fill_between(x_deg, mean_deg - se_deg, mean_deg + se_deg, alpha=0.3, color='red')\n",
    "ax.axvline(N_CD_ACQ, ls=\"--\", color='gray', alpha=0.6, linewidth=4, label=\"Acquisition → Degradation\")\n",
    "ax.set_xlabel(\"Episode\", fontsize=30, fontweight='bold')\n",
    "ax.set_ylabel(\"KL Divergence (nats)\", fontsize=30, fontweight='bold')\n",
    "ax.set_title(\"Contingency Degradation\", fontsize=36, fontweight='bold', pad=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=26)\n",
    "ax.legend(fontsize=26, frameon=True, fancybox=True, shadow=True, loc='best')\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll plots complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run many seeds\n",
    "results = []\n",
    "for k in range(N_SEEDS):\n",
    "    res = run_one_seed(SEED0 + k)\n",
    "    results.append(res)\n",
    "\n",
    "def _pad_stack(arrs):\n",
    "    L = max(len(a) for a in arrs)\n",
    "    M = np.full((len(arrs), L), np.nan)\n",
    "    for i,a in enumerate(arrs):\n",
    "        M[i,:len(a)] = a\n",
    "    return M\n",
    "\n",
    "def mean_se(arrs):\n",
    "    M = _pad_stack(arrs)\n",
    "    mean = np.nanmean(M, axis=0)\n",
    "    se   = np.nanstd(M, axis=0, ddof=max(1,min(len(arrs)-1,1))) / np.sqrt(max(1,len(arrs)))\n",
    "    return mean, se\n",
    "\n",
    "KL_acq_runs = [r[\"KL_acq\"] for r in results]\n",
    "JS_acq_runs = [r[\"JS_acq\"] for r in results]\n",
    "H_acq_runs  = [r[\"H_acq\"]  for r in results]\n",
    "MS_acq_runs = [r[\"MS_acq\"] for r in results]\n",
    "\n",
    "KL_ext_runs = [r[\"KL_ext\"] for r in results]\n",
    "JS_ext_runs = [r[\"JS_ext\"] for r in results]\n",
    "H_ext_runs  = [r[\"H_ext\"]  for r in results]\n",
    "MS_ext_runs = [r[\"MS_ext\"] for r in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_band(y_runs, title, ylabel):\n",
    "    mean, se = mean_se(y_runs)\n",
    "    x = np.arange(len(mean))\n",
    "    plt.plot(x, mean, lw=2.2, label=f\"mean ({len(y_runs)} seeds)\")\n",
    "    plt.fill_between(x, mean-se, mean+se, alpha=0.25, label=\"±1 SE\")\n",
    "    plt.title(title); plt.xlabel(\"Episode\"); plt.ylabel(ylabel); plt.legend(); plt.grid(alpha=0.2)\n",
    "\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_acq_runs, \"KL (acquisition)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_acq_runs, \"JS (acquisition)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_acq_runs,  \"Avg H(S'|S) (acquisition)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_acq_runs, \"Markovization (acquisition)\", \"[0,1]\"); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_ext_runs, \"KL (extinction)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_ext_runs, \"JS (extinction)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_ext_runs,  \"Avg H(S'|S) (extinction)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_ext_runs, \"Markovization (extinction)\", \"[0,1]\"); plt.show()\n",
    "\n",
    "# Visualize ref_fn_acq/ref_fn_ext target transition structures\n",
    "env_acq_vis = GridEnvRightDownNoSelf(cue_states=[CUE], env_size=(4, 4), rewarded_terminal=[15])\n",
    "T_ref_acq_fixed = build_gt_acquisition_with_clones(env_acq_vis, cue_state=CUE)\n",
    "\n",
    "env_ext_vis = GridEnvRightDownNoCue(cue_states=[CUE], env_size=(4, 4), rewarded_terminal=[15])\n",
    "env_ext_vis.clone_dict = dict(getattr(env_acq_vis, \"clone_dict\", {}))\n",
    "env_ext_vis.reverse_clone_dict = dict(getattr(env_acq_vis, \"reverse_clone_dict\", {}))\n",
    "# Build extinction reference with same shape as acquisition (with clones, but zeros for clones)\n",
    "T_ref_ext_fixed = build_gt_extinction_with_clone_shape(env_ext_vis, T_ref_acq_fixed.shape)\n",
    "\n",
    "ref_fn_acq = _ref_fn_fixed(T_ref_acq_fixed)\n",
    "ref_fn_ext = _ref_fn_fixed(T_ref_ext_fixed)\n",
    "\n",
    "T_ref_acq = ref_fn_acq(np.zeros((1, 1, 1)))\n",
    "T_ref_ext = ref_fn_ext(np.zeros((1, 1, 1)))\n",
    "\n",
    "print(\"ref_fn_acq output shape:\", T_ref_acq.shape)\n",
    "print(\"ref_fn_ext output shape:\", T_ref_ext.shape)\n",
    "\n",
    "adj_acq = T_ref_acq.sum(axis=1)\n",
    "adj_ext = T_ref_ext.sum(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "for ax, mat, title in zip(\n",
    "    axes,\n",
    "    [adj_acq, adj_ext],\n",
    "    [\"Reference transitions (acquisition)\", \"Reference transitions (extinction)\"]\n",
    "):\n",
    "    im = ax.imshow(mat, cmap=\"magma\", vmin=0.0, vmax=1.0)\n",
    "    ax.set_xlabel(\"next state\")\n",
    "    ax.set_ylabel(\"current state\")\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure results include the final transition tensors before plotting\n",
    "if not results or \"T_acq_final\" not in results[-1] or \"T_ext_final\" not in results[-1]:\n",
    "    results = []\n",
    "    for k in range(N_SEEDS):\n",
    "        results.append(run_one_seed(SEED0 + k))\n",
    "\n",
    "def plot_band(y_runs, title, ylabel):\n",
    "    mean, se = mean_se(y_runs)\n",
    "    x = np.arange(len(mean))\n",
    "    plt.plot(x, mean, lw=2.2, label=f\"mean ({len(y_runs)} seeds)\")\n",
    "    plt.fill_between(x, mean-se, mean+se, alpha=0.25, label=\"±1 SE\")\n",
    "    plt.title(title); plt.xlabel(\"Episode\"); plt.ylabel(ylabel); plt.legend(); plt.grid(alpha=0.2)\n",
    "\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_acq_runs, \"KL (acquisition)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_acq_runs, \"JS (acquisition)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_acq_runs,  \"Avg H(S'|S) (acquisition)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_acq_runs, \"Markovization (acquisition)\", \"[0,1]\"); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_ext_runs, \"KL (extinction)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_ext_runs, \"JS (extinction)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_ext_runs,  \"Avg H(S'|S) (extinction)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_ext_runs, \"Markovization (extinction)\", \"[0,1]\"); plt.show()\n",
    "\n",
    "# Visualize ref_fn_acq/ref_fn_ext target transition structures\n",
    "env_acq_vis = GridEnvRightDownNoSelf(cue_states=[CUE], env_size=(4, 4), rewarded_terminal=[15])\n",
    "T_ref_acq_fixed = build_gt_acquisition_with_clones(env_acq_vis, cue_state=CUE)\n",
    "\n",
    "env_ext_vis = GridEnvRightDownNoCue(cue_states=[CUE], env_size=(4, 4), rewarded_terminal=[15])\n",
    "env_ext_vis.clone_dict = dict(getattr(env_acq_vis, \"clone_dict\", {}))\n",
    "env_ext_vis.reverse_clone_dict = dict(getattr(env_acq_vis, \"reverse_clone_dict\", {}))\n",
    "# Build extinction reference with same shape as acquisition (with clones, but zeros for clones)\n",
    "T_ref_ext_fixed = build_gt_extinction_with_clone_shape(env_ext_vis, T_ref_acq_fixed.shape)\n",
    "\n",
    "ref_fn_acq = _ref_fn_fixed(T_ref_acq_fixed)\n",
    "ref_fn_ext = _ref_fn_fixed(T_ref_ext_fixed)\n",
    "\n",
    "T_ref_acq = ref_fn_acq(np.zeros((1, 1, 1)))\n",
    "T_ref_ext = ref_fn_ext(np.zeros((1, 1, 1)))\n",
    "\n",
    "print(\"ref_fn_acq output shape:\", T_ref_acq.shape)\n",
    "print(\"ref_fn_ext output shape:\", T_ref_ext.shape)\n",
    "\n",
    "adj_acq = T_ref_acq.sum(axis=1)\n",
    "adj_ext = T_ref_ext.sum(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "for ax, mat, title in zip(\n",
    "    axes,\n",
    "    [adj_acq, adj_ext],\n",
    "    [\"Reference transitions (acquisition)\", \"Reference transitions (extinction)\"]\n",
    "):\n",
    "    im = ax.imshow(mat, cmap=\"magma\", vmin=0.0, vmax=1.0)\n",
    "    ax.set_xlabel(\"next state\")\n",
    "    ax.set_ylabel(\"current state\")\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "def plot_transition_tensor(T, title_prefix):\n",
    "    if T is None:\n",
    "        print(f\"No transition matrix available for {title_prefix}.\")\n",
    "        return\n",
    "    n_actions = T.shape[1]\n",
    "    fig, axes = plt.subplots(1, n_actions, figsize=(4 * n_actions, 4), constrained_layout=True)\n",
    "    if n_actions == 1:\n",
    "        axes = [axes]\n",
    "    for a_idx, ax in enumerate(axes):\n",
    "        im = ax.imshow(T[:, a_idx, :], cmap=\"magma\", vmin=0.0, vmax=1.0)\n",
    "        ax.set_title(f\"{title_prefix} — action {a_idx}\")\n",
    "        ax.set_xlabel(\"next state\")\n",
    "        ax.set_ylabel(\"current state\")\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.show()\n",
    "\n",
    "    avg_prob = T.mean(axis=1)\n",
    "    adj_mask = (avg_prob >= THRESH).astype(int)\n",
    "    fig2, (ax_prob, ax_adj) = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True)\n",
    "    im_prob = ax_prob.imshow(avg_prob, cmap=\"magma\", vmin=0.0, vmax=1.0)\n",
    "    ax_prob.set_title(f\"{title_prefix} — mean over actions\")\n",
    "    ax_prob.set_xlabel(\"next state\")\n",
    "    ax_prob.set_ylabel(\"current state\")\n",
    "    fig2.colorbar(im_prob, ax=ax_prob, fraction=0.046, pad=0.04)\n",
    "\n",
    "    im_adj = ax_adj.imshow(adj_mask, cmap=\"gray_r\", vmin=0, vmax=1)\n",
    "    ax_adj.set_title(f\"{title_prefix} — adjacency (≥ {THRESH})\")\n",
    "    ax_adj.set_xlabel(\"next state\")\n",
    "    ax_adj.set_ylabel(\"current state\")\n",
    "    fig2.colorbar(im_adj, ax=ax_adj, fraction=0.046, pad=0.04)\n",
    "    plt.show()\n",
    "\n",
    "plot_transition_tensor(results[-1][\"T_acq_final\"], \"Learned acquisition T (final episode)\")\n",
    "plot_transition_tensor(results[-1][\"T_ext_final\"], \"Learned extinction T (final episode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters (paper-style) ---\n",
    "N_LI_PRE = 500\n",
    "N_LI_ACQ = 1000\n",
    "N_CD_ACQ = 1000\n",
    "N_CD_DEG = 1000\n",
    "SEEDS    = list(range(30))  # 30 seeds\n",
    "\n",
    "# Use your v2 acquisition cfg (keep it as you’ve set earlier)\n",
    "cfg_li = CoDAConfig()  # latent inhibition relies on v2-like gating & uncertainty\n",
    "cfg_cd = CoDAConfig()  # degradation uses same; merges depend on retrospective fall\n",
    "\n",
    "# -------- Latent inhibition --------\n",
    "res_li = run_latent_inhibition_many(cfg_li, SEEDS, pre_episodes=N_LI_PRE, acq_episodes=N_LI_ACQ, max_steps=MAX_STEPS, cue=CUE)\n",
    "plot_latent_inhibition_summary(res_li, pre_episodes=N_LI_PRE)\n",
    "\n",
    "# -------- Contingency degradation --------\n",
    "res_deg = run_degradation_many(cfg_cd, SEEDS, acq_episodes=N_CD_ACQ, degr_episodes=N_CD_DEG, max_steps=MAX_STEPS, cue=CUE)\n",
    "# plot_degradation_summary(res_deg)\n",
    "\n",
    "# ===== Contingency degradation: plots in the same style as acquisition/extinction =====\n",
    "\n",
    "# Unpack runs\n",
    "KL_acq_runs = res_deg[\"KL_acq\"]\n",
    "JS_acq_runs = res_deg[\"JS_acq\"]\n",
    "H_acq_runs  = res_deg[\"H_acq\"]\n",
    "MS_acq_runs = res_deg[\"MS_acq\"]\n",
    "\n",
    "KL_deg_runs = res_deg[\"KL_deg\"]\n",
    "JS_deg_runs = res_deg[\"JS_deg\"]\n",
    "H_deg_runs  = res_deg[\"H_deg\"]\n",
    "MS_deg_runs = res_deg[\"MS_deg\"]\n",
    "\n",
    "# Reuse your mean±SE band helper\n",
    "def mean_se(arrs):\n",
    "    L = max(len(a) for a in arrs)\n",
    "    M = np.full((len(arrs), L), np.nan)\n",
    "    for i, a in enumerate(arrs):\n",
    "        M[i, :len(a)] = a\n",
    "    mean = np.nanmean(M, axis=0)\n",
    "    se   = np.nanstd(M, axis=0, ddof=max(1, min(len(arrs)-1, 1))) / np.sqrt(max(1, len(arrs)))\n",
    "    return mean, se\n",
    "\n",
    "def plot_band(y_runs, title, ylabel):\n",
    "    mean, se = mean_se(y_runs)\n",
    "    x = np.arange(len(mean))\n",
    "    plt.plot(x, mean, lw=2.2, label=f\"mean ({len(y_runs)} seeds)\")\n",
    "    plt.fill_between(x, mean-se, mean+se, alpha=0.25, label=\"±1 SE\")\n",
    "    plt.title(title); plt.xlabel(\"Episode\"); plt.ylabel(ylabel); plt.legend(); plt.grid(alpha=0.2)\n",
    "\n",
    "# Acquisition portion (degradation runs, pre-switch)\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_acq_runs, \"KL (acquisition)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_acq_runs, \"JS (acquisition)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_acq_runs,  \"Avg H(S'|S) (acquisition)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_acq_runs, \"Markovization (acquisition)\", \"[0,1]\"); plt.show()\n",
    "\n",
    "# Degradation portion (post-switch)\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_deg_runs, \"KL (contingency degradation)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_deg_runs, \"JS (contingency degradation)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_deg_runs,  \"Avg H(S'|S) (contingency degradation)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_deg_runs, \"Markovization (contingency degradation)\", \"[0,1]\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH: Fixed version that populates clone_dict for proper graph visualization\n",
    "def build_gt_acquisition_with_clones(env, cue_state: int) -> np.ndarray:\n",
    "    S0 = env.num_unique_states\n",
    "    A  = _n_actions(env)\n",
    "    T0 = _build_base_T(env)\n",
    "\n",
    "    terminals = set(env.rewarded_terminals) | set(env.unrewarded_terminals)\n",
    "    rewT = env.rewarded_terminals\n",
    "    unrewT = env.unrewarded_terminals\n",
    "\n",
    "    D = _descendants_until_terminal(env, cue_state, terminals)\n",
    "    clone_of = {orig: S0 + k for k, orig in enumerate(sorted(D))}\n",
    "    S = S0 + len(clone_of)\n",
    "    T = np.zeros((S, A, S), dtype=float)\n",
    "    T[:S0, :, :S0] = T0\n",
    "\n",
    "    for a in env.valid_actions[cue_state]:\n",
    "        sp = _base_successor(env, cue_state, a)\n",
    "        if sp in D:\n",
    "            T[cue_state, a, sp] = 0.0\n",
    "            T[cue_state, a, clone_of[sp]] = 1.0\n",
    "\n",
    "    for orig, cl in clone_of.items():\n",
    "        for a in env.valid_actions[orig]:\n",
    "            sp = _base_successor(env, orig, a)\n",
    "            if sp in terminals:\n",
    "                for t in rewT:   T[cl, a, t] = 1.0\n",
    "                for t in unrewT: T[cl, a, t] = 0.0\n",
    "            else:\n",
    "                T[cl, a, clone_of[sp] if sp in clone_of else sp] = 1.0\n",
    "\n",
    "        for a in env.valid_actions[orig]:\n",
    "            sp = _base_successor(env, orig, a)\n",
    "            if sp in rewT:\n",
    "                T[orig, a, sp] = 0.0\n",
    "                idx = rewT.index(sp)\n",
    "                T[orig, a, unrewT[idx]] = 1.0\n",
    "    \n",
    "    # KEY FIX: Populate clone_dict so plotting overlays clones on their parent states\n",
    "    for orig, cl in clone_of.items():\n",
    "        env.clone_dict[cl] = orig  # Maps clone_id -> original_state_id\n",
    "    env.reverse_clone_dict = {parent: cl for cl, parent in env.clone_dict.items()}\n",
    "    \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect T snapshots during acquisition and extinction.\n",
    "# If you already recorded them earlier, just reuse those lists.\n",
    "T_series_acq = []\n",
    "T_series_ext = []\n",
    "\n",
    "# Re-run quick pass to collect snapshots only (no plotting) -- uses your existing variables:\n",
    "# Acquisition\n",
    "with_clones = False\n",
    "for ep in range(1, N_ACQ+1):\n",
    "    if with_clones:\n",
    "        (states, actions) = generate_dataset_post_augmentation(env, agent.get_T(), n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "    else:\n",
    "        (states, actions) = generate_dataset(env, n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "    agent.update_with_episode(states, actions)\n",
    "    if agent.maybe_split():\n",
    "        with_clones = True\n",
    "    T_series_acq.append(agent.get_T().copy())\n",
    "\n",
    "# Extinction\n",
    "env2 = GridEnvRightDownNoCue(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "env2.clone_dict = dict(getattr(env, \"clone_dict\", {}))\n",
    "env2.reverse_clone_dict = dict(getattr(env, \"reverse_clone_dict\", {}))\n",
    "agent.env = env2\n",
    "\n",
    "for ep in range(N_ACQ+1, N_ACQ+N_EXT+1):\n",
    "    (states, actions) = generate_dataset_post_augmentation(env2, agent.get_T(), n_episodes=1, max_steps=MAX_STEPS)[0]\n",
    "    agent.update_with_episode(states, actions)\n",
    "    agent.maybe_merge()\n",
    "    T_series_ext.append(agent.get_T().copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learned vs ground-truth representations\n",
    "\n",
    "# Check if T_series exist, otherwise run the data collection first\n",
    "if 'T_series_acq' not in locals() or 'T_series_ext' not in locals():\n",
    "    print(\"ERROR: Please run cell 19 first to generate T_series_acq and T_series_ext\")\n",
    "    print(\"Cell 19 is the one that starts with: # Collect T snapshots during acquisition and extinction.\")\n",
    "else:\n",
    "    # Get final learned representations\n",
    "    T_learned_acq = T_series_acq[-1].copy()\n",
    "    T_learned_ext = T_series_ext[-1].copy()\n",
    "\n",
    "    # Build ground-truth representations\n",
    "    T_gt_acq = build_gt_acquisition_with_clones(env, cue_state=CUE)\n",
    "    T_gt_ext = build_gt_extinction_no_clones(env2)\n",
    "\n",
    "    print(f\"Learned acquisition graph shape: {T_learned_acq.shape}\")\n",
    "    print(f\"Ground-truth acquisition graph shape: {T_gt_acq.shape}\")\n",
    "    print(f\"Learned extinction graph shape: {T_learned_ext.shape}\")\n",
    "    print(f\"Ground-truth extinction graph shape: {T_gt_ext.shape}\")\n",
    "\n",
    "    # Compute comparison metrics\n",
    "    def compare_graphs(T_learned, T_gt, name=\"\"):\n",
    "        # Pad to same shape if needed\n",
    "        max_shape = tuple(max(s1, s2) for s1, s2 in zip(T_learned.shape, T_gt.shape))\n",
    "        T_l = _pad3(T_learned, max_shape)\n",
    "        T_g = _pad3(T_gt, max_shape)\n",
    "        \n",
    "        # Aggregate over actions for adjacency\n",
    "        A_l = T_l.sum(axis=1)\n",
    "        A_g = T_g.sum(axis=1)\n",
    "        \n",
    "        # Binary adjacency at threshold\n",
    "        B_l = (A_l >= THRESH).astype(int)\n",
    "        B_g = (A_g >= THRESH).astype(int)\n",
    "        \n",
    "        # Metrics\n",
    "        edge_match = np.sum(B_l == B_g) / B_l.size\n",
    "        l1_dist = np.mean(np.abs(A_l - A_g))\n",
    "        kl_div = np.sum(np.where((A_g > 0) & (A_l > 0), \n",
    "                                 A_g * np.log((A_g + 1e-10) / (A_l + 1e-10)), 0))\n",
    "        \n",
    "        print(f\"\\n{name} Comparison:\")\n",
    "        print(f\"  Edge match rate: {edge_match:.3f}\")\n",
    "        print(f\"  L1 distance (adjacency): {l1_dist:.4f}\")\n",
    "        print(f\"  KL divergence: {kl_div:.4f}\")\n",
    "        \n",
    "        return {\"edge_match\": edge_match, \"l1\": l1_dist, \"kl\": kl_div}\n",
    "\n",
    "    metrics_acq = compare_graphs(T_learned_acq, T_gt_acq, \"Acquisition\")\n",
    "    metrics_ext = compare_graphs(T_learned_ext, T_gt_ext, \"Extinction\")\n",
    "\n",
    "    # Visualize learned vs ground-truth (separate figures)\n",
    "    \n",
    "    # Acquisition: Ground-truth\n",
    "    # print(\"\\n=== Ground-Truth (Acquisition) ===\")\n",
    "    # T_vis_gt_acq = make_terminals_absorbing_for_plot(T_gt_acq, env.rewarded_terminals + env.unrewarded_terminals)\n",
    "    # env_temp = GridEnvRightDownNoSelf(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "    # env_temp.plot_graph(T_vis_gt_acq, niter=N_ACQ, threshold=THRESH, save=False)\n",
    "    # plt.title(f\"Ground-Truth (Acquisition, ep={N_ACQ})\", fontsize=12, fontweight='bold')\n",
    "    # plt.show()\n",
    "\n",
    "    # Acquisition: Ground-truth\n",
    "    print(\"\\n=== Ground-Truth (Acquisition) ===\")\n",
    "    T_vis_gt_acq = make_terminals_absorbing_for_plot(T_gt_acq, env.rewarded_terminals + env.unrewarded_terminals)\n",
    "    env_temp = GridEnvRightDownNoSelf(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "    env_temp.clone_dict = dict(env.clone_dict)  # ADD THIS LINE\n",
    "    env_temp.reverse_clone_dict = dict(env.reverse_clone_dict)  # ADD THIS LINE TOO\n",
    "    env_temp.plot_graph(T_vis_gt_acq, niter=N_ACQ, threshold=THRESH, save=False, title=f\"Ground-Truth (Acquisition, ep={N_ACQ})\")\n",
    "    plt.show()\n",
    "\n",
    "    # Acquisition: Learned\n",
    "    print(\"\\n=== Learned (Acquisition) ===\")\n",
    "    T_vis_learned_acq = make_terminals_absorbing_for_plot(T_learned_acq, env.rewarded_terminals + env.unrewarded_terminals)\n",
    "    sanitize_for_plot(env, T_vis_learned_acq)\n",
    "    env.plot_graph(T_vis_learned_acq, niter=N_ACQ, threshold=THRESH, save=False, title=f\"Learned (Acquisition, ep={N_ACQ})\")\n",
    "    plt.show()\n",
    "\n",
    "    # Extinction: Ground-truth\n",
    "    print(\"\\n=== Ground-Truth (Extinction) ===\")\n",
    "    T_vis_gt_ext = make_terminals_absorbing_for_plot(T_gt_ext, env2.rewarded_terminals + env2.unrewarded_terminals)\n",
    "    env2_temp = GridEnvRightDownNoCue(cue_states=[CUE], env_size=(4,4), rewarded_terminal=[15])\n",
    "    env2_temp.clone_dict = dict(env2.clone_dict)  # ADD THIS LINE\n",
    "    env2_temp.reverse_clone_dict = dict(env2.reverse_clone_dict)  # ADD THIS LINE TOO\n",
    "    env2_temp.plot_graph(T_vis_gt_ext, niter=N_ACQ+N_EXT, threshold=THRESH, save=False, title=f\"Ground-Truth (Extinction, ep={N_ACQ+N_EXT})\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # Extinction: Learned\n",
    "    print(\"\\n=== Learned (Extinction) ===\")\n",
    "    T_vis_learned_ext = make_terminals_absorbing_for_plot(T_learned_ext, env2.rewarded_terminals + env2.unrewarded_terminals)\n",
    "    sanitize_for_plot(env2, T_vis_learned_ext)\n",
    "    env2.plot_graph(T_vis_learned_ext, niter=N_ACQ+N_EXT, threshold=THRESH, save=False, title=f\"Learned (Extinction, ep={N_ACQ+N_EXT})\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot comparison metrics as bar charts\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    metrics_list = ['edge_match', 'l1', 'kl']\n",
    "    titles = ['Edge Match Rate', 'L1 Distance', 'KL Divergence']\n",
    "    ylabels = ['Match Rate', 'L1', 'KL (nats)']\n",
    "\n",
    "    for idx, (metric, title, ylabel) in enumerate(zip(metrics_list, titles, ylabels)):\n",
    "        ax = axes[idx]\n",
    "        values = [metrics_acq[metric], metrics_ext[metric]]\n",
    "        bars = ax.bar(['Acquisition', 'Extinction'], values, color=['#1f77b4', '#ff7f0e'], alpha=0.7)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Now, degradation and latent inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always-reward variant: terminal delivers reward regardless of cue/history\n",
    "class GridEnvRightDownAlwaysReward(GridEnvRightDownNoSelf):\n",
    "    def step(self, action):\n",
    "        # Use base dynamics to compute next state & done flag\n",
    "        ns, _, done = super().step(action)\n",
    "        # Force reward at terminal regardless of visited_cue\n",
    "        if done and hasattr(self, \"is_terminal\") and self.is_terminal(ns):\n",
    "            return ns, 1, True\n",
    "        return ns, 0, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Analytic ground-truth builders (ACQUISITION / EXTINCTION / DEGRADATION)\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "\n",
    "def _base_T_from_env(env):\n",
    "    \"\"\"\n",
    "    Deterministic [S, A, S] from grid geometry (no clones).\n",
    "    Assumes env.valid_actions, env.state_to_pos, env.pos_to_state, env.base_actions are defined.\n",
    "    Terminals are set absorbing (no outgoing mass).\n",
    "    \"\"\"\n",
    "    S = int(env.num_unique_states)\n",
    "    A = max(a for acts in env.valid_actions.values() for a in acts) + 1 if env.valid_actions else 0\n",
    "    T = np.zeros((S, A, S), dtype=float)\n",
    "\n",
    "    # deterministic right/down moves\n",
    "    for s, acts in env.valid_actions.items():\n",
    "        for a in acts:\n",
    "            i, j = env.state_to_pos[s]\n",
    "            di, dj = env.base_actions[a]\n",
    "            ni, nj = i + di, j + dj\n",
    "            sp = env.pos_to_state[(ni, nj)]\n",
    "            T[s, a, sp] = 1.0\n",
    "\n",
    "    # terminals absorbing\n",
    "    for t in (env.rewarded_terminals + env.unrewarded_terminals):\n",
    "        if 0 <= t < S:\n",
    "            T[t, :, :] = 0.0\n",
    "    return T\n",
    "\n",
    "def gt_acquisition_with_clones(env, cue_states):\n",
    "    \"\"\"\n",
    "    Acquisition GT: Markovian map created by cloning the *successors* of each cue,\n",
    "    and redirecting cue->successor edges to cue->clone(successor). Clones inherit\n",
    "    their parent successor's outgoing transitions.\n",
    "    Returns [S_gt, A, S_gt]; S_gt >= original S.\n",
    "    \"\"\"\n",
    "    T = _base_T_from_env(env)\n",
    "    S, A, _ = T.shape\n",
    "    successor_to_clone = {}\n",
    "\n",
    "    for cue in cue_states:\n",
    "        for a in env.get_valid_actions(cue):\n",
    "            i, j = env.state_to_pos[cue]\n",
    "            di, dj = env.base_actions[a]\n",
    "            ni, nj = i + di, j + dj\n",
    "            sp = env.pos_to_state[(ni, nj)]\n",
    "\n",
    "            # reuse clone if this successor already cloned\n",
    "            if sp in successor_to_clone:\n",
    "                cl = successor_to_clone[sp]\n",
    "            else:\n",
    "                cl = S\n",
    "                S += 1\n",
    "                # grow T by 1 state (pad last axis and first axis consistently)\n",
    "                T = np.pad(T, ((0,1),(0,0),(0,1)), mode='constant')\n",
    "                # clone inherits outgoing from parent successor (NOTE: parent row is in old slice [:, :S-1])\n",
    "                T[cl, :, :T.shape[2]-1] = T[sp, :, :T.shape[2]-1]\n",
    "                successor_to_clone[sp] = cl\n",
    "\n",
    "            # redirect cue->sp to cue->cl\n",
    "            mass = T[cue, a, sp]\n",
    "            T[cue, a, sp] = 0.0\n",
    "            T[cue, a, cl] = mass\n",
    "\n",
    "    # terminals absorbing (safety)\n",
    "    for t in (env.rewarded_terminals + env.unrewarded_terminals):\n",
    "        if 0 <= t < T.shape[0]:\n",
    "            T[t, :, :] = 0.0\n",
    "    return T\n",
    "\n",
    "def gt_extinction_no_clones(env2):\n",
    "    \"\"\"\n",
    "    Extinction GT: no clones; terminal transitions *always* go to the NON-reward terminal.\n",
    "    We redirect any incoming mass to rewarded terminal -> unrewarded terminal.\n",
    "    \"\"\"\n",
    "    T = _base_T_from_env(env2)\n",
    "    # map rewarded terminal(s) to paired unrewarded terminal(s)\n",
    "    for idx, rt in enumerate(env2.rewarded_terminals):\n",
    "        if idx < len(env2.unrewarded_terminals):\n",
    "            nt = env2.unrewarded_terminals[idx]\n",
    "            mask = T[:, :, rt] > 0\n",
    "            T[:, :, nt][mask] = T[:, :, rt][mask]\n",
    "            T[:, :, rt][mask] = 0.0\n",
    "    return T\n",
    "\n",
    "def gt_degradation_no_clones(env_always_reward):\n",
    "    \"\"\"\n",
    "    Contingency degradation GT: reward is given regardless of cue/history.\n",
    "    Structural GT is simply the base right/down graph with terminals absorbing (no clones).\n",
    "    \"\"\"\n",
    "    return _base_T_from_env(env_always_reward)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Backwards compatibility: provide 'build_*' names\n",
    "# so existing code that calls build_gt_* keeps working.\n",
    "# ------------------------------------------\n",
    "def build_gt_acquisition_with_clones(env, cue_state):\n",
    "    return gt_acquisition_with_clones(env, cue_states=[cue_state])\n",
    "\n",
    "def build_gt_extinction_no_clones(env2):\n",
    "    return gt_extinction_no_clones(env2)\n",
    "\n",
    "def build_gt_degradation_no_clones(env_always_reward):\n",
    "    return gt_degradation_no_clones(env_always_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_degradation_no_clones(env_always_reward):\n",
    "    \"\"\"\n",
    "    Degradation GT: no clones; transitions are the base right/down grid transitions.\n",
    "    Reward is always delivered at the (rewarded) terminal, but that does not change the\n",
    "    transition *structure*, so this is simply the base graph without clones.\n",
    "    \"\"\"\n",
    "    T = base_T_from_env(env_always_reward)\n",
    "    # Terminals have no outgoing edges; incoming edges unchanged (deterministic).\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_latent_inhibition_seed(seed:int,\n",
    "                               cfg:CoDAConfig,\n",
    "                               pre_episodes:int = 500,\n",
    "                               acq_episodes:int = 1000,\n",
    "                               max_steps:int = 20,\n",
    "                               cue:int = 5):\n",
    "    \"\"\"\n",
    "    Phase 1 (pre-exposure):  no reward (extinction-like) to inflate P(US & ~CS)\n",
    "    Phase 2 (acquisition):   normal cued task; splitting should be delayed\n",
    "    Metrics are computed across the entire run vs. the acquisition GT-with-clones.\n",
    "    References: paper pp. 3–4, Fig. 3b; Methods p.8 (protocol & rationale).  [oai_citation:1‡Learning_the_structure_of_aliased_state_spaces_in_non_Markovian_tasks (1).pdf](sediment://file_00000000335c722fbbd1dce642c2838e)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # -------- Pre-exposure (no reward) --------\n",
    "    env_pre = GridEnvRightDownNoCue(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent   = CoDAAgent(env_pre, cfg)\n",
    "\n",
    "    T_series = []\n",
    "\n",
    "    for ep in range(1, pre_episodes+1):\n",
    "        (states, actions) = generate_dataset(env_pre, n_episodes=1, max_steps=max_steps)[0]\n",
    "        agent.update_with_episode(states, actions)\n",
    "        # No splitting expected (no US); still collect T\n",
    "        T_series.append(agent.get_T().copy())\n",
    "\n",
    "    # -------- Acquisition (normal cued task) --------\n",
    "    env_acq = GridEnvRightDownNoSelf(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent.env = env_acq  # keep uncertainty/accumulators; switch to cued env\n",
    "\n",
    "    with_clones = False\n",
    "    for ep in range(1, acq_episodes+1):\n",
    "        if with_clones:\n",
    "            (states, actions) = generate_dataset_post_augmentation(env_acq, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        else:\n",
    "            (states, actions) = generate_dataset(env_acq, n_episodes=1, max_steps=max_steps)[0]\n",
    "\n",
    "        agent.update_with_episode(states, actions)\n",
    "        new = agent.maybe_split()\n",
    "        if new:\n",
    "            with_clones = True\n",
    "\n",
    "        T_series.append(agent.get_T().copy())\n",
    "\n",
    "    # GT for latent inhibition evaluation: acquisition graph with clones\n",
    "    T_ref = gt_acquisition_with_clones(env_acq, cue_states=[cue])\n",
    "\n",
    "    # Metrics over time (full series vs acquisition GT)\n",
    "    KL = kl_over_time_fixed(T_series, T_ref, use_js=False)\n",
    "    JS = kl_over_time_fixed(T_series, T_ref, use_js=True)\n",
    "    H  = entropy_over_time(T_series)\n",
    "    MS = np.array([markovization_score(T) for T in T_series])\n",
    "\n",
    "    return dict(T_series=T_series, T_ref=T_ref, KL=KL, JS=JS, H=H, MS=MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_contingency_degradation_seed(seed:int,\n",
    "                                     cfg:CoDAConfig,\n",
    "                                     acq_episodes:int = 1000,\n",
    "                                     degr_episodes:int = 1000,\n",
    "                                     max_steps:int = 20,\n",
    "                                     cue:int = 5,\n",
    "                                     wash_in:int = 50,\n",
    "                                     edge_eps_early:float = 1e-4,\n",
    "                                     edge_eps_late:float  = 1e-6):\n",
    "    \"\"\"\n",
    "    Phase 1 (acquisition):      normal cued task (splits form).\n",
    "    Phase 2 (degradation):      reward at terminal regardless of cue/history (always reward).\n",
    "                                RC falls while PC~1, so clones should merge.\n",
    "    We report metrics separately for acq (vs acq GT-with-clones) and degr (vs degr GT no-clones).\n",
    "    References: paper p.4 (Contingency degradation; Eq. 3); Methods p.8.  [oai_citation:2‡Learning_the_structure_of_aliased_state_spaces_in_non_Markovian_tasks (1).pdf](sediment://file_00000000335c722fbbd1dce642c2838e)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # -------- Acquisition --------\n",
    "    env_acq = GridEnvRightDownNoSelf(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent   = CoDAAgent(env_acq, cfg)\n",
    "\n",
    "    T_series_acq = []\n",
    "    with_clones = False\n",
    "\n",
    "    for ep in range(1, acq_episodes+1):\n",
    "        if with_clones:\n",
    "            (states, actions) = generate_dataset_post_augmentation(env_acq, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        else:\n",
    "            (states, actions) = generate_dataset(env_acq, n_episodes=1, max_steps=max_steps)[0]\n",
    "\n",
    "        agent.update_with_episode(states, actions)\n",
    "        if agent.maybe_split():\n",
    "            with_clones = True\n",
    "\n",
    "        T_series_acq.append(agent.get_T().copy())\n",
    "\n",
    "    T_ref_acq = gt_acquisition_with_clones(env_acq, cue_states=[cue])\n",
    "\n",
    "    # -------- Degradation (always reward; no reset) --------\n",
    "    env_deg = GridEnvRightDownAlwaysReward(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    # carry learned clones into degradation env so merges are meaningful\n",
    "    env_deg.clone_dict = dict(getattr(env_acq, \"clone_dict\", {}))\n",
    "    env_deg.reverse_clone_dict = dict(getattr(env_acq, \"reverse_clone_dict\", {}))\n",
    "    agent.env = env_deg\n",
    "\n",
    "    # short wash-in to encourage structural merges early (optional)\n",
    "    orig = dict(count_decay=agent.cfg.count_decay, trace_decay=agent.cfg.trace_decay, retro_decay=agent.cfg.retro_decay,\n",
    "                theta_merge=agent.cfg.theta_merge, confidence=agent.cfg.confidence,\n",
    "                min_presence_episodes=agent.cfg.min_presence_episodes,\n",
    "                min_effective_exposure=agent.cfg.min_effective_exposure)\n",
    "\n",
    "    agent.cfg.count_decay = 0.98\n",
    "    agent.cfg.trace_decay = 0.98\n",
    "    agent.cfg.retro_decay = 0.98\n",
    "    agent.cfg.theta_merge = 0.60\n",
    "    agent.cfg.confidence  = 0.99\n",
    "    agent.cfg.min_presence_episodes += 3\n",
    "    agent.cfg.min_effective_exposure = int(agent.cfg.min_effective_exposure * 1.5)\n",
    "\n",
    "    T_series_deg = []\n",
    "    for k in range(degr_episodes):\n",
    "        (states, actions) = generate_dataset_post_augmentation(env_deg, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        agent.update_with_episode(states, actions)\n",
    "        agent._edge_eps_override = edge_eps_early if k < wash_in else edge_eps_late\n",
    "        agent.maybe_merge()\n",
    "        T_series_deg.append(agent.get_T().copy())\n",
    "\n",
    "        if k == wash_in - 1:\n",
    "            # restore original cfg after wash-in\n",
    "            for key, val in orig.items():\n",
    "                setattr(agent.cfg, key, val)\n",
    "\n",
    "    T_ref_deg = gt_degradation_no_clones(env_deg)\n",
    "\n",
    "    # Metrics (separate for each phase)\n",
    "    KL_acq = kl_over_time_fixed(T_series_acq, T_ref_acq, use_js=False)\n",
    "    JS_acq = kl_over_time_fixed(T_series_acq, T_ref_acq, use_js=True)\n",
    "    H_acq  = entropy_over_time(T_series_acq)\n",
    "    MS_acq = np.array([markovization_score(T) for T in T_series_acq])\n",
    "\n",
    "    KL_deg = kl_over_time_fixed(T_series_deg, T_ref_deg, use_js=False)\n",
    "    JS_deg = kl_over_time_fixed(T_series_deg, T_ref_deg, use_js=True)\n",
    "    H_deg  = entropy_over_time(T_series_deg)\n",
    "    MS_deg = np.array([markovization_score(T) for T in T_series_deg])\n",
    "\n",
    "    return dict(\n",
    "        env_acq=env_acq, env_deg=env_deg,\n",
    "        T_series_acq=T_series_acq, T_series_deg=T_series_deg,\n",
    "        T_ref_acq=T_ref_acq, T_ref_deg=T_ref_deg,\n",
    "        KL_acq=KL_acq, JS_acq=JS_acq, H_acq=H_acq, MS_acq=MS_acq,\n",
    "        KL_deg=KL_deg, JS_deg=JS_deg, H_deg=H_deg, MS_deg=MS_deg\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Latent inhibition: multi-seed --------\n",
    "def run_latent_inhibition_many(cfg:CoDAConfig,\n",
    "                               seeds:list,\n",
    "                               pre_episodes:int=500,\n",
    "                               acq_episodes:int=1000,\n",
    "                               max_steps:int=20,\n",
    "                               cue:int=5):\n",
    "    runs = [run_latent_inhibition_seed(s, cfg, pre_episodes, acq_episodes, max_steps, cue) for s in seeds]\n",
    "    KL_runs = [r[\"KL\"] for r in runs]\n",
    "    JS_runs = [r[\"JS\"] for r in runs]\n",
    "    H_runs  = [r[\"H\"]  for r in runs]\n",
    "    MS_runs = [r[\"MS\"] for r in runs]\n",
    "    return dict(runs=runs, KL=KL_runs, JS=JS_runs, H=H_runs, MS=MS_runs)\n",
    "\n",
    "def plot_latent_inhibition_summary(res_li, pre_episodes:int, title_suffix=\"\"):\n",
    "    plot_band(res_li[\"KL\"], f\"KL (latent inhibition{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_li[\"JS\"], f\"JS (latent inhibition{title_suffix})\", \"JS\")\n",
    "    plot_band(res_li[\"H\"],  f\"Avg H(S'|S) (latent inhibition{title_suffix})\", \"nats\")\n",
    "    plot_band(res_li[\"MS\"], f\"Markovization (latent inhibition{title_suffix})\", \"[0,1]\")\n",
    "    # vertical line to show transition from pre-exposure to acquisition\n",
    "    for fig_num in plt.get_fignums()[-4:]:\n",
    "        plt.figure(fig_num)\n",
    "        plt.axvline(pre_episodes, ls=\"--\", alpha=0.4)\n",
    "\n",
    "# -------- Degradation: multi-seed --------\n",
    "def run_degradation_many(cfg:CoDAConfig,\n",
    "                         seeds:list,\n",
    "                         acq_episodes:int=1000,\n",
    "                         degr_episodes:int=1000,\n",
    "                         max_steps:int=20,\n",
    "                         cue:int=5):\n",
    "    runs = [run_contingency_degradation_seed(s, cfg, acq_episodes, degr_episodes, max_steps, cue) for s in seeds]\n",
    "    return dict(\n",
    "        runs=runs,\n",
    "        KL_acq=[r[\"KL_acq\"] for r in runs], JS_acq=[r[\"JS_acq\"] for r in runs],\n",
    "        H_acq=[r[\"H_acq\"] for r in runs],   MS_acq=[r[\"MS_acq\"] for r in runs],\n",
    "        KL_deg=[r[\"KL_deg\"] for r in runs], JS_deg=[r[\"JS_deg\"] for r in runs],\n",
    "        H_deg=[r[\"H_deg\"] for r in runs],   MS_deg=[r[\"MS_deg\"] for r in runs],\n",
    "    )\n",
    "\n",
    "def plot_degradation_summary(res_deg, title_suffix=\"\"):\n",
    "    plot_band(res_deg[\"KL_acq\"], f\"KL (acquisition — degradation runs{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_deg[\"JS_acq\"], f\"JS (acquisition — degradation runs{title_suffix})\", \"JS\")\n",
    "    plot_band(res_deg[\"H_acq\"],  f\"Avg H(S'|S) (acquisition — degradation runs{title_suffix})\", \"nats\")\n",
    "    plot_band(res_deg[\"MS_acq\"], f\"Markovization (acquisition — degradation runs{title_suffix})\", \"[0,1]\")\n",
    "\n",
    "    plot_band(res_deg[\"KL_deg\"], f\"KL (contingency degradation{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_deg[\"JS_deg\"], f\"JS (contingency degradation{title_suffix})\", \"JS\")\n",
    "    plot_band(res_deg[\"H_deg\"],  f\"Avg H(S'|S) (contingency degradation{title_suffix})\", \"nats\")\n",
    "    plot_band(res_deg[\"MS_deg\"], f\"Markovization (contingency degradation{title_suffix})\", \"[0,1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters (paper-style) ---\n",
    "N_LI_PRE = 500\n",
    "N_LI_ACQ = 1000\n",
    "N_CD_ACQ = 1000\n",
    "N_CD_DEG = 1000\n",
    "SEEDS    = list(range(30))  # 30 seeds\n",
    "\n",
    "# Use your v2 acquisition cfg (keep it as you’ve set earlier)\n",
    "cfg_li = CoDAConfig()  # latent inhibition relies on v2-like gating & uncertainty\n",
    "cfg_cd = CoDAConfig()  # degradation uses same; merges depend on retrospective fall\n",
    "\n",
    "# -------- Latent inhibition --------\n",
    "res_li = run_latent_inhibition_many(cfg_li, SEEDS, pre_episodes=N_LI_PRE, acq_episodes=N_LI_ACQ, max_steps=MAX_STEPS, cue=CUE)\n",
    "plot_latent_inhibition_summary(res_li, pre_episodes=N_LI_PRE)\n",
    "\n",
    "# -------- Contingency degradation --------\n",
    "res_deg = run_degradation_many(cfg_cd, SEEDS, acq_episodes=N_CD_ACQ, degr_episodes=N_CD_DEG, max_steps=MAX_STEPS, cue=CUE)\n",
    "plot_degradation_summary(res_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Contingency degradation: plots in the same style as acquisition/extinction =====\n",
    "\n",
    "# Unpack runs\n",
    "KL_acq_runs = res_deg[\"KL_acq\"]\n",
    "JS_acq_runs = res_deg[\"JS_acq\"]\n",
    "H_acq_runs  = res_deg[\"H_acq\"]\n",
    "MS_acq_runs = res_deg[\"MS_acq\"]\n",
    "\n",
    "KL_deg_runs = res_deg[\"KL_deg\"]\n",
    "JS_deg_runs = res_deg[\"JS_deg\"]\n",
    "H_deg_runs  = res_deg[\"H_deg\"]\n",
    "MS_deg_runs = res_deg[\"MS_deg\"]\n",
    "\n",
    "# Reuse your mean±SE band helper\n",
    "def mean_se(arrs):\n",
    "    L = max(len(a) for a in arrs)\n",
    "    M = np.full((len(arrs), L), np.nan)\n",
    "    for i, a in enumerate(arrs):\n",
    "        M[i, :len(a)] = a\n",
    "    mean = np.nanmean(M, axis=0)\n",
    "    se   = np.nanstd(M, axis=0, ddof=max(1, min(len(arrs)-1, 1))) / np.sqrt(max(1, len(arrs)))\n",
    "    return mean, se\n",
    "\n",
    "def plot_band(y_runs, title, ylabel):\n",
    "    mean, se = mean_se(y_runs)\n",
    "    x = np.arange(len(mean))\n",
    "    plt.plot(x, mean, lw=2.2, label=f\"mean ({len(y_runs)} seeds)\")\n",
    "    plt.fill_between(x, mean-se, mean+se, alpha=0.25, label=\"±1 SE\")\n",
    "    plt.title(title); plt.xlabel(\"Episode\"); plt.ylabel(ylabel); plt.legend(); plt.grid(alpha=0.2)\n",
    "\n",
    "# Acquisition portion (degradation runs, pre-switch)\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_acq_runs, \"KL (acquisition)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_acq_runs, \"JS (acquisition)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_acq_runs,  \"Avg H(S'|S) (acquisition)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_acq_runs, \"Markovization (acquisition)\", \"[0,1]\"); plt.show()\n",
    "\n",
    "# Degradation portion (post-switch)\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_deg_runs, \"KL (contingency degradation)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_deg_runs, \"JS (contingency degradation)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_deg_runs,  \"Avg H(S'|S) (contingency degradation)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_deg_runs, \"Markovization (contingency degradation)\", \"[0,1]\"); plt.show()\n",
    "\n",
    "for fig_num in plt.get_fignums()[-4:]:  # last 4 figures (degradation plots)\n",
    "    plt.figure(fig_num)\n",
    "    plt.axvline(N_CD_ACQ, ls=\"--\", color=\"tab:cyan\", alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Core utilities + Analytic Ground-Truth builders + Metrics\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Small numeric helpers\n",
    "# ---------------------------\n",
    "EPS = 1e-12\n",
    "\n",
    "def _safe_row_norm(x: np.ndarray, axis: int = -1, eps: float = EPS) -> np.ndarray:\n",
    "    y = x.astype(float, copy=True)\n",
    "    s = y.sum(axis=axis, keepdims=True)\n",
    "    s[s < eps] = 1.0\n",
    "    return y / s\n",
    "\n",
    "def _pad3(A: np.ndarray, shape):\n",
    "    \"\"\"Zero-pad a [S,A,S] tensor to 'shape' without cropping.\"\"\"\n",
    "    S, A_, S2 = A.shape\n",
    "    Sg, Ag, S2g = shape\n",
    "    out = np.zeros((Sg, Ag, S2g), dtype=float)\n",
    "    out[:min(S,Sg), :min(A_,Ag), :min(S2,S2g)] = A[:min(S,Sg), :min(A_,Ag), :min(S2,S2g)]\n",
    "    return out\n",
    "\n",
    "def _agg(T: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Aggregate over actions to get P(s'|s) as [S,S] row-stochastic.\"\"\"\n",
    "    P = T.sum(axis=1)\n",
    "    return _safe_row_norm(P, axis=1)\n",
    "\n",
    "def _row_kl(p: np.ndarray, q: np.ndarray, eps: float = EPS) -> float:\n",
    "    p = np.clip(p, eps, 1.0); p /= p.sum()\n",
    "    q = np.clip(q, eps, 1.0); q /= q.sum()\n",
    "    return float(np.sum(p * (np.log(p) - np.log(q))))\n",
    "\n",
    "def _row_js(p: np.ndarray, q: np.ndarray, eps: float = EPS) -> float:\n",
    "    p = np.clip(p, eps, 1.0); p /= p.sum()\n",
    "    q = np.clip(q, eps, 1.0); q /= q.sum()\n",
    "    m = 0.5*(p+q)\n",
    "    return 0.5*_row_kl(p, m, eps) + 0.5*_row_kl(q, m, eps)\n",
    "\n",
    "def _row_H(p: np.ndarray, eps: float = EPS) -> float:\n",
    "    p = np.clip(p, eps, 1.0); p /= p.sum()\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "# ---------------------------\n",
    "# Deterministic base transitions from geometry\n",
    "# ---------------------------\n",
    "def base_T_from_env(env):\n",
    "    \"\"\"\n",
    "    Deterministic [S, A, S] from grid geometry (no clones).\n",
    "    Assumes env.valid_actions, env.state_to_pos, env.pos_to_state, env.base_actions.\n",
    "    Terminals are set absorbing (no outgoing mass).\n",
    "    \"\"\"\n",
    "    S = int(env.num_unique_states)\n",
    "    A = max(a for acts in env.valid_actions.values() for a in acts) + 1 if env.valid_actions else 0\n",
    "    T = np.zeros((S, A, S), dtype=float)\n",
    "\n",
    "    for s, acts in env.valid_actions.items():\n",
    "        for a in acts:\n",
    "            i, j = env.state_to_pos[s]\n",
    "            di, dj = env.base_actions[a]\n",
    "            ni, nj = i + di, j + dj\n",
    "            sp = env.pos_to_state[(ni, nj)]\n",
    "            T[s, a, sp] = 1.0\n",
    "\n",
    "    # terminals absorbing\n",
    "    for t in (env.rewarded_terminals + env.unrewarded_terminals):\n",
    "        if 0 <= t < S:\n",
    "            T[t, :, :] = 0.0\n",
    "    return T\n",
    "\n",
    "# internal alias used by some earlier snippets\n",
    "_base_T_from_env = base_T_from_env\n",
    "\n",
    "# ---------------------------\n",
    "# Analytic ground-truth builders\n",
    "# ---------------------------\n",
    "def gt_acquisition_with_clones(env, cue_states):\n",
    "    \"\"\"\n",
    "    Acquisition GT: Markovian by cloning each 'successor' of the cue state(s), and\n",
    "    redirecting cue->successor edges to cue->clone(successor). Clones inherit the\n",
    "    parent's outgoing transitions.\n",
    "    \"\"\"\n",
    "    T = base_T_from_env(env)\n",
    "    S, A, _ = T.shape\n",
    "    successor_to_clone = {}\n",
    "\n",
    "    for cue in cue_states:\n",
    "        for a in env.get_valid_actions(cue):\n",
    "            i, j = env.state_to_pos[cue]\n",
    "            di, dj = env.base_actions[a]\n",
    "            ni, nj = i + di, j + dj\n",
    "            sp = env.pos_to_state[(ni, nj)]\n",
    "\n",
    "            if sp in successor_to_clone:\n",
    "                cl = successor_to_clone[sp]\n",
    "            else:\n",
    "                cl = S\n",
    "                S += 1\n",
    "                T = np.pad(T, ((0,1),(0,0),(0,1)), mode='constant')\n",
    "                # clone inherits outgoing of parent successor (into old slice)\n",
    "                T[cl, :, :T.shape[2]-1] = T[sp, :, :T.shape[2]-1]\n",
    "                successor_to_clone[sp] = cl\n",
    "\n",
    "            # redirect cue->sp to cue->cl\n",
    "            mass = T[cue, a, sp]\n",
    "            T[cue, a, sp] = 0.0\n",
    "            T[cue, a, cl] = mass\n",
    "\n",
    "    # safety: terminals absorbing\n",
    "    for t in (env.rewarded_terminals + env.unrewarded_terminals):\n",
    "        if 0 <= t < T.shape[0]:\n",
    "            T[t, :, :] = 0.0\n",
    "    return T\n",
    "\n",
    "def gt_extinction_no_clones(env2):\n",
    "    \"\"\"\n",
    "    Extinction GT: no clones; any transition that would go into a rewarded terminal\n",
    "    is redirected to the paired unrewarded terminal. Terminals are absorbing.\n",
    "    \"\"\"\n",
    "    T = base_T_from_env(env2)\n",
    "    for idx, rt in enumerate(env2.rewarded_terminals):\n",
    "        if idx < len(env2.unrewarded_terminals):\n",
    "            nt = env2.unrewarded_terminals[idx]\n",
    "            mask = T[:, :, rt] > 0\n",
    "            T[:, :, nt][mask] = T[:, :, rt][mask]\n",
    "            T[:, :, rt][mask] = 0.0\n",
    "    return T\n",
    "\n",
    "def gt_degradation_no_clones(env_always_reward):\n",
    "    \"\"\"\n",
    "    Contingency degradation GT: reward is given regardless of cue/history.\n",
    "    Structural GT is the base right/down graph without clones; terminals absorbing.\n",
    "    \"\"\"\n",
    "    return base_T_from_env(env_always_reward)\n",
    "\n",
    "# Back-compat names some cells call:\n",
    "def build_gt_acquisition_with_clones(env, cue_state):\n",
    "    return gt_acquisition_with_clones(env, cue_states=[cue_state])\n",
    "\n",
    "def build_gt_extinction_no_clones(env2):\n",
    "    return gt_extinction_no_clones(env2)\n",
    "\n",
    "def build_gt_degradation_no_clones(env_always_reward):\n",
    "    return gt_degradation_no_clones(env_always_reward)\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics (fixed reference)\n",
    "# ---------------------------\n",
    "def kl_over_time_fixed(T_series, T_ref_fixed, use_js: bool = False):\n",
    "    \"\"\"\n",
    "    Row-average KL (or JS if use_js=True) between learned T_t and a fixed\n",
    "    reference T_ref_fixed (both aggregated over actions). Pads shapes as needed.\n",
    "    Returns 1D array with one value per episode in T_series.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    shape = T_ref_fixed.shape\n",
    "    Q = _agg(T_ref_fixed)\n",
    "    for T in T_series:\n",
    "        P = _agg(_pad3(T, shape))\n",
    "        if use_js:\n",
    "            row_vals = [_row_js(P[i], Q[i]) for i in range(P.shape[0])]\n",
    "        else:\n",
    "            row_vals = [_row_kl(P[i], Q[i]) for i in range(P.shape[0])]\n",
    "        scores.append(float(np.mean(row_vals)))\n",
    "    return np.array(scores)\n",
    "\n",
    "def entropy_over_time(T_series):\n",
    "    \"\"\"Average next-state entropy H(S'|S) per episode.\"\"\"\n",
    "    vals = []\n",
    "    for T in T_series:\n",
    "        P = _agg(T)\n",
    "        vals.append(float(np.mean([_row_H(P[i]) for i in range(P.shape[0])])))\n",
    "    return np.array(vals)\n",
    "\n",
    "def markovization_score(T: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    1 - normalized conditional entropy averaged across states.\n",
    "    Returns a score in [0,1], higher = more deterministic/Markovian.\n",
    "    \"\"\"\n",
    "    P = _agg(T)\n",
    "    H = np.array([_row_H(P[i]) for i in range(P.shape[0])])\n",
    "    Hmax = np.log(max(2, P.shape[1]))\n",
    "    return float(1.0 - np.mean(H)/Hmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvRightDownAlwaysReward(GridEnvRightDownNoSelf):\n",
    "    def step(self, action):\n",
    "        # Use base dynamics to compute next state & done flag\n",
    "        ns, _, done = super().step(action)\n",
    "        # Force reward at terminal regardless of visited_cue\n",
    "        if done and hasattr(self, \"is_terminal\") and self.is_terminal(ns):\n",
    "            return ns, 1, True\n",
    "        return ns, 0, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_degradation_no_clones(env_always_reward):\n",
    "    \"\"\"\n",
    "    Degradation GT: no clones; transitions are the base right/down grid transitions.\n",
    "    Reward is always delivered at the (rewarded) terminal, but that does not change the\n",
    "    transition *structure*, so this is simply the base graph without clones.\n",
    "    \"\"\"\n",
    "    T = base_T_from_env(env_always_reward)\n",
    "    # Terminals have no outgoing edges; incoming edges unchanged (deterministic).\n",
    "    return T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_latent_inhibition_seed(seed:int,\n",
    "                               cfg:CoDAConfig,\n",
    "                               pre_episodes:int = 500,\n",
    "                               acq_episodes:int = 1000,\n",
    "                               max_steps:int = 20,\n",
    "                               cue:int = 5):\n",
    "    \"\"\"\n",
    "    Phase 1 (pre-exposure):  no reward (extinction-like) to inflate P(US & ~CS)\n",
    "    Phase 2 (acquisition):   normal cued task; splitting should be delayed\n",
    "    Metrics are computed across the entire run vs. the acquisition GT-with-clones.\n",
    "    References: paper pp. 3–4, Fig. 3b; Methods p.8 (protocol & rationale). :contentReference[oaicite:1]{index=1}\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # -------- Pre-exposure (no reward) --------\n",
    "    env_pre = GridEnvRightDownNoCue(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent   = CoDAAgent(env_pre, cfg)\n",
    "\n",
    "    T_series = []\n",
    "\n",
    "    for ep in range(1, pre_episodes+1):\n",
    "        (states, actions) = generate_dataset(env_pre, n_episodes=1, max_steps=max_steps)[0]\n",
    "        agent.update_with_episode(states, actions)\n",
    "        # No splitting expected (no US); still collect T\n",
    "        T_series.append(agent.get_T().copy())\n",
    "\n",
    "    # -------- Acquisition (normal cued task) --------\n",
    "    env_acq = GridEnvRightDownNoSelf(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent.env = env_acq  # keep uncertainty/accumulators; switch to cued env\n",
    "\n",
    "    with_clones = False\n",
    "    for ep in range(1, acq_episodes+1):\n",
    "        if with_clones:\n",
    "            (states, actions) = generate_dataset_post_augmentation(env_acq, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        else:\n",
    "            (states, actions) = generate_dataset(env_acq, n_episodes=1, max_steps=max_steps)[0]\n",
    "\n",
    "        agent.update_with_episode(states, actions)\n",
    "        new = agent.maybe_split()\n",
    "        if new:\n",
    "            with_clones = True\n",
    "\n",
    "        T_series.append(agent.get_T().copy())\n",
    "\n",
    "    # GT for latent inhibition evaluation: acquisition graph with clones\n",
    "    T_ref = gt_acquisition_with_clones(env_acq, cue_states=[cue])\n",
    "\n",
    "    # Metrics over time (full series vs acquisition GT)\n",
    "    KL = kl_over_time_fixed(T_series, T_ref, use_js=False)\n",
    "    JS = kl_over_time_fixed(T_series, T_ref, use_js=True)\n",
    "    H  = entropy_over_time(T_series)\n",
    "    MS = np.array([markovization_score(T) for T in T_series])\n",
    "\n",
    "    return dict(T_series=T_series, T_ref=T_ref, KL=KL, JS=JS, H=H, MS=MS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_contingency_degradation_seed(seed:int,\n",
    "                                     cfg:CoDAConfig,\n",
    "                                     acq_episodes:int = 1000,\n",
    "                                     degr_episodes:int = 1000,\n",
    "                                     max_steps:int = 20,\n",
    "                                     cue:int = 5,\n",
    "                                     wash_in:int = 50,\n",
    "                                     edge_eps_early:float = 1e-4,\n",
    "                                     edge_eps_late:float  = 1e-6):\n",
    "    \"\"\"\n",
    "    Phase 1 (acquisition):      normal cued task (splits form).\n",
    "    Phase 2 (degradation):      reward at terminal regardless of cue/history (always reward).\n",
    "                                RC falls while PC~1, so clones should merge.\n",
    "    We report metrics separately for acq (vs acq GT-with-clones) and degr (vs degr GT no-clones).\n",
    "    References: paper p.4 (Contingency degradation; Eq. 3); Methods p.8. :contentReference[oaicite:2]{index=2}\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # -------- Acquisition --------\n",
    "    env_acq = GridEnvRightDownNoSelf(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    agent   = CoDAAgent(env_acq, cfg)\n",
    "\n",
    "    T_series_acq = []\n",
    "    with_clones = False\n",
    "\n",
    "    for ep in range(1, acq_episodes+1):\n",
    "        if with_clones:\n",
    "            (states, actions) = generate_dataset_post_augmentation(env_acq, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        else:\n",
    "            (states, actions) = generate_dataset(env_acq, n_episodes=1, max_steps=max_steps)[0]\n",
    "\n",
    "        agent.update_with_episode(states, actions)\n",
    "        new = agent.maybe_split()\n",
    "        if new:\n",
    "            with_clones = True\n",
    "\n",
    "        T_curr = agent.get_T().copy()\n",
    "        T_series_acq.append(T_curr)\n",
    "\n",
    "        # === plot last iteration graph ===\n",
    "        if ep == acq_episodes:\n",
    "            print(f\"Latent inhibition — final acquisition graph (seed {seed})\")\n",
    "            T_vis = make_terminals_absorbing_for_plot(T_curr, env_acq.rewarded_terminals + env_acq.unrewarded_terminals)\n",
    "            sanitize_for_plot(env_acq, T_vis)\n",
    "            env_acq.plot_graph(T_vis, niter=ep, threshold=0.3, save=False, title=f\"Latent Inhibition — Final Graph (seed={seed})\")\n",
    "            plt.show()\n",
    "\n",
    "    T_ref_acq = gt_acquisition_with_clones(env_acq, cue_states=[cue])\n",
    "\n",
    "    # -------- Degradation (always reward; no reset) --------\n",
    "    env_deg = GridEnvRightDownAlwaysReward(cue_states=[cue], env_size=(4,4), rewarded_terminal=[15])\n",
    "    # carry learned clones into degradation env so merges are meaningful\n",
    "    env_deg.clone_dict = dict(getattr(env_acq, \"clone_dict\", {}))\n",
    "    env_deg.reverse_clone_dict = dict(getattr(env_acq, \"reverse_clone_dict\", {}))\n",
    "    agent.env = env_deg\n",
    "\n",
    "    # short wash-in to encourage structural merges early (optional)\n",
    "    orig = dict(count_decay=agent.cfg.count_decay, trace_decay=agent.cfg.trace_decay, retro_decay=agent.cfg.retro_decay,\n",
    "                theta_merge=agent.cfg.theta_merge, confidence=agent.cfg.confidence,\n",
    "                min_presence_episodes=agent.cfg.min_presence_episodes,\n",
    "                min_effective_exposure=agent.cfg.min_effective_exposure)\n",
    "\n",
    "    agent.cfg.count_decay = 0.98\n",
    "    agent.cfg.trace_decay = 0.98\n",
    "    agent.cfg.retro_decay = 0.98\n",
    "    agent.cfg.theta_merge = 0.60\n",
    "    agent.cfg.confidence  = 0.99\n",
    "    agent.cfg.min_presence_episodes += 3\n",
    "    agent.cfg.min_effective_exposure = int(agent.cfg.min_effective_exposure * 1.5)\n",
    "\n",
    "    T_series_deg = []\n",
    "    for k in range(degr_episodes):\n",
    "        (states, actions) = generate_dataset_post_augmentation(env_deg, agent.get_T(), n_episodes=1, max_steps=max_steps)[0]\n",
    "        agent.update_with_episode(states, actions)\n",
    "        agent._edge_eps_override = edge_eps_early if k < wash_in else edge_eps_late\n",
    "        agent.maybe_merge()\n",
    "\n",
    "        T_curr = agent.get_T().copy()\n",
    "        T_series_deg.append(T_curr)\n",
    "\n",
    "        # === plot last iteration graph ===\n",
    "        if k == degr_episodes - 1:\n",
    "            print(f\"Contingency degradation — final graph (seed {seed})\")\n",
    "            T_vis = make_terminals_absorbing_for_plot(T_curr, env_deg.rewarded_terminals + env_deg.unrewarded_terminals)\n",
    "            sanitize_for_plot(env_deg, T_vis)\n",
    "            env_deg.plot_graph(T_vis, niter=k+1, threshold=0.3, save=False, title=f\"Contingency Degradation — Final Graph (seed={seed})\")\n",
    "            # plt.title(f\"Contingency Degradation — Final Graph (seed={seed})\")\n",
    "            plt.show()\n",
    "\n",
    "    T_ref_deg = gt_degradation_no_clones(env_deg)\n",
    "\n",
    "    # Metrics (separate for each phase)\n",
    "    KL_acq = kl_over_time_fixed(T_series_acq, T_ref_acq, use_js=False)\n",
    "    JS_acq = kl_over_time_fixed(T_series_acq, T_ref_acq, use_js=True)\n",
    "    H_acq  = entropy_over_time(T_series_acq)\n",
    "    MS_acq = np.array([markovization_score(T) for T in T_series_acq])\n",
    "\n",
    "    KL_deg = kl_over_time_fixed(T_series_deg, T_ref_deg, use_js=False)\n",
    "    JS_deg = kl_over_time_fixed(T_series_deg, T_ref_deg, use_js=True)\n",
    "    H_deg  = entropy_over_time(T_series_deg)\n",
    "    MS_deg = np.array([markovization_score(T) for T in T_series_deg])\n",
    "\n",
    "    return dict(\n",
    "        env_acq=env_acq, env_deg=env_deg,\n",
    "        T_series_acq=T_series_acq, T_series_deg=T_series_deg,\n",
    "        T_ref_acq=T_ref_acq, T_ref_deg=T_ref_deg,\n",
    "        KL_acq=KL_acq, JS_acq=JS_acq, H_acq=H_acq, MS_acq=MS_acq,\n",
    "        KL_deg=KL_deg, JS_deg=JS_deg, H_deg=H_deg, MS_deg=MS_deg\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Latent inhibition: multi-seed --------\n",
    "def run_latent_inhibition_many(cfg:CoDAConfig,\n",
    "                               seeds:list,\n",
    "                               pre_episodes:int=500,\n",
    "                               acq_episodes:int=1000,\n",
    "                               max_steps:int=20,\n",
    "                               cue:int=5):\n",
    "    runs = [run_latent_inhibition_seed(s, cfg, pre_episodes, acq_episodes, max_steps, cue) for s in seeds]\n",
    "    KL_runs = [r[\"KL\"] for r in runs]\n",
    "    JS_runs = [r[\"JS\"] for r in runs]\n",
    "    H_runs  = [r[\"H\"]  for r in runs]\n",
    "    MS_runs = [r[\"MS\"] for r in runs]\n",
    "    return dict(runs=runs, KL=KL_runs, JS=JS_runs, H=H_runs, MS=MS_runs)\n",
    "\n",
    "def plot_latent_inhibition_summary(res_li, pre_episodes:int, title_suffix=\"\"):\n",
    "    plot_band(res_li[\"KL\"], f\"KL (latent inhibition{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_li[\"JS\"], f\"JS (latent inhibition{title_suffix})\", \"JS\")\n",
    "    plot_band(res_li[\"H\"],  f\"Avg H(S'|S) (latent inhibition{title_suffix})\", \"nats\")\n",
    "    plot_band(res_li[\"MS\"], f\"Markovization (latent inhibition{title_suffix})\", \"[0,1]\")\n",
    "    # vertical line to show transition from pre-exposure to acquisition\n",
    "    for fig_num in plt.get_fignums()[-4:]:\n",
    "        plt.figure(fig_num)\n",
    "        plt.axvline(pre_episodes, ls=\"--\", alpha=0.4)\n",
    "\n",
    "# -------- Degradation: multi-seed --------\n",
    "def run_degradation_many(cfg:CoDAConfig,\n",
    "                         seeds:list,\n",
    "                         acq_episodes:int=1000,\n",
    "                         degr_episodes:int=1000,\n",
    "                         max_steps:int=20,\n",
    "                         cue:int=5):\n",
    "    runs = [run_contingency_degradation_seed(s, cfg, acq_episodes, degr_episodes, max_steps, cue) for s in seeds]\n",
    "    return dict(\n",
    "        runs=runs,\n",
    "        KL_acq=[r[\"KL_acq\"] for r in runs], JS_acq=[r[\"JS_acq\"] for r in runs],\n",
    "        H_acq=[r[\"H_acq\"] for r in runs],   MS_acq=[r[\"MS_acq\"] for r in runs],\n",
    "        KL_deg=[r[\"KL_deg\"] for r in runs], JS_deg=[r[\"JS_deg\"] for r in runs],\n",
    "        H_deg=[r[\"H_deg\"] for r in runs],   MS_deg=[r[\"MS_deg\"] for r in runs],\n",
    "    )\n",
    "\n",
    "def plot_degradation_summary(res_deg, title_suffix=\"\"):\n",
    "    plot_band(res_deg[\"KL_acq\"], f\"KL (acquisition — degradation runs{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_deg[\"JS_acq\"], f\"JS (acquisition — degradation runs{title_suffix})\", \"JS\")\n",
    "    plot_band(res_deg[\"H_acq\"],  f\"Avg H(S'|S) (acquisition — degradation runs{title_suffix})\", \"nats\")\n",
    "    plot_band(res_deg[\"MS_acq\"], f\"Markovization (acquisition — degradation runs{title_suffix})\", \"[0,1]\")\n",
    "\n",
    "    plot_band(res_deg[\"KL_deg\"], f\"KL (contingency degradation{title_suffix})\", \"KL (nats)\")\n",
    "    plot_band(res_deg[\"JS_deg\"], f\"JS (contingency degradation{title_suffix})\", \"JS\")\n",
    "    plot_band(res_deg[\"H_deg\"],  f\"Avg H(S'|S) (contingency degradation{title_suffix})\", \"nats\")\n",
    "    plot_band(res_deg[\"MS_deg\"], f\"Markovization (contingency degradation{title_suffix})\", \"[0,1]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters (paper-style) ---\n",
    "N_LI_PRE = 500\n",
    "N_LI_ACQ = 1000\n",
    "N_CD_ACQ = 1000\n",
    "N_CD_DEG = 1000\n",
    "SEEDS    = list(range(30))  # 30 seeds\n",
    "\n",
    "# Use your v2 acquisition cfg (keep it as you’ve set earlier)\n",
    "cfg_li = CoDAConfig()  # latent inhibition relies on v2-like gating & uncertainty\n",
    "cfg_cd = CoDAConfig()  # degradation uses same; merges depend on retrospective fall\n",
    "\n",
    "# -------- Latent inhibition --------\n",
    "res_li = run_latent_inhibition_many(cfg_li, SEEDS, pre_episodes=N_LI_PRE, acq_episodes=N_LI_ACQ, max_steps=MAX_STEPS, cue=CUE)\n",
    "plot_latent_inhibition_summary(res_li, pre_episodes=N_LI_PRE)\n",
    "\n",
    "# -------- Contingency degradation --------\n",
    "res_deg = run_degradation_many(cfg_cd, SEEDS, acq_episodes=N_CD_ACQ, degr_episodes=N_CD_DEG, max_steps=MAX_STEPS, cue=CUE)\n",
    "# plot_degradation_summary(res_deg)\n",
    "\n",
    "# ===== Contingency degradation: plots in the same style as acquisition/extinction =====\n",
    "\n",
    "# Unpack runs\n",
    "KL_acq_runs = res_deg[\"KL_acq\"]\n",
    "JS_acq_runs = res_deg[\"JS_acq\"]\n",
    "H_acq_runs  = res_deg[\"H_acq\"]\n",
    "MS_acq_runs = res_deg[\"MS_acq\"]\n",
    "\n",
    "KL_deg_runs = res_deg[\"KL_deg\"]\n",
    "JS_deg_runs = res_deg[\"JS_deg\"]\n",
    "H_deg_runs  = res_deg[\"H_deg\"]\n",
    "MS_deg_runs = res_deg[\"MS_deg\"]\n",
    "\n",
    "# Reuse your mean±SE band helper\n",
    "def mean_se(arrs):\n",
    "    L = max(len(a) for a in arrs)\n",
    "    M = np.full((len(arrs), L), np.nan)\n",
    "    for i, a in enumerate(arrs):\n",
    "        M[i, :len(a)] = a\n",
    "    mean = np.nanmean(M, axis=0)\n",
    "    se   = np.nanstd(M, axis=0, ddof=max(1, min(len(arrs)-1, 1))) / np.sqrt(max(1, len(arrs)))\n",
    "    return mean, se\n",
    "\n",
    "def plot_band(y_runs, title, ylabel):\n",
    "    mean, se = mean_se(y_runs)\n",
    "    x = np.arange(len(mean))\n",
    "    plt.plot(x, mean, lw=2.2, label=f\"mean ({len(y_runs)} seeds)\")\n",
    "    plt.fill_between(x, mean-se, mean+se, alpha=0.25, label=\"±1 SE\")\n",
    "    plt.title(title); plt.xlabel(\"Episode\"); plt.ylabel(ylabel); plt.legend(); plt.grid(alpha=0.2)\n",
    "\n",
    "# Acquisition portion (degradation runs, pre-switch)\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_acq_runs, \"KL (acquisition)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_acq_runs, \"JS (acquisition)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_acq_runs,  \"Avg H(S'|S) (acquisition)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_acq_runs, \"Markovization (acquisition)\", \"[0,1]\"); plt.show()\n",
    "\n",
    "# Degradation portion (post-switch)\n",
    "plt.figure(figsize=(10,4)); plot_band(KL_deg_runs, \"KL (contingency degradation)\", \"KL (nats)\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(JS_deg_runs, \"JS (contingency degradation)\", \"JS\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(H_deg_runs,  \"Avg H(S'|S) (contingency degradation)\", \"nats\"); plt.show()\n",
    "plt.figure(figsize=(10,4)); plot_band(MS_deg_runs, \"Markovization (contingency degradation)\", \"[0,1]\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "You can set `save=True` in `plot_graph` to export the changed snapshots as PNGs only for those episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## Metrics: KL/JS vs episode, Entropy, and Markovization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute metrics using the module we prepared\n",
    "from coda_metrics import kl_over_time, entropy_over_time, markovization_score, ref_empirical_from_rollouts, greedy_right_down_policy\n",
    "import numpy as np\n",
    "\n",
    "def ref_builder_factory(env, policy_fn, nroll=300, max_steps=20):\n",
    "    def _make_ref(T_learned):\n",
    "        return ref_empirical_from_rollouts(env, policy_fn, n_episodes=nroll, max_steps=max_steps)\n",
    "    return _make_ref\n",
    "\n",
    "# Build episode-wise empirical references\n",
    "ref_fn_acq = ref_builder_factory(env,  greedy_right_down_policy, nroll=300, max_steps=20)\n",
    "ref_fn_ext = ref_builder_factory(env2, greedy_right_down_policy, nroll=300, max_steps=20)\n",
    "\n",
    "KL_acq = kl_over_time(T_series_acq, ref_fn_acq, use_js=False)\n",
    "JS_acq = kl_over_time(T_series_acq, ref_fn_acq, use_js=True)\n",
    "H_acq  = entropy_over_time(T_series_acq)\n",
    "MS_acq = np.array([markovization_score(T) for T in T_series_acq])\n",
    "\n",
    "KL_ext = kl_over_time(T_series_ext, ref_fn_ext, use_js=False)\n",
    "JS_ext = kl_over_time(T_series_ext, ref_fn_ext, use_js=True)\n",
    "H_ext  = entropy_over_time(T_series_ext)\n",
    "MS_ext = np.array([markovization_score(T) for T in T_series_ext])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot (one metric per figure)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def _offset_plot(ax, y1, y2, label1, label2):\n",
    "    ax.plot(y1, label=label1)\n",
    "    off = len(y1)\n",
    "    ax.plot(off + np.arange(len(y2)), y2, label=label2)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"episode\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"KL (learned || empirical reference)\")\n",
    "_offset_plot(ax, KL_acq, KL_ext, \"acq\", \"ext\")\n",
    "ax.set_ylabel(\"KL\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"JS distance\")\n",
    "_offset_plot(ax, JS_acq, JS_ext, \"acq\", \"ext\")\n",
    "ax.set_ylabel(\"JS\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Avg next-state entropy H(S'|S)\")\n",
    "_offset_plot(ax, H_acq, H_ext, \"acq\", \"ext\")\n",
    "ax.set_ylabel(\"nats\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Markovization score (1 - normalized H)\")\n",
    "_offset_plot(ax, MS_acq, MS_ext, \"acq\", \"ext\")\n",
    "ax.set_ylabel(\"[0,1]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## Plots (separate panels with mean ± SE shading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _pad_runs(runs):\n",
    "    L = max(len(r) for r in runs)\n",
    "    out = np.full((len(runs), L), np.nan, dtype=float)\n",
    "    for i, r in enumerate(runs):\n",
    "        out[i, :len(r)] = r\n",
    "    return out\n",
    "\n",
    "def _plot_with_band(ax, runs, title, ylabel):\n",
    "    M = _pad_runs(runs) if isinstance(runs, (list, tuple)) and len(runs)>0 and isinstance(runs[0], (list, np.ndarray)) else np.atleast_2d(runs)\n",
    "    mean = np.nanmean(M, axis=0)\n",
    "    se   = np.nanstd(M, axis=0, ddof=1) / np.sqrt(max(1, M.shape[0]))\n",
    "    x = np.arange(len(mean))\n",
    "    ax.plot(x, mean, lw=2.0, label=\"mean\")\n",
    "    ax.fill_between(x, mean - se, mean + se, alpha=0.2, label=\"±1 SE\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "\n",
    "# Wrap single-run arrays as [array] so the function produces a zero-width band\n",
    "KL_acq_runs = [KL_acq] if not isinstance(KL_acq, (list, tuple)) else KL_acq\n",
    "JS_acq_runs = [JS_acq] if not isinstance(JS_acq, (list, tuple)) else JS_acq\n",
    "H_acq_runs  = [H_acq]  if not isinstance(H_acq,  (list, tuple)) else H_acq\n",
    "MS_acq_runs = [MS_acq] if not isinstance(MS_acq, (list, tuple)) else MS_acq\n",
    "\n",
    "KL_ext_runs = [KL_ext] if not isinstance(KL_ext, (list, tuple)) else KL_ext\n",
    "JS_ext_runs = [JS_ext] if not isinstance(JS_ext, (list, tuple)) else JS_ext\n",
    "H_ext_runs  = [H_ext]  if not isinstance(H_ext,  (list, tuple)) else H_ext\n",
    "MS_ext_runs = [MS_ext] if not isinstance(MS_ext, (list, tuple)) else MS_ext\n",
    "\n",
    "# Acquisition-only figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,6), constrained_layout=True)\n",
    "_plot_with_band(axes[0,0], KL_acq_runs, \"KL (acquisition)\", \"KL (nats)\")\n",
    "_plot_with_band(axes[0,1], JS_acq_runs, \"JS (acquisition)\", \"JS\")\n",
    "_plot_with_band(axes[1,0], H_acq_runs,  \"Avg H(S'|S) (acquisition)\", \"nats\")\n",
    "_plot_with_band(axes[1,1], MS_acq_runs, \"Markovization (acquisition)\", \"[0,1]\")\n",
    "plt.show()\n",
    "\n",
    "# Extinction-only figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,6), constrained_layout=True)\n",
    "_plot_with_band(axes[0,0], KL_ext_runs, \"KL (extinction)\", \"KL (nats)\")\n",
    "_plot_with_band(axes[0,1], JS_ext_runs, \"JS (extinction)\", \"JS\")\n",
    "_plot_with_band(axes[1,0], H_ext_runs,  \"Avg H(S'|S) (extinction)\", \"nats\")\n",
    "_plot_with_band(axes[1,1], MS_ext_runs, \"Markovization (extinction)\", \"[0,1]\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
