{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hmm_transition_graph(T, niter=0,\n",
    "                              highlight_node=-1, highlight_node_2=-1,\n",
    "                              threshold=0.01, save=False, savename='img'):\n",
    "    \"\"\"\n",
    "    Plot the HMM's transition matrix as a directed graph, using NetworkX.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    T : 2D np.array of shape (N, N)\n",
    "        Transition matrix (T[i,j] = P(z_{t+1}=j | z_t=i)).\n",
    "    niter : int\n",
    "        An iteration or epoch number (used in the figure title).\n",
    "    highlight_node : int\n",
    "        Index of a node to highlight in red (default: -1 = none).\n",
    "    highlight_node_2 : int\n",
    "        Index of a node to highlight in yellow (default: -1 = none).\n",
    "    threshold : float\n",
    "        Only plot edges with probability > threshold (to avoid clutter).\n",
    "    save : bool\n",
    "        Whether to save the figure to disk.\n",
    "    savename : str\n",
    "        Filename prefix for saving.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of hidden states\n",
    "    n_state = T.shape[0]\n",
    "\n",
    "    # Initialize a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes (one per hidden state)\n",
    "    for s in range(n_state):\n",
    "        G.add_node(s)\n",
    "\n",
    "    # Edge-adding + edge labels\n",
    "    edge_colors = []\n",
    "    for s in range(n_state):\n",
    "        for s_next in range(n_state):\n",
    "            prob = T[s, s_next]\n",
    "            if prob > threshold:  # filter out very small probabilities\n",
    "                label = f\"p={prob:.2f}\"\n",
    "                G.add_edge(s, s_next, label=label)\n",
    "                # (optional) color all edges the same or by probability\n",
    "                edge_colors.append(\"blue\")\n",
    "\n",
    "    # Layout for the nodes (you can use nx.circular_layout, nx.spring_layout, etc.)\n",
    "    pos = nx.circular_layout(G)\n",
    "\n",
    "    # Prepare figure\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Node coloring\n",
    "    colors = []\n",
    "    for node in G.nodes():\n",
    "        if node == highlight_node:\n",
    "            colors.append(\"red\")\n",
    "        elif node == highlight_node_2:\n",
    "            colors.append(\"yellow\")\n",
    "        else:\n",
    "            colors.append(\"lightblue\")\n",
    "\n",
    "    # Draw nodes and labels\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=1200)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12, font_color='black')\n",
    "\n",
    "    # Draw edges\n",
    "    edges = list(G.edges())\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges,\n",
    "                           edge_color=edge_colors,\n",
    "                           arrowstyle='->', arrowsize=20, width=2)\n",
    "\n",
    "    # Edge labels\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        G, pos,\n",
    "        edge_labels=nx.get_edge_attributes(G, 'label'),\n",
    "        font_size=10, label_pos=0.5\n",
    "    )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"HMM Transition Graph at iteration {niter}\", size=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f\"{savename}_iteration_{niter}.png\", dpi=200)\n",
    "        print(f\"Saved figure to {savename}_iteration_{niter}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_nth_order_observations(sequence, n):\n",
    "    \"\"\"\n",
    "    Convert a single sequence of discrete observations into an n-th order 'super-observation' sequence.\n",
    "    \n",
    "    E.g., if sequence = [2, 5, 5, 1] and n=2,\n",
    "    we create pairs:\n",
    "      (2, 5), (5, 5), (5, 1)\n",
    "    Then we map each pair to a unique integer ID (so we can use it in a MultinomialHMM).\n",
    "    \n",
    "    Returns: \n",
    "      - X: a 1D numpy array of encoded observations \n",
    "    \"\"\"\n",
    "    if len(sequence) < n:\n",
    "        return np.array([])  # not enough data to form even one super-observation\n",
    "    \n",
    "    # Create a sliding window of size n\n",
    "    # Example: n=2 => [(seq[0], seq[1]), (seq[1], seq[2]), ...]\n",
    "    windows = [tuple(sequence[i:i+n]) for i in range(len(sequence) - n + 1)]\n",
    "    \n",
    "    # We must map each unique window to a unique integer.\n",
    "    # We'll do this globally below, so just return the list of tuples for now.\n",
    "    return windows\n",
    "\n",
    "def encode_super_observations(all_windows):\n",
    "    \"\"\"\n",
    "    Given a list of lists of 'super-observations' (tuples),\n",
    "    map each unique tuple to a unique integer code.\n",
    "    \n",
    "    Returns:\n",
    "      - X_cat: a concatenated 1D numpy array of encoded observations\n",
    "      - lengths: a list for each sequence length (required by hmmlearn for multiple sequences)\n",
    "      - mapping_dict: the dictionary that maps each tuple to an integer ID\n",
    "    \"\"\"\n",
    "    # Flatten all super-observation tuples across all sequences\n",
    "    flattened = [obs for seq in all_windows for obs in seq]\n",
    "    unique_obs = list(set(flattened))\n",
    "    \n",
    "    # Create a mapping from tuple -> integer\n",
    "    mapping_dict = {obs_tuple: i for i, obs_tuple in enumerate(unique_obs)}\n",
    "    \n",
    "    # Encode each sequence\n",
    "    encoded_sequences = []\n",
    "    lengths = []\n",
    "    for seq in all_windows:\n",
    "        encoded_seq = [mapping_dict[tup] for tup in seq]\n",
    "        encoded_sequences.append(encoded_seq)\n",
    "        lengths.append(len(encoded_seq))\n",
    "    \n",
    "    # Concatenate into one big array (hmmlearn’s fit expects a 2D array of shape (n_samples, 1))\n",
    "    X_cat = np.concatenate([np.array(es) for es in encoded_sequences])\n",
    "    X_cat = X_cat.reshape(-1, 1)  # shape (total_length, 1)\n",
    "    \n",
    "    return X_cat, lengths, mapping_dict\n",
    "\n",
    "def train_nth_order_hmm(\n",
    "    sequences, n=2, hidden_states_range=[2, 3, 4, 5], \n",
    "    max_iter=100, random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    sequences: list of lists, each an integer-coded sequence of observations\n",
    "    n: order of the Markov model\n",
    "    hidden_states_range: list of possible numbers of hidden states to try\n",
    "    max_iter: maximum EM iterations\n",
    "    \"\"\"\n",
    "    # 1. Create n-th order super-observations for each sequence\n",
    "    all_windows = [create_nth_order_observations(seq, n) for seq in sequences]\n",
    "    \n",
    "    # 2. Encode these super-observations into integers\n",
    "    X_cat, lengths, mapping = encode_super_observations(all_windows)\n",
    "    \n",
    "    best_model = None\n",
    "    best_bic = float(\"inf\")\n",
    "    best_num_states = None\n",
    "    \n",
    "    # We'll do a simple model selection loop over number of hidden states\n",
    "    for n_states in hidden_states_range:\n",
    "        # Create and fit a MultinomialHMM\n",
    "        model = hmm.MultinomialHMM(\n",
    "            n_components=n_states, \n",
    "            n_iter=max_iter, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        model.fit(X_cat, lengths=lengths)\n",
    "        \n",
    "        # Compute the log-likelihood on the training data\n",
    "        log_likelihood = model.score(X_cat, lengths=lengths)\n",
    "        \n",
    "        # Number of parameters for a MultinomialHMM:\n",
    "        #   ~ n_states - 1 (initial state distribution) \n",
    "        #   + n_states*(n_states - 1) (transition matrix) \n",
    "        #   + n_states*(num_observation_symbols - 1) (emission matrix)\n",
    "        # This is approximate. For a fully correct count, see your HMM’s parameterization.\n",
    "        num_obs_symbols = len(mapping) \n",
    "        n_params = (n_states - 1) \\\n",
    "                   + n_states*(n_states - 1) \\\n",
    "                   + n_states*(num_obs_symbols - 1)\n",
    "        \n",
    "        # BIC = -2 * log(L) + n_params * log(N)\n",
    "        # where N is total number of data points\n",
    "        N = X_cat.shape[0]\n",
    "        bic = -2 * log_likelihood + n_params * np.log(N)\n",
    "        \n",
    "        print(f\"States={n_states}, LogLik={log_likelihood:.2f}, BIC={bic:.2f}\")\n",
    "        \n",
    "        if bic < best_bic:\n",
    "            best_bic = bic\n",
    "            best_model = model\n",
    "            best_num_states = n_states\n",
    "    \n",
    "    print(f\"\\nBest model: {best_num_states} hidden states, BIC={best_bic:.2f}\")\n",
    "    return best_model, mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Example Usage\n",
    "# ---------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "# Suppose we have discrete sequences of observations (actions/states) coded as integers\n",
    "# For instance, \"actions\" might be 0..5, or 0..10, etc.\n",
    "# We’ll create a small dummy dataset:\n",
    "sequences = [\n",
    "    [2, 2, 5, 1, 1, 5, 3, 2],\n",
    "    [2, 5, 5, 1, 2, 2],\n",
    "    [1, 1, 5, 5, 5, 3, 2, 2, 1],\n",
    "    [2, 2, 5, 3, 3, 1, 5],\n",
    "]\n",
    "\n",
    "# Train a 2nd-order HMM, trying 2..5 hidden states\n",
    "best_model, obs_mapping = train_nth_order_hmm(sequences, n=2, hidden_states_range=[2, 3, 4, 5])\n",
    "\n",
    "# After training, 'best_model' has your HMM parameters\n",
    "# e.g., transition matrix:\n",
    "print(\"\\nLearned Transition Matrix:\")\n",
    "print(best_model.transmat_)\n",
    "\n",
    "print(\"\\nLearned Emission Matrix (rows = hidden states, columns = super-observation symbols):\")\n",
    "print(best_model.emissionprob_)\n",
    "\n",
    "# If you want to decode a new sequence, do:\n",
    "new_sequence = [2, 5, 1, 1, 5]\n",
    "new_windows = create_nth_order_observations(new_sequence, n=2)\n",
    "new_encoded = [obs_mapping[tup] for tup in new_windows]  # map to integers\n",
    "X_test = np.array(new_encoded).reshape(-1,1)\n",
    "\n",
    "log_prob, hidden_state_seq = best_model.decode(X_test, algorithm=\"viterbi\")\n",
    "print(\"\\nDecoded hidden state sequence for new data:\")\n",
    "print(hidden_state_seq)\n",
    "print(f\"Log-likelihood of that sequence: {log_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you've already fit your model and have 'best_model'\n",
    "trans_mat = best_model.transmat_\n",
    "\n",
    "# Plot it\n",
    "plot_hmm_transition_graph(\n",
    "    T=trans_mat,\n",
    "    niter=1,             # or any iteration number\n",
    "    highlight_node=0,    # e.g., highlight state 0\n",
    "    highlight_node_2=2,  # highlight state 2\n",
    "    threshold=0.05,      # omit edges with probability below 0.05\n",
    "    save=True,\n",
    "    savename='my_hmm_graph'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('cscg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4c7db56e4aa600ad0a9a975c34bbf2d671fd5a4715ac0a7956790af44717dcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
