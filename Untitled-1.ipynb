{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DriftingBanditEnv:\n",
    "    \"\"\"\n",
    "    Simulates an environment with multiple hidden 'contexts'.\n",
    "    Each context has a fixed Bernoulli reward probability per arm.\n",
    "    The environment changes context abruptly (or occasionally) over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, context_probs, switch_timepoints, num_arms=2, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_probs: list of lists of shape (num_contexts, num_arms),\n",
    "                           i.e. context_probs[c][a] = reward probability for arm a in context c.\n",
    "            switch_timepoints: list of time steps where context changes to the next one\n",
    "            num_arms: number of arms\n",
    "        \"\"\"\n",
    "        self.num_arms = num_arms\n",
    "        self.context_probs = context_probs\n",
    "        self.switch_timepoints = switch_timepoints\n",
    "        self.current_context_idx = 0\n",
    "        self.t = 0\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns (reward, done).\n",
    "        Reward is 1 or 0 sampled from the current context's Bernoulli distribution for the chosen arm.\n",
    "        We artificially end after we pass the last switch_timepoint.\n",
    "        \"\"\"\n",
    "        # Check if we should switch context\n",
    "        if self.current_context_idx < len(self.switch_timepoints):\n",
    "            if self.t >= self.switch_timepoints[self.current_context_idx]:\n",
    "                self.current_context_idx += 1\n",
    "        \n",
    "        # If we are out of contexts, environment is done\n",
    "        if self.current_context_idx >= len(self.context_probs):\n",
    "            return 0, True  # or any dummy reward\n",
    "\n",
    "        # Generate reward\n",
    "        p = self.context_probs[self.current_context_idx][action]\n",
    "        reward = 1 if self.rng.random() < p else 0\n",
    "        \n",
    "        self.t += 1\n",
    "        return reward, False\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# LATENT CAUSE (CONTEXT) AGENT\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "class LatentContextBanditAgent:\n",
    "    \"\"\"\n",
    "    Maintains multiple latent contexts (causes), each with Beta parameters for each arm.\n",
    "    Uses a simplified CRP prior or \"hazard rate\" approach to spawn new contexts if needed.\n",
    "    Performs Thompson Sampling over the mixture of contexts.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms=2, alpha_0=1.0, beta_0=1.0, hazard_rate=0.01, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_arms: how many arms in the bandit\n",
    "            alpha_0, beta_0: initial Beta prior for each (arm, context)\n",
    "            hazard_rate: probability of creating a new context on any given step\n",
    "        \"\"\"\n",
    "        self.num_arms = num_arms\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.beta_0 = beta_0\n",
    "        self.hazard_rate = hazard_rate\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        \n",
    "        # We'll store for each context: alpha[arm], beta[arm], and a posterior weight w.\n",
    "        self.contexts = []\n",
    "        # Start with one context\n",
    "        self.contexts.append({\n",
    "            'alpha': np.ones(num_arms) * alpha_0,\n",
    "            'beta':  np.ones(num_arms) * beta_0,\n",
    "            'weight': 1.0  # unnormalized posterior weight\n",
    "        })\n",
    "    \n",
    "    def choose_arm(self):\n",
    "        \"\"\"\n",
    "        Thompson Sampling over all contexts, weighted by posterior context probability.\n",
    "        Steps:\n",
    "        1) Sample a context index from the categorical distribution of their posterior weights.\n",
    "        2) From that context's Beta distribution for each arm, sample a value -> pick arm with max sample.\n",
    "        \"\"\"\n",
    "        # Normalize context weights\n",
    "        weights = np.array([c['weight'] for c in self.contexts])\n",
    "        weights /= weights.sum()\n",
    "        \n",
    "        # First sample which context is active\n",
    "        ctx_idx = self.rng.choice(len(self.contexts), p=weights)\n",
    "        \n",
    "        # Thompson sample from that context's alpha/beta\n",
    "        alpha = self.contexts[ctx_idx]['alpha']\n",
    "        beta = self.contexts[ctx_idx]['beta']\n",
    "        \n",
    "        theta_samples = self.rng.beta(alpha, beta)\n",
    "        chosen_arm = np.argmax(theta_samples)\n",
    "        return chosen_arm\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        \"\"\"\n",
    "        After observing a reward (0 or 1) for chosen_arm, update the posterior over:\n",
    "          1) The contexts' weights\n",
    "          2) The alpha/beta for each context\n",
    "          3) Possibly spawn a new context\n",
    "        \"\"\"\n",
    "        # Step 0: compute likelihood of the reward under each context\n",
    "        weights = np.array([c['weight'] for c in self.contexts])\n",
    "        likelihoods = np.zeros(len(self.contexts))\n",
    "        \n",
    "        for i, ctx in enumerate(self.contexts):\n",
    "            alpha = ctx['alpha'][chosen_arm]\n",
    "            beta = ctx['beta'][chosen_arm]\n",
    "            # Probability of reward=1 or 0 from Beta( alpha, beta )\n",
    "            # p( reward=1 ) = alpha / (alpha + beta ), p(reward=0) = beta / (alpha + beta)\n",
    "            p_reward = alpha / (alpha + beta)\n",
    "            if reward == 1:\n",
    "                likelihoods[i] = p_reward\n",
    "            else:\n",
    "                likelihoods[i] = 1.0 - p_reward\n",
    "        \n",
    "        # Unnormalized posterior = prior_weight * likelihood\n",
    "        posterior_unnorm = weights * likelihoods\n",
    "        post_sum = posterior_unnorm.sum()\n",
    "        \n",
    "        # Step 1: Probability that we create a new context\n",
    "        # We use a \"hazard_rate\" approach: there's some small prob of new cause each step.\n",
    "        # Another approach uses CRP with a concentration parameter, etc.\n",
    "        prob_new_context = self.hazard_rate\n",
    "        \n",
    "        # Step 2: If we decide to spawn new context, do so with probability proportional\n",
    "        #         to prob_new_context * \"likelihood of data under new context\"\n",
    "        # For a brand-new context: reward likelihood is the same as the prior's predictive:\n",
    "        #  p_new_context(reward=1) = alpha_0/(alpha_0+beta_0), or reward=0 likewise.\n",
    "        p_reward_new = self.alpha_0 / (self.alpha_0 + self.beta_0)\n",
    "        if reward == 1:\n",
    "            new_ctx_likelihood = p_reward_new\n",
    "        else:\n",
    "            new_ctx_likelihood = 1.0 - p_reward_new\n",
    "        \n",
    "        # Posterior mass of new context (unnormalized):\n",
    "        new_context_weight = prob_new_context * new_ctx_likelihood\n",
    "        \n",
    "        # Normalize across existing contexts + new context\n",
    "        total_weight = post_sum + new_context_weight\n",
    "        if total_weight < 1e-15:\n",
    "            # If everything is extremely unlikely, fallback to uniform\n",
    "            # (this can happen if alpha/beta are very large or data is contradictory)\n",
    "            # But in practice, might want to handle differently\n",
    "            n_contexts = len(self.contexts)\n",
    "            for ctx in self.contexts:\n",
    "                ctx['weight'] = 1.0 / n_contexts\n",
    "            return\n",
    "        \n",
    "        # Step 3: Update the weight for each existing context\n",
    "        for i in range(len(self.contexts)):\n",
    "            self.contexts[i]['weight'] = posterior_unnorm[i] / total_weight\n",
    "        \n",
    "        # Step 4: Possibly spawn new context\n",
    "        # Sample a Bernoulli if we want to be random, or do a \"soft\" approach:\n",
    "        # We'll do a \"soft\" approach here: if new_context_weight is > 0, create a new context with that posterior weight\n",
    "        if new_context_weight > 0.0:\n",
    "            spawn_prob = new_context_weight / total_weight\n",
    "            # We can either always create one with that posterior mass,\n",
    "            # or flip a coin with probability = spawn_prob.\n",
    "            # We'll create it deterministically here:\n",
    "            if self.rng.random() < spawn_prob:\n",
    "                new_ctx = {\n",
    "                    'alpha': np.ones(self.num_arms) * self.alpha_0,\n",
    "                    'beta':  np.ones(self.num_arms) * self.beta_0,\n",
    "                    'weight': spawn_prob\n",
    "                }\n",
    "                self.contexts.append(new_ctx)\n",
    "        \n",
    "        # Step 5: Update the alpha/beta of each context *proportionally* to how likely\n",
    "        # it was to have generated the observation.\n",
    "        # For an agent that merges updates into a single posterior, we weigh the update.\n",
    "        \n",
    "        for i, ctx in enumerate(self.contexts):\n",
    "            # The fraction of posterior mass for this context among *existing contexts*\n",
    "            # that actually saw the observation\n",
    "            post_context = ctx['weight']\n",
    "            # \"Learning rate\" for the update:\n",
    "            # if the context got 50% posterior mass, we effectively do 0.5 of an update\n",
    "            # This is a simple approximate method.\n",
    "            update_scale = post_context  # Weighted by final posterior mass\n",
    "\n",
    "            # Update the alpha/beta for the chosen arm\n",
    "            if reward == 1:\n",
    "                ctx['alpha'][chosen_arm] += update_scale\n",
    "            else:\n",
    "                ctx['beta'][chosen_arm] += update_scale\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RUN A SMALL EXPERIMENT\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose we have 3 contexts for a 2-armed bandit:\n",
    "    #   Context 0: arm0 p=0.7, arm1 p=0.4\n",
    "    #   Context 1: arm0 p=0.2, arm1 p=0.8\n",
    "    #   Context 2: arm0 p=0.6, arm1 p=0.6\n",
    "    # The environment will switch contexts at t=300, 600, 900 (for example).\n",
    "    # Then ends after t=1200 or so.\n",
    "    \n",
    "    context_probs = [\n",
    "        [0.7, 0.4],  # context 0\n",
    "        [0.2, 0.8],  # context 1\n",
    "        [0.6, 0.6]   # context 2\n",
    "    ]\n",
    "    # switch_timepoints = [300, 600, 900, 1200] means:\n",
    "    #   [0..299]   => context 0\n",
    "    #   [300..599] => context 1\n",
    "    #   [600..899] => context 2\n",
    "    #   [900..1199]=> context 3 (but we only have 3 contexts so environment ends)\n",
    "    switch_timepoints = [300, 600, 900, 1200]\n",
    "\n",
    "    env = DriftingBanditEnv(context_probs, switch_timepoints, num_arms=2, seed=42)\n",
    "    agent = LatentContextBanditAgent(num_arms=2, alpha_0=1.0, beta_0=1.0, hazard_rate=0.01, seed=0)\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    contexts_count_history = []\n",
    "\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done:\n",
    "        arm = agent.choose_arm()\n",
    "        r, done = env.step(arm)\n",
    "        agent.update(arm, r)\n",
    "        \n",
    "        rewards.append(r)\n",
    "        actions.append(arm)\n",
    "        contexts_count_history.append(len(agent.contexts))\n",
    "        \n",
    "        t += 1\n",
    "\n",
    "    # Plot the results\n",
    "    cum_rewards = np.cumsum(rewards)\n",
    "    steps = np.arange(len(rewards)) + 1\n",
    "    avg_reward = cum_rewards / steps\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(steps, avg_reward, label='Average Reward')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Cumulative Average Reward')\n",
    "    plt.title('Latent Context Agent on Drifting Bandit')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot how many contexts the agent \"believes\" in over time\n",
    "    plt.figure()\n",
    "    plt.plot(steps, contexts_count_history)\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Number of Discovered Contexts')\n",
    "    plt.title('Growth of Latent Contexts Over Time')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final average reward:\", avg_reward[-1])\n",
    "    print(\"Final number of inferred contexts:\", len(agent.contexts))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4c7db56e4aa600ad0a9a975c34bbf2d671fd5a4715ac0a7956790af44717dcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
