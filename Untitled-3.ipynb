{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "# import ace_tools as tools\n",
    "\n",
    "# Experiment parameters (lighter for runtime)\n",
    "n_subjects = 100\n",
    "n_trials   = 800\n",
    "block_len  = 100\n",
    "reversals  = np.arange(block_len, n_trials, block_len)\n",
    "n_arms = 2\n",
    "\n",
    "high_p = 0.8\n",
    "low_p  = 0.2\n",
    "\n",
    "alpha0 = 5\n",
    "beta0  = 5\n",
    "hazard = 0.02\n",
    "\n",
    "alpha_td = 0.1\n",
    "eps_td   = 0.1\n",
    "\n",
    "rng = np.random.default_rng(2)\n",
    "\n",
    "def init_lc():\n",
    "    return {\"alpha\": [np.full(n_arms, alpha0, dtype=float)],\n",
    "            \"beta\":  [np.full(n_arms, beta0, dtype=float)],\n",
    "            \"w\":     [1.0]}\n",
    "\n",
    "def choose_arm_lc(agent):\n",
    "    w = np.array(agent[\"w\"])\n",
    "    w /= w.sum()\n",
    "    ctx = rng.choice(len(w), p=w)\n",
    "    th = rng.beta(agent[\"alpha\"][ctx], agent[\"beta\"][ctx])\n",
    "    return int(np.argmax(th)), ctx\n",
    "\n",
    "def update_lc(agent, arm, reward, ctx_idx):\n",
    "    K = len(agent[\"w\"])\n",
    "    w = np.array(agent[\"w\"])\n",
    "    like = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        p = agent[\"alpha\"][k][arm] / (agent[\"alpha\"][k][arm] + agent[\"beta\"][k][arm])\n",
    "        like[k] = p if reward else 1 - p\n",
    "    p_new = alpha0 / (alpha0 + beta0)\n",
    "    like_new = p_new if reward else 1 - p_new\n",
    "    un = w * like\n",
    "    new_w = hazard * like_new\n",
    "    tot = un.sum() + new_w\n",
    "    post = un / tot\n",
    "    spawn_prob = new_w / tot\n",
    "    if spawn_prob > 1e-6:\n",
    "        agent[\"alpha\"].append(np.full(n_arms, alpha0, dtype=float))\n",
    "        agent[\"beta\"].append(np.full(n_arms, beta0, dtype=float))\n",
    "        agent[\"w\"].append(spawn_prob)\n",
    "        post = np.append(post, spawn_prob)\n",
    "    post /= post.sum()\n",
    "    for k in range(len(agent[\"w\"])):\n",
    "        agent[\"w\"][k] = post[k]\n",
    "        if reward:\n",
    "            agent[\"alpha\"][k][arm] += post[k]\n",
    "        else:\n",
    "            agent[\"beta\"][k][arm]  += post[k]\n",
    "\n",
    "def init_td():\n",
    "    return {\"Q\": np.zeros(n_arms, dtype=float)}\n",
    "\n",
    "def choose_arm_td(agent):\n",
    "    if rng.random() < eps_td:\n",
    "        return rng.integers(n_arms)\n",
    "    return int(np.argmax(agent[\"Q\"]))\n",
    "\n",
    "def update_td(agent, arm, reward):\n",
    "    agent[\"Q\"][arm] += alpha_td * (reward - agent[\"Q\"][arm])\n",
    "\n",
    "def run_agent(agent_type):\n",
    "    rewards = np.zeros((n_subjects, n_trials))\n",
    "    for s in range(n_subjects):\n",
    "        probs = np.array([high_p, low_p], dtype=float)\n",
    "        if agent_type == \"LC\":\n",
    "            agent = init_lc()\n",
    "        else:\n",
    "            agent = init_td()\n",
    "        for t in range(n_trials):\n",
    "            if t in reversals:\n",
    "                probs = probs[::-1]\n",
    "            if agent_type == \"LC\":\n",
    "                arm, ctx = choose_arm_lc(agent)\n",
    "                reward = 1 if rng.random() < probs[arm] else 0\n",
    "                update_lc(agent, arm, reward, ctx)\n",
    "            else:\n",
    "                arm = choose_arm_td(agent)\n",
    "                reward = 1 if rng.random() < probs[arm] else 0\n",
    "                update_td(agent, arm, reward)\n",
    "            rewards[s, t] = reward\n",
    "    return rewards\n",
    "\n",
    "rewards_lc = run_agent(\"LC\")\n",
    "rewards_td = run_agent(\"TD\")\n",
    "\n",
    "mean_curve_lc = rewards_lc.mean(axis=0)\n",
    "mean_curve_td = rewards_td.mean(axis=0)\n",
    "cum_avg_lc = mean_curve_lc.cumsum() / (np.arange(n_trials)+1)\n",
    "cum_avg_td = mean_curve_td.cumsum() / (np.arange(n_trials)+1)\n",
    "\n",
    "window = 20\n",
    "adapt_scores_lc = []\n",
    "adapt_scores_td = []\n",
    "for rev in reversals:\n",
    "    adapt_scores_lc.append(rewards_lc[:, rev:rev+window].mean(axis=1))\n",
    "    adapt_scores_td.append(rewards_td[:, rev:rev+window].mean(axis=1))\n",
    "adapt_scores_lc = np.concatenate(adapt_scores_lc)\n",
    "adapt_scores_td = np.concatenate(adapt_scores_td)\n",
    "diff = adapt_scores_lc - adapt_scores_td\n",
    "t_stat = diff.mean() / (diff.std(ddof=1) / sqrt(len(diff)))\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Agent\": [\"Latent‑Context\", \"TD‑Q\"],\n",
    "    \"Final cumulative average reward\": [cum_avg_lc[-1], cum_avg_td[-1]],\n",
    "    \"Mean adaptation reward (first 20 trials)\": [adapt_scores_lc.mean(), adapt_scores_td.mean()]\n",
    "})\n",
    "# tools.display_dataframe_to_user(\"Comparison Metrics\", summary_df)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cum_avg_lc, label=\"Latent‑Context\")\n",
    "plt.plot(cum_avg_td, label=\"TD‑Q\")\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.ylabel(\"Cumulative average reward\")\n",
    "plt.title(\"Reversal learning – cumulative performance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "rel_window = 50\n",
    "rel_trials = np.arange(-rel_window, rel_window)\n",
    "adapt_curve_lc = np.zeros_like(rel_trials, dtype=float)\n",
    "adapt_curve_td = np.zeros_like(rel_trials, dtype=float)\n",
    "for offset in rel_trials:\n",
    "    pts = reversals + offset\n",
    "    mask = (pts >= 0) & (pts < n_trials)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    adapt_curve_lc[offset+rel_window] = rewards_lc[:, pts[mask]].mean()\n",
    "    adapt_curve_td[offset+rel_window] = rewards_td[:, pts[mask]].mean()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rel_trials, adapt_curve_lc, label=\"Latent‑Context\")\n",
    "plt.plot(rel_trials, adapt_curve_td, label=\"TD‑Q\")\n",
    "plt.xlabel(\"Trial (0 = reversal)\")\n",
    "plt.ylabel(\"Mean reward\")\n",
    "plt.title(\"Adaptation dynamics around reversals\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"t‑statistic (LC – TD adaptation scores) = {t_stat:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ace_tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4c7db56e4aa600ad0a9a975c34bbf2d671fd5a4715ac0a7956790af44717dcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
