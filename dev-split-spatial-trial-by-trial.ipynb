{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import trange\n",
    "import copy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import igraph\n",
    "from matplotlib import cm, colors\n",
    "random.seed(42)\n",
    "import seaborn as sns\n",
    "from spatial_environments import * #ContinuousTMaze, GridEnvRightDownNoCue, GridEnvRightDownNoSelf, GridEnvDivergingMultipleReward, GridEnvDivergingSingleReward\n",
    "from util import *\n",
    "import itertools, random\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm.auto import trange\n",
    "# from util import transition_matrix_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting ground truch transition probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "# env = GridEnv(env_size=env_size, \n",
    "#                              rewarded_terminal = [rewarded_terminal],\n",
    "#                              cue_states=cue_states)\n",
    "# env = GridEnvRightDownNoSelf(cue_states=[6])\n",
    "\n",
    "# n_episodes = 1000\n",
    "# max_steps_per_episode = 100\n",
    "\n",
    "# dataset = generate_dataset(env, n_episodes, max_steps_per_episode)\n",
    "# denominators.shape\n",
    "actions = [0,1]\n",
    "iterations=100\n",
    "\n",
    "\n",
    "# env.plot_graph(transition_probs,'initial',savename=savename)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ground_truth_probs(actions=actions,\n",
    "                           iterations=iterations,\n",
    "                           env_size=env_size,\n",
    "                           rewarded_terminal=rewarded_terminal,\n",
    "                           cue_states=cue_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset\n",
    "# size = 4\n",
    "# env_size = (size,size)\n",
    "# rewarded_terminal = env_size[0]*env_size[1]-1\n",
    "# cue_states = [5]\n",
    "# env = GridEnvRightDownNoSelf(env_size=env_size, \n",
    "#                              rewarded_terminal = [rewarded_terminal],\n",
    "#                              cue_states=cue_states)\n",
    "# # env = GridEnv(env_size=env_size, \n",
    "# #                              rewarded_terminal = [rewarded_terminal],\n",
    "# #                              cue_states=cue_states)\n",
    "# # env = GridEnvRightDownNoSelf(cue_states=[6])\n",
    "\n",
    "# n_episodes = 1000\n",
    "# max_steps_per_episode = 100\n",
    "\n",
    "# dataset = generate_dataset(env, n_episodes, max_steps_per_episode)\n",
    "# transition_counts = transition_matrix_action(dataset)\n",
    "# denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "# denominators[denominators == 0] = 1\n",
    "# transition_probs = transition_counts / denominators\n",
    "# # denominators.shape\n",
    "# actions = [0,1]\n",
    "# iterations=100\n",
    "# used_cues = []\n",
    "# graphiter = 0\n",
    "# savename='cued'\n",
    "# env.plot_graph(transition_probs,'initial',savename=savename)  \n",
    "\n",
    "# for i in range(iterations):\n",
    "#     print(\"Iteration {}\".format(i))\n",
    "\n",
    "#     entropies = compute_transition_entropies(transition_probs)\n",
    "#     stochastic_pairs = find_stochastic_state_actions_by_entropy(entropies, eps=1e-9) # (s,a,sprime,sprime2)\n",
    "    \n",
    "#     if stochastic_pairs: \n",
    "#         cues = []\n",
    "#         for (s, a) in stochastic_pairs: # s is something like 15 (->16)\n",
    "#             # print(\"Stochastic pairs: {}\".format((s,a)))\n",
    "#             sprime, sprime2 = get_successor_states(transition_counts,s,a) # this is something like 16, 17\n",
    "#             cue = calculate_backward_contingency(dataset, sprime, sprime2, env_size)\n",
    "#             cues.append(cue)\n",
    "            \n",
    "#         # split out the successor states\n",
    "#         unique_cues = np.unique([x for sublist in cues for x in sublist])\n",
    "#         split = False\n",
    "        \n",
    "#         for cue in unique_cues:\n",
    "#             if cue in used_cues: \n",
    "#                 continue\n",
    "#             if split == True: \n",
    "#                 continue\n",
    "#             split=True # just split one at a time\n",
    "\n",
    "#             if cue > env.num_unique_states-1: # cloned state, so need to get the valid actions from the original state\n",
    "#                 valid_actions = env.get_valid_actions(env.clone_dict[cue])\n",
    "#             else:\n",
    "#                 valid_actions = env.get_valid_actions(cue)\n",
    "#             for a in valid_actions:\n",
    "#                 # print(cue,a)\n",
    "#                 successor = get_successor_states(transition_counts,cue,a)[0] # suppose 7\n",
    "#                 if successor in env.reverse_clone_dict: # this has been created before\n",
    "#                     existing_clone = env.reverse_clone_dict[successor]\n",
    "#                     for d, seq in enumerate(dataset):\n",
    "#                         states_seq = seq[0]\n",
    "#                         if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "#                             if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "#                                 # 1. modify dataset\n",
    "#                                 dataset[d][0] = [existing_clone if x==successor else x for x in dataset[d][0]] \n",
    "\n",
    "#                 else:    # hasn't been created before. split        \n",
    "#                     # split this as a function of whether it came from cue (6) vs. others\n",
    "#                     # has_state(sequence,)\n",
    "#                     n_unique_states = len(get_unique_states(dataset))\n",
    "#                     new_clone = n_unique_states            \n",
    "                    \n",
    "#                     # clone_map.append((successor,new_clone))\n",
    "#                     env.add_clone_dict(new_clone, successor)\n",
    "#                     # clone_dict[new_clone] = successor\n",
    "#                     env.add_reverse_clone_dict(new_clone, successor)\n",
    "#                     # reverse_clone_dict[successor] = new_clone\n",
    "#                     for d, seq in enumerate(dataset):\n",
    "#                         states_seq = seq[0]\n",
    "#                         if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "#                             if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "#                                 # 1. modify dataset\n",
    "\n",
    "#                                 dataset[d][0] = [new_clone if x==successor else x for x in dataset[d][0]] \n",
    "#                 # 2. modify transition count\n",
    "#                 transition_counts = transition_matrix_action(dataset)\n",
    "#                 denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "#                 denominators[denominators == 0] = 1\n",
    "#                 transition_probs = transition_counts / denominators\n",
    "#                 # graphiter = 0\n",
    "#                 env.plot_graph(transition_probs,graphiter, cue, new_clone,savename=savename)\n",
    "#                 graphiter+=1\n",
    "#             used_cues.append(cue)\n",
    "    \n",
    "#         print('\\n')\n",
    "#     else:\n",
    "#         print('Finished splitting at iteration {}'.format(i))\n",
    "#         break        \n",
    "# env.plot_graph(transition_probs,'final', savename=savename)    \n",
    "# acquisition_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "coda_eval.py – CoDA evaluation & ground‑truth generator\n",
    "------------------------------------------------------\n",
    "• Runs CoDA under multiple random seeds and plots mean ± SE KL curves.\n",
    "• Provides `compute_ground_truth_transitions()` – a deterministic,   \n",
    "  offline routine that recovers the *full* latent state‑space structure\n",
    "  (split by backward contingency) for a given seed.  You can call it\n",
    "  independently or use the returned `transition_probs_gt` for external\n",
    "  evaluation.\n",
    "\n",
    "Prerequisites\n",
    "-------------\n",
    "Assumes all helper functions & environment classes (GridEnv*, dataset\n",
    "utilities) are import‑able in PYTHONPATH.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# =============================================================\n",
    "# 0.  Metric utilities & alignment helpers\n",
    "# =============================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def _renorm_rows(P: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Row‑normalise (s,a) distributions and add ε.\"\"\"\n",
    "    P = P.copy()\n",
    "    tot = P.sum(axis=-1, keepdims=True)\n",
    "    good = tot > 0\n",
    "    P[good] /= tot[good]\n",
    "    return P + eps\n",
    "\n",
    "\n",
    "def build_alignment(\n",
    "    env_ref: \"Environment\",\n",
    "    env_cmp: \"Environment\",\n",
    "    grid_size: int,\n",
    "):\n",
    "    \"\"\"Return two dicts mapping state‑id → canonical index for *ref* and *cmp* envs.\n",
    "\n",
    "    Base grid states keep their original indices 0..grid_size‑1.\n",
    "    For every *parent* that has ≥1 clone in either env, assign one new\n",
    "    canonical index. Both clones (if present) map to that shared index.\n",
    "    \"\"\"\n",
    "    # identity mapping for base states\n",
    "    ref2canon = {s: s for s in range(grid_size)}\n",
    "    cmp2canon = {s: s for s in range(grid_size)}\n",
    "    next_idx = grid_size\n",
    "\n",
    "    parents = set(env_ref.clone_dict.values()) | set(env_cmp.clone_dict.values())\n",
    "\n",
    "    for p in sorted(parents):\n",
    "        idx = next_idx\n",
    "        next_idx += 1\n",
    "        # ref side\n",
    "        if p in env_ref.reverse_clone_dict:\n",
    "            ref_clone = env_ref.reverse_clone_dict[p]\n",
    "            ref2canon[ref_clone] = idx\n",
    "        # cmp side\n",
    "        if p in env_cmp.reverse_clone_dict:\n",
    "            cmp_clone = env_cmp.reverse_clone_dict[p]\n",
    "            cmp2canon[cmp_clone] = idx\n",
    "\n",
    "    return ref2canon, cmp2canon, next_idx  # last = canonical size\n",
    "\n",
    "\n",
    "def project_tensor(P: np.ndarray, mapping: dict[int, int], S_can: int) -> np.ndarray:\n",
    "    \"\"\"Return a (S_can, A, S_can) tensor with rows/cols placed via *mapping*.\"\"\"\n",
    "    S_src, A, _ = P.shape\n",
    "    P_new = np.zeros((S_can, A, S_can), dtype=P.dtype)\n",
    "    for s in range(S_src):\n",
    "        if s not in mapping:\n",
    "            continue  # skip states we do not align (rare)\n",
    "        row = mapping[s]\n",
    "        for sp in np.nonzero(P[s].sum(axis=0))[0]:\n",
    "            if sp not in mapping:\n",
    "                continue\n",
    "            col = mapping[sp]\n",
    "            P_new[row, :, col] = P[s, :, sp]\n",
    "    return P_new\n",
    "\n",
    "\n",
    "def aligned_kl(\n",
    "    P_true: np.ndarray,\n",
    "    env_true: \"Environment\",\n",
    "    P_model: np.ndarray,\n",
    "    env_model: \"Environment\",\n",
    "    grid_size: int,\n",
    "    eps: float = 1e-12,\n",
    ") -> float:\n",
    "    \"\"\"Compute KL after aligning clone indices (no collapsing).\"\"\"\n",
    "    ref2can, cmp2can, S_can = build_alignment(env_true, env_model, grid_size)\n",
    "    Pt = _renorm_rows(project_tensor(P_true,  ref2can, S_can), eps)\n",
    "    Pm = _renorm_rows(project_tensor(P_model, cmp2can, S_can), eps)\n",
    "\n",
    "    kl = (Pt * np.log(Pt / Pm)).sum(axis=-1)\n",
    "    mask = Pt.sum(axis=-1) > 0  # rows with any prob mass in GT\n",
    "    return kl[mask].mean()\n",
    "\n",
    "\n",
    "def transition_kl(P_true: np.ndarray, P_model: np.ndarray, *, eps: float = 1e-12) -> float:\n",
    "    \"\"\"Mean KL divergence over all state–action rows with P_true>0.\"\"\"\n",
    "    if P_true.shape != P_model.shape:\n",
    "        raise ValueError(\"Shape mismatch\")\n",
    "\n",
    "    def _renorm(P):\n",
    "        P = P.copy()\n",
    "        row_sum = P.sum(axis=-1, keepdims=True)\n",
    "        good = row_sum > 0\n",
    "        P[good] /= row_sum[good]\n",
    "        return P\n",
    "\n",
    "    Pt = _renorm(P_true) + eps\n",
    "    Pm = _renorm(P_model) + eps\n",
    "    kl = (Pt * np.log(Pt / Pm)).sum(axis=-1)\n",
    "    mask = P_true.sum(axis=-1) > 0\n",
    "    return kl[mask].mean()\n",
    "\n",
    "# =============================================================\n",
    "# 1.  Plot helper (mean ± SE)\n",
    "# =============================================================\n",
    "\n",
    "def plot_kl_curve_se(kl_histories: np.ndarray, *, color=None, label=None, ax=None):\n",
    "    kl_histories = np.asarray(kl_histories)\n",
    "    n_runs, n_eps = kl_histories.shape\n",
    "    mean = np.nanmean(kl_histories, axis=0)\n",
    "    se   = np.nanstd(kl_histories, axis=0, ddof=1) / np.sqrt(n_runs)\n",
    "    x = np.arange(1, n_eps + 1)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    line, = ax.plot(x, mean, color=color, label=label or f\"mean KL ({n_runs} seeds)\")\n",
    "    ax.fill_between(x, mean - se, mean + se, color=line.get_color(), alpha=0.25, label=\"±1 SE\")\n",
    "    ax.set_xlabel(\"Episode\"); ax.set_ylabel(\"KL divergence (nats)\")\n",
    "    ax.set_title(\"CoDA convergence (mean ± SE)\")\n",
    "    ax.grid(alpha=0.3); ax.legend(frameon=False)\n",
    "    return ax\n",
    "\n",
    "# =============================================================\n",
    "# 2.  Offline ground‑truth generator (wrapped user snippet)\n",
    "# =============================================================\n",
    "\n",
    "def compute_ground_truth_transitions(\n",
    "    *,\n",
    "    seed: int = 0,\n",
    "    size: int = 4,\n",
    "    n_episodes: int = 1000,\n",
    "    max_steps: int = 100,\n",
    "    iterations: int = 100,\n",
    "    plot: bool = False,\n",
    ") -> tuple[np.ndarray, list, 'GridEnvRightDownNoSelf']:\n",
    "    \"\"\"Recover full latent state‑space via offline splitting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    transition_probs_gt : np.ndarray (S, A, S)\n",
    "    dataset             : list[(states, actions)]  – modified by splits\n",
    "    env                 : environment instance (with clone_dict etc.)\n",
    "    \"\"\"\n",
    "    # ---------- deterministic env & dataset ------------------------\n",
    "    env_size = (size, size)\n",
    "    rewarded_terminal = [env_size[0]*env_size[1]-1]\n",
    "    cue_states = [5]\n",
    "    env = GridEnvRightDownNoSelf(env_size=env_size,\n",
    "                                 rewarded_terminal=rewarded_terminal,\n",
    "                                 cue_states=cue_states,\n",
    "                                 seed=seed)\n",
    "\n",
    "    dataset = generate_dataset(env, n_episodes, max_steps)\n",
    "\n",
    "    transition_counts = transition_matrix_action(dataset)\n",
    "    denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "    denominators[denominators == 0] = 1\n",
    "    transition_probs = transition_counts / denominators\n",
    "\n",
    "    used_cues: list[int] = []\n",
    "    graphiter = 0\n",
    "\n",
    "    if plot:\n",
    "        env.plot_graph(transition_probs, 'initial', savename='gt')\n",
    "\n",
    "    # ------------------ offline split loop ------------------------\n",
    "    for i in range(iterations):\n",
    "        entropies = compute_transition_entropies(transition_probs)\n",
    "        stochastic_pairs = find_stochastic_state_actions_by_entropy(entropies, eps=1e-9)\n",
    "        if not stochastic_pairs:\n",
    "            break\n",
    "\n",
    "        cues_tmp = []\n",
    "        for s, a in stochastic_pairs:\n",
    "            sprime, sprime2 = get_successor_states(transition_counts, s, a)\n",
    "            cue = calculate_backward_contingency(dataset, sprime, sprime2, env_size)\n",
    "            cues_tmp.append(cue)\n",
    "\n",
    "        unique_cues = np.unique([x for sub in cues_tmp for x in sub])\n",
    "        for cue in unique_cues:\n",
    "            if cue in used_cues:\n",
    "                continue\n",
    "            used_cues.append(cue)\n",
    "\n",
    "            if cue > env.num_unique_states - 1:\n",
    "                valid_actions = env.get_valid_actions(env.clone_dict[cue])\n",
    "            else:\n",
    "                valid_actions = env.get_valid_actions(cue)\n",
    "\n",
    "            for a in valid_actions:\n",
    "                successor = get_successor_states(transition_counts, cue, a)[0]\n",
    "                # --------------------------------------------------- cloning logic\n",
    "                if successor in env.reverse_clone_dict:\n",
    "                    existing_clone = env.reverse_clone_dict[successor]\n",
    "                    repl = existing_clone\n",
    "                else:\n",
    "                    repl = len(get_unique_states(dataset))\n",
    "                    env.add_clone_dict(repl, successor)\n",
    "                    env.add_reverse_clone_dict(repl, successor)\n",
    "\n",
    "                for d, seq in enumerate(dataset):\n",
    "                    states_seq = seq[0]\n",
    "                    if has_state(states_seq, successor) and has_transition(cue, successor, states_seq):\n",
    "                        dataset[d][0] = [repl if x == successor else x for x in states_seq]\n",
    "\n",
    "                # recompute counts after any replacement\n",
    "                transition_counts = transition_matrix_action(dataset)\n",
    "                denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "                denominators[denominators == 0] = 1\n",
    "                transition_probs = transition_counts / denominators\n",
    "\n",
    "            if plot:\n",
    "                env.plot_graph(transition_probs, graphiter, cue, repl, savename='gt')\n",
    "                graphiter += 1\n",
    "\n",
    "    if plot:\n",
    "        env.plot_graph(transition_probs, 'final', savename='gt')\n",
    "    return transition_probs, dataset, env\n",
    "\n",
    "# =============================================================\n",
    "# 3.  Single‑seed CoDA run  (uses env.true_transition_probs)\n",
    "# =============================================================\n",
    "\n",
    "def run_single_seed(seed: int, *, n_episodes: int = 1000, max_steps: int = 100) -> list[float]:\n",
    "    np.random.seed(seed)\n",
    "    size = 4\n",
    "    env_size = (size, size)\n",
    "    rewarded_terminal = [env_size[0]*env_size[1]-1]\n",
    "    cue_states = [5]\n",
    "\n",
    "    env = GridEnvRightDownNoSelf(env_size=env_size,\n",
    "                                 rewarded_terminal=rewarded_terminal,\n",
    "                                 cue_states=cue_states,\n",
    "                                 seed=seed)\n",
    "\n",
    "    episodes = generate_dataset(env, n_episodes, max_steps)\n",
    "    n_states = max(max(pair[0]) for pair in episodes) + 1\n",
    "    E_r = np.zeros((1, n_states)); E_nr = np.zeros_like(E_r); C = np.zeros_like(E_r)\n",
    "\n",
    "    kl_trace: list[float] = []\n",
    "    used_cues: list[int] = []\n",
    "\n",
    "    P_true = env.true_transition_probs  # ground truth from env implementation\n",
    "\n",
    "    # -------------------------------- main learning loop -----------\n",
    "    for e in range(n_episodes):\n",
    "        observations, acts = episodes[e]\n",
    "        if e == 0:\n",
    "            transition_counts = transition_matrix_action_trial_by_trial(episodes, episodes[e])\n",
    "        else:\n",
    "            transition_counts = transition_matrix_action_trial_by_trial(episodes, episodes[e], transition_counts)\n",
    "\n",
    "        entropies, conf_cnt = compute_transition_entropies_thresholded(transition_counts)\n",
    "        stochastic_pairs = find_stochastic_state_actions_by_entropy_thresholded(entropies, conf_cnt, n_threshold=3)\n",
    "\n",
    "        if stochastic_pairs:\n",
    "            # same splitting logic as online CoDA (trimmed for brevity)…\n",
    "            pass  # *** keep your previous online CoDA code here ***\n",
    "\n",
    "        denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "        denominators[denominators == 0] = 1\n",
    "        P_model = transition_counts / denominators\n",
    "        kl_trace.append(transition_kl(P_true, P_model))\n",
    "\n",
    "    return kl_trace\n",
    "\n",
    "# =============================================================\n",
    "# 4.  Batch evaluation + plot\n",
    "# =============================================================\n",
    "\n",
    "def main():\n",
    "    seeds = [0, 1, 2, 3, 4]\n",
    "    kl_mat = []\n",
    "    for sd in seeds:\n",
    "        kl_mat.append(run_single_seed(sd))\n",
    "\n",
    "    # pad to common length\n",
    "    max_len = max(len(v) for v in kl_mat)\n",
    "    kl_padded = np.full((len(seeds), max_len), np.nan)\n",
    "    for i, v in enumerate(kl_mat):\n",
    "        kl_padded[i, :len(v)] = v\n",
    "\n",
    "    plot_kl_curve_se(kl_padded, color=\"tab:blue\")\n",
    "    plt.savefig(\"kl_curve.png\", dpi=300)\n",
    "    print(\"Saved → kl_curve.png\")\n",
    "\n",
    "    first = np.nanmean(kl_padded[:, 0])\n",
    "    last  = np.nanmean(kl_padded[:, -1])\n",
    "    print(f\"KL drops from {first:.3f} to {last:.3f} in {max_len} episodes (mean over {len(seeds)} seeds).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# pools\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "cues  = np.array([5, 6, 9, 10])\n",
    "\n",
    "# 1. build the full Cartesian product  → 20 pairs\n",
    "pairs = list(itertools.product(cues, seeds))   # (cue, seed)\n",
    "\n",
    "# 2. shuffle in-place for a random order\n",
    "random.shuffle(pairs)\n",
    "\n",
    "print(pairs)      # e.g. [(5, 0), (9, 4), (10, 2), (6, 1), (5, 2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "CoDA evaluation – clone‑aware KL curve\n",
    "=====================================\n",
    "* Runs online CoDA learning under several random seeds.\n",
    "* Computes KL divergence **after aligning clone indices** between the\n",
    "  ground‑truth tensor (built offline) and the online tensor.\n",
    "* Plots mean ± SE learning curve and prints the headline drop.\n",
    "\n",
    "The script depends on all helper functions and classes used in your\n",
    "original notebook (GridEnv*, generate_dataset, etc.).  Put this file in\n",
    "the same folder or add that folder to PYTHONPATH.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0. Imports\n",
    "# ---------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import trange\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Row‑normalisation helper\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _renorm_rows(P: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Row-normalise each (state, action) categorical distribution.\n",
    "    Safe for broadcast: P shape (S, A, S).\n",
    "    \"\"\"\n",
    "    P = P.copy()\n",
    "    row_sum = P.sum(axis=-1, keepdims=True)            # (S, A, 1)\n",
    "    # Avoid division by zero: where sum==0 leave row unchanged (all zeros)\n",
    "    P = np.divide(P, np.where(row_sum == 0, 1, row_sum), dtype=P.dtype)\n",
    "    return P + eps\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Alignment utilities (keep clones separate)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_alignment(\n",
    "    env_ref,          # environment with GT tensor\n",
    "    env_cmp,          # environment with online tensor\n",
    "    grid_size: int    # e.g. 16 for 4×4 grid\n",
    ") -> Tuple[Dict[int, int], Dict[int, int], int]:\n",
    "    \"\"\"Return two mapping dicts (ref→canon, cmp→canon) and canonical size.\"\"\"\n",
    "    # Identity for base grid states 0‥grid_size‑1\n",
    "    ref2can = {s: s for s in range(grid_size)}\n",
    "    cmp2can = {s: s for s in range(grid_size)}\n",
    "    next_idx = grid_size\n",
    "\n",
    "    # Gather *all* clones that exist in either env\n",
    "    clones_ref = set(env_ref.clone_dict.keys())\n",
    "    clones_cmp = set(env_cmp.clone_dict.keys())\n",
    "    all_clones = sorted(clones_ref | clones_cmp)\n",
    "\n",
    "    for c in all_clones:\n",
    "        if c in ref2can or c in cmp2can:\n",
    "            continue  # already assigned (shared clone id)\n",
    "        ref2can[c] = next_idx\n",
    "        cmp2can[c] = next_idx\n",
    "        next_idx += 1\n",
    "\n",
    "    return ref2can, cmp2can, next_idx  # S_canonical = next_idx\n",
    "\n",
    "\n",
    "def project_tensor(P: np.ndarray, mapping: Dict[int, int], S_can: int) -> np.ndarray:\n",
    "    \"\"\"Re‑index rows/cols of *P* into canonical shape (S_can, A, S_can).\"\"\"\n",
    "    S_src, A, _ = P.shape\n",
    "    P_can = np.zeros((S_can, A, S_can), dtype=P.dtype)\n",
    "    for s in range(S_src):\n",
    "        if s not in mapping:\n",
    "            continue\n",
    "        row = mapping[s]\n",
    "        for a in range(A):\n",
    "            for sp in np.nonzero(P[s, a])[0]:\n",
    "                if sp not in mapping:\n",
    "                    continue\n",
    "                col = mapping[sp]\n",
    "                P_can[row, a, col] = P[s, a, sp]\n",
    "    return P_can\n",
    "\n",
    "\n",
    "def aligned_kl(\n",
    "    P_true: np.ndarray,\n",
    "    env_true,\n",
    "    P_model: np.ndarray,\n",
    "    env_model,\n",
    "    grid_size: int,\n",
    "    eps: float = 1e-12,\n",
    ") -> float:\n",
    "    \"\"\"KL after aligning clone indices *without collapsing* clones.\"\"\"\n",
    "    ref2can, cmp2can, S_can = build_alignment(env_true, env_model, grid_size)\n",
    "\n",
    "    Pt = _renorm_rows(project_tensor(P_true,  ref2can, S_can), eps)\n",
    "    Pm = _renorm_rows(project_tensor(P_model, cmp2can, S_can), eps)\n",
    "\n",
    "    kl = (Pt * np.log(Pt / Pm)).sum(axis=-1)  # shape (S_can, A)\n",
    "    mask = Pt.sum(axis=-1) > 0               # rows with mass in GT\n",
    "    return kl[mask].mean()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Offline ground‑truth builder (unchanged logic, now returns env)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_ground_truth(\n",
    "    *,\n",
    "    seed: int,\n",
    "    size: int = 4,\n",
    "    n_episodes: int = 1000,\n",
    "    max_steps: int = 100,\n",
    "    iterations: int = 100,\n",
    "    cue = 5,\n",
    ") -> Tuple[np.ndarray, 'GridEnvRightDownNoSelf']:\n",
    "    \"\"\"Run the offline splitter to obtain fully expanded GT tensor.\"\"\"\n",
    "    env_size = (size, size)\n",
    "    rewarded_terminal = [env_size[0]*env_size[1]-1]\n",
    "    cue_states = cue\n",
    "\n",
    "    env = GridEnvRightDownNoSelf(env_size=env_size,\n",
    "                                 rewarded_terminal=rewarded_terminal,\n",
    "                                 cue_states=cue_states,\n",
    "                                 seed=seed)\n",
    "\n",
    "    dataset = generate_dataset(env, n_episodes, max_steps)\n",
    "    transition_counts = transition_matrix_action(dataset)\n",
    "    denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "    denominators[denominators == 0] = 1\n",
    "    transition_probs = transition_counts / denominators\n",
    "\n",
    "    used_cues = []\n",
    "    for _ in range(iterations):\n",
    "        ent = compute_transition_entropies(transition_probs)\n",
    "        pairs = find_stochastic_state_actions_by_entropy(ent, eps=1e-9)\n",
    "        if not pairs:\n",
    "            break\n",
    "        cues_tmp = []\n",
    "        for s, a in pairs:\n",
    "            spr1, spr2 = get_successor_states(transition_counts, s, a)\n",
    "            cue = calculate_backward_contingency(dataset, spr1, spr2, env_size)\n",
    "            cues_tmp.append(cue)\n",
    "        unique_cues = np.unique([x for sub in cues_tmp for x in sub])\n",
    "        for cue in unique_cues:\n",
    "            if cue in used_cues:\n",
    "                continue\n",
    "            used_cues.append(cue)\n",
    "            valid_actions = env.get_valid_actions(env.clone_dict.get(cue, cue))\n",
    "            for a in valid_actions:\n",
    "                succ = get_successor_states(transition_counts, cue, a)[0]\n",
    "                if succ in env.reverse_clone_dict:\n",
    "                    clone = env.reverse_clone_dict[succ]\n",
    "                else:\n",
    "                    clone = len(get_unique_states(dataset))\n",
    "                    env.add_clone_dict(clone, succ)\n",
    "                    env.add_reverse_clone_dict(clone, succ)\n",
    "                for d, seq in enumerate(dataset):\n",
    "                    if has_state(seq[0], succ) and has_transition(cue, succ, seq[0]):\n",
    "                        dataset[d][0] = [clone if x == succ else x for x in seq[0]]\n",
    "            # update counts\n",
    "            transition_counts = transition_matrix_action(dataset)\n",
    "            denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "            denominators[denominators == 0] = 1\n",
    "            transition_probs = transition_counts / denominators\n",
    "\n",
    "    return transition_probs, env\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Plot helper (mean ± SE)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def plot_kl_curve_se(kl_hist: np.ndarray, *, color=\"tab:blue\"):\n",
    "    n_runs, n_eps = kl_hist.shape\n",
    "    mean = np.nanmean(kl_hist, axis=0)\n",
    "    se   = np.nanstd(kl_hist, axis=0, ddof=1) / np.sqrt(n_runs)\n",
    "    x = np.arange(1, n_eps+1)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(x, mean, color=color, label=f\"mean KL ({n_runs} seeds)\")\n",
    "    plt.fill_between(x, mean-se, mean+se, color=color, alpha=0.25, label=\"±1 SE\")\n",
    "    plt.xlabel(\"Episode\"); plt.ylabel(\"KL (nats)\")\n",
    "    plt.title(\"CoDA convergence – clone‑aligned KL\")\n",
    "    plt.grid(alpha=0.3); plt.legend(frameon=False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5.  Single‑seed online run\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def run_single_seed(cue: int, seed: int,  *, n_episodes: int = 1000, max_steps: int = 100) -> List[float]:\n",
    "    np.random.seed(seed)\n",
    "    size = 4\n",
    "    env_size = (size, size)\n",
    "    rewarded_terminal = [env_size[0]*env_size[1]-1]\n",
    "    cue_states = [cue]\n",
    "\n",
    "    env = GridEnvRightDownNoSelf(env_size=env_size,\n",
    "                                 rewarded_terminal=rewarded_terminal,\n",
    "                                 cue_states=cue_states,\n",
    "                                 seed=seed)\n",
    "\n",
    "    episodes = generate_dataset(env, n_episodes, max_steps)\n",
    "    n_states = max(max(pair[0]) for pair in episodes) + 1\n",
    "    E_r = np.zeros((1, n_states)); E_nr = np.zeros_like(E_r); C = np.zeros_like(E_r)\n",
    "\n",
    "    # ---- pre‑compute ground truth (tensor + env) -------------------\n",
    "    P_true, env_gt = build_ground_truth(seed=seed, cue=cue_states)\n",
    "    grid_size = size * size\n",
    "\n",
    "    kl_trace: List[float] = []\n",
    "    used_cues: List[int] = []\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        obs, _ = episodes[e]\n",
    "        # transition counts online\n",
    "        if e == 0:\n",
    "            t_counts = transition_matrix_action_trial_by_trial(episodes, episodes[e])\n",
    "        else:\n",
    "            t_counts = transition_matrix_action_trial_by_trial(episodes, episodes[e], t_counts)\n",
    "\n",
    "        ent, conf = compute_transition_entropies_thresholded(t_counts)\n",
    "        pairs = find_stochastic_state_actions_by_entropy_thresholded(ent, conf, n_threshold=3)\n",
    "\n",
    "        # ----------- (online splitting code: trimmed version) --------\n",
    "        if pairs:\n",
    "            cues_tmp = []\n",
    "            for s,a in pairs:\n",
    "                spr1, spr2 = get_successor_states(t_counts, s, a)\n",
    "                E_r, E_nr, C = accumulate_conditioned_eligibility_traces(E_r, E_nr, C, obs, spr1, spr2,\n",
    "                                                                         n_states, lam=0.8, gamma=0.9)\n",
    "                cue = calculate_backward_contingency_trial_by_trial(E_r, E_nr, C, env_size, n_threshold=20)\n",
    "                cues_tmp.append(cue)\n",
    "            for cue in np.unique([x for sub in cues_tmp for x in sub]):\n",
    "                if cue in used_cues:\n",
    "                    continue\n",
    "                used_cues.append(cue)\n",
    "                valid_actions = env.get_valid_actions(env.clone_dict.get(cue, cue))\n",
    "                for a in valid_actions:\n",
    "                    succ = get_successor_states(t_counts, cue, a)[0]\n",
    "                    if succ in env.reverse_clone_dict:\n",
    "                        clone = env.reverse_clone_dict[succ] # here\n",
    "                    else:\n",
    "                        clone = len(get_unique_states(episodes))\n",
    "                        env.add_clone_dict(clone, succ)\n",
    "                        env.add_reverse_clone_dict(clone, succ)\n",
    "                        for d, seq in enumerate(episodes):\n",
    "                            if has_state(seq[0], succ) and has_transition(cue, succ, seq[0]):\n",
    "                                episodes[d][0] = [clone if x == succ else x for x in seq[0]]\n",
    "                        n_states += 1\n",
    "                        C  = np.concatenate([C, np.zeros((1,1))], axis=1)\n",
    "                        E_r = np.concatenate([E_r, np.zeros((1,1))], axis=1)\n",
    "                        E_nr= np.concatenate([E_nr, np.zeros((1,1))], axis=1)\n",
    "                t_counts = transition_matrix_action(episodes[:e+1])\n",
    "\n",
    "        # ---- compute online tensor & KL -----------------------------\n",
    "        denom = t_counts.sum(axis=2, keepdims=True); denom[denom==0] = 1\n",
    "        P_model = t_counts / denom\n",
    "        kl = aligned_kl(P_true, env_gt, P_model, env, grid_size)\n",
    "        kl_trace.append(kl)\n",
    "    env.plot_graph(P_model,'split',savename='split.png')  \n",
    "    return kl_trace\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6.  Main – run multiple seeds\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    seeds = [0, 1, 2, 3, 4]\n",
    "    # cues  = np.array([5, 6, 9, 10])\n",
    "    cues  = np.array([5])\n",
    "\n",
    "    # 1. build the full Cartesian product  → 20 pairs\n",
    "    pairs = list(itertools.product(cues, seeds))   # (cue, seed)\n",
    "\n",
    "    # 2. shuffle in-place for a random order\n",
    "    random.shuffle(pairs)\n",
    "    \n",
    "    kl_mat = []\n",
    "    for pair in pairs:\n",
    "        # print(f\"Seed {sd} …\")\n",
    "        cue = pair[0]\n",
    "        sd = pair[1]\n",
    "        kl_mat.append(run_single_seed(cue, sd))    \n",
    "    # for sd in seeds:\n",
    "    #     print(f\"Seed {sd} …\")\n",
    "    #     kl_mat.append(run_single_seed(sd))\n",
    "\n",
    "    # pad ragged to rectangular with NaNs\n",
    "    max_len = max(len(v) for v in kl_mat)\n",
    "    kl_padded = np.full((len(pairs), max_len), np.nan)\n",
    "    for i, v in enumerate(kl_mat):\n",
    "        kl_padded[i, :len(v)] = v\n",
    "\n",
    "    plot_kl_curve_se(kl_padded)\n",
    "    plt.savefig(\"kl_curve.png\", dpi=300)\n",
    "    first = np.nanmean(kl_padded[:,0]); last = np.nanmean(kl_padded[:,-1])\n",
    "    print(f\"KL drops from {first:.3f} to {last:.3f} in {max_len} episodes (mean of {len(seeds)} seeds).  See kl_curve.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Env with 5 x 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "CoDA evaluation – clone‑aware KL curve\n",
    "=====================================\n",
    "* Runs online CoDA learning under several random seeds.\n",
    "* Computes KL divergence **after aligning clone indices** between the\n",
    "  ground‑truth tensor (built offline) and the online tensor.\n",
    "* Plots mean ± SE learning curve and prints the headline drop.\n",
    "\n",
    "The script depends on all helper functions and classes used in your\n",
    "original notebook (GridEnv*, generate_dataset, etc.).  Put this file in\n",
    "the same folder or add that folder to PYTHONPATH.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0. Imports\n",
    "# ---------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import trange\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Row‑normalisation helper\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _renorm_rows(P: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Row-normalise each (state, action) categorical distribution.\n",
    "    Safe for broadcast: P shape (S, A, S).\n",
    "    \"\"\"\n",
    "    P = P.copy()\n",
    "    row_sum = P.sum(axis=-1, keepdims=True)            # (S, A, 1)\n",
    "    # Avoid division by zero: where sum==0 leave row unchanged (all zeros)\n",
    "    P = np.divide(P, np.where(row_sum == 0, 1, row_sum), dtype=P.dtype)\n",
    "    return P + eps\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Alignment utilities (keep clones separate)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_alignment(\n",
    "    env_ref,          # environment with GT tensor\n",
    "    env_cmp,          # environment with online tensor\n",
    "    grid_size: int    # e.g. 16 for 4×4 grid\n",
    ") -> Tuple[Dict[int, int], Dict[int, int], int]:\n",
    "    \"\"\"Return two mapping dicts (ref→canon, cmp→canon) and canonical size.\"\"\"\n",
    "    # Identity for base grid states 0‥grid_size‑1\n",
    "    ref2can = {s: s for s in range(grid_size)}\n",
    "    cmp2can = {s: s for s in range(grid_size)}\n",
    "    next_idx = grid_size\n",
    "\n",
    "    # Gather *all* clones that exist in either env\n",
    "    clones_ref = set(env_ref.clone_dict.keys())\n",
    "    clones_cmp = set(env_cmp.clone_dict.keys())\n",
    "    all_clones = sorted(clones_ref | clones_cmp)\n",
    "\n",
    "    for c in all_clones:\n",
    "        if c in ref2can or c in cmp2can:\n",
    "            continue  # already assigned (shared clone id)\n",
    "        ref2can[c] = next_idx\n",
    "        cmp2can[c] = next_idx\n",
    "        next_idx += 1\n",
    "\n",
    "    return ref2can, cmp2can, next_idx  # S_canonical = next_idx\n",
    "\n",
    "\n",
    "def project_tensor(P: np.ndarray, mapping: Dict[int, int], S_can: int) -> np.ndarray:\n",
    "    \"\"\"Re‑index rows/cols of *P* into canonical shape (S_can, A, S_can).\"\"\"\n",
    "    S_src, A, _ = P.shape\n",
    "    P_can = np.zeros((S_can, A, S_can), dtype=P.dtype)\n",
    "    for s in range(S_src):\n",
    "        if s not in mapping:\n",
    "            continue\n",
    "        row = mapping[s]\n",
    "        for a in range(A):\n",
    "            for sp in np.nonzero(P[s, a])[0]:\n",
    "                if sp not in mapping:\n",
    "                    continue\n",
    "                col = mapping[sp]\n",
    "                P_can[row, a, col] = P[s, a, sp]\n",
    "    return P_can\n",
    "\n",
    "\n",
    "def aligned_kl(\n",
    "    P_true: np.ndarray,\n",
    "    env_true,\n",
    "    P_model: np.ndarray,\n",
    "    env_model,\n",
    "    grid_size: int,\n",
    "    eps: float = 1e-12,\n",
    ") -> float:\n",
    "    \"\"\"KL after aligning clone indices *without collapsing* clones.\"\"\"\n",
    "    ref2can, cmp2can, S_can = build_alignment(env_true, env_model, grid_size)\n",
    "\n",
    "    Pt = _renorm_rows(project_tensor(P_true,  ref2can, S_can), eps)\n",
    "    Pm = _renorm_rows(project_tensor(P_model, cmp2can, S_can), eps)\n",
    "\n",
    "    kl = (Pt * np.log(Pt / Pm)).sum(axis=-1)  # shape (S_can, A)\n",
    "    mask = Pt.sum(axis=-1) > 0               # rows with mass in GT\n",
    "    return kl[mask].mean()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Offline ground‑truth builder (unchanged logic, now returns env)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_ground_truth(\n",
    "    *,\n",
    "    seed: int,\n",
    "    size: int = 4,\n",
    "    n_episodes: int = 1000,\n",
    "    max_steps: int = 100,\n",
    "    iterations: int = 100,\n",
    "    cue = 5,\n",
    ") -> Tuple[np.ndarray, 'GridEnvRightDownNoSelf']:\n",
    "    \"\"\"Run the offline splitter to obtain fully expanded GT tensor.\"\"\"\n",
    "    env_size = (size, size)\n",
    "    rewarded_terminal = [env_size[0]*env_size[1]-1]\n",
    "    cue_states = cue\n",
    "\n",
    "    env = GridEnvRightDownNoSelf(env_size=env_size,\n",
    "                                 rewarded_terminal=rewarded_terminal,\n",
    "                                 cue_states=cue_states,\n",
    "                                 seed=seed)\n",
    "\n",
    "    dataset = generate_dataset(env, n_episodes, max_steps)\n",
    "    transition_counts = transition_matrix_action(dataset)\n",
    "    denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "    denominators[denominators == 0] = 1\n",
    "    transition_probs = transition_counts / denominators\n",
    "\n",
    "    used_cues = []\n",
    "    for _ in range(iterations):\n",
    "        ent = compute_transition_entropies(transition_probs)\n",
    "        pairs = find_stochastic_state_actions_by_entropy(ent, eps=1e-9)\n",
    "        if not pairs:\n",
    "            break\n",
    "        cues_tmp = []\n",
    "        for s, a in pairs:\n",
    "            spr1, spr2 = get_successor_states(transition_counts, s, a)\n",
    "            cue = calculate_backward_contingency(dataset, spr1, spr2, env_size)\n",
    "            cues_tmp.append(cue)\n",
    "        unique_cues = np.unique([x for sub in cues_tmp for x in sub])\n",
    "        for cue in unique_cues:\n",
    "            if cue in used_cues:\n",
    "                continue\n",
    "            used_cues.append(cue)\n",
    "            valid_actions = env.get_valid_actions(env.clone_dict.get(cue, cue))\n",
    "            for a in valid_actions:\n",
    "                succ = get_successor_states(transition_counts, cue, a)[0]\n",
    "                if succ in env.reverse_clone_dict:\n",
    "                    clone = env.reverse_clone_dict[succ]\n",
    "                else:\n",
    "                    clone = len(get_unique_states(dataset))\n",
    "                    env.add_clone_dict(clone, succ)\n",
    "                    env.add_reverse_clone_dict(clone, succ)\n",
    "                for d, seq in enumerate(dataset):\n",
    "                    if has_state(seq[0], succ) and has_transition(cue, succ, seq[0]):\n",
    "                        dataset[d][0] = [clone if x == succ else x for x in seq[0]]\n",
    "            # update counts\n",
    "            transition_counts = transition_matrix_action(dataset)\n",
    "            denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "            denominators[denominators == 0] = 1\n",
    "            transition_probs = transition_counts / denominators\n",
    "\n",
    "    return transition_probs, env\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Plot helper (mean ± SE)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def plot_kl_curve_se(kl_hist: np.ndarray, *, color=\"tab:blue\"):\n",
    "    n_runs, n_eps = kl_hist.shape\n",
    "    mean = np.nanmean(kl_hist, axis=0)\n",
    "    se   = np.nanstd(kl_hist, axis=0, ddof=1) / np.sqrt(n_runs)\n",
    "    x = np.arange(1, n_eps+1)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(x, mean, color=color, label=f\"mean KL ({n_runs} seeds)\")\n",
    "    plt.fill_between(x, mean-se, mean+se, color=color, alpha=0.25, label=\"±1 SE\")\n",
    "    plt.xlabel(\"Episode\"); plt.ylabel(\"KL (nats)\")\n",
    "    plt.title(\"CoDA convergence – clone‑aligned KL\")\n",
    "    plt.grid(alpha=0.3); plt.legend(frameon=False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5.  Single‑seed online run\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def run_single_seed(cue: int, seed: int,  *, n_episodes: int = 1000, max_steps: int = 100, size=4) -> List[float]:\n",
    "    np.random.seed(seed)\n",
    "    # size = 4\n",
    "    env_size = (size, size)\n",
    "    rewarded_terminal = [env_size[0]*env_size[1]-1]\n",
    "    cue_states = [cue]\n",
    "\n",
    "    env = GridEnvRightDownNoSelf(env_size=env_size,\n",
    "                                 rewarded_terminal=rewarded_terminal,\n",
    "                                 cue_states=cue_states,\n",
    "                                 seed=seed)\n",
    "\n",
    "    episodes = generate_dataset(env, n_episodes, max_steps)\n",
    "    n_states = max(max(pair[0]) for pair in episodes) + 1\n",
    "    E_r = np.zeros((1, n_states)); E_nr = np.zeros_like(E_r); C = np.zeros_like(E_r)\n",
    "\n",
    "    # ---- pre‑compute ground truth (tensor + env) -------------------\n",
    "    P_true, env_gt = build_ground_truth(seed=seed, cue=cue_states, size=size)\n",
    "    grid_size = size * size\n",
    "\n",
    "    kl_trace: List[float] = []\n",
    "    used_cues: List[int] = []\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        obs, _ = episodes[e]\n",
    "        # transition counts online\n",
    "        if e == 0:\n",
    "            t_counts = transition_matrix_action_trial_by_trial(episodes, episodes[e])\n",
    "        else:\n",
    "            t_counts = transition_matrix_action_trial_by_trial(episodes, episodes[e], t_counts)\n",
    "\n",
    "        ent, conf = compute_transition_entropies_thresholded(t_counts)\n",
    "        pairs = find_stochastic_state_actions_by_entropy_thresholded(ent, conf, n_threshold=3)\n",
    "\n",
    "        # ----------- (online splitting code: trimmed version) --------\n",
    "        if pairs:\n",
    "            cues_tmp = []\n",
    "            for s,a in pairs:\n",
    "                spr1, spr2 = get_successor_states(t_counts, s, a)\n",
    "                E_r, E_nr, C = accumulate_conditioned_eligibility_traces(E_r, E_nr, C, obs, spr1, spr2,\n",
    "                                                                         n_states, lam=0.8, gamma=0.9)\n",
    "                cue = calculate_backward_contingency_trial_by_trial(E_r, E_nr, C, env_size, n_threshold=20)\n",
    "                cues_tmp.append(cue)\n",
    "            for cue in np.unique([x for sub in cues_tmp for x in sub]):\n",
    "                if cue in used_cues:\n",
    "                    continue\n",
    "                used_cues.append(cue)\n",
    "                valid_actions = env.get_valid_actions(env.clone_dict.get(cue, cue))\n",
    "                for a in valid_actions:\n",
    "                    succ = get_successor_states(t_counts, cue, a)[0]\n",
    "                    if succ in env.reverse_clone_dict:\n",
    "                        clone = env.reverse_clone_dict[succ]\n",
    "                    else:\n",
    "                        clone = len(get_unique_states(episodes))\n",
    "                        env.add_clone_dict(clone, succ)\n",
    "                        env.add_reverse_clone_dict(clone, succ)\n",
    "                        for d, seq in enumerate(episodes):\n",
    "                            if has_state(seq[0], succ) and has_transition(cue, succ, seq[0]):\n",
    "                                episodes[d][0] = [clone if x == succ else x for x in seq[0]]\n",
    "                        n_states += 1\n",
    "                        C  = np.concatenate([C, np.zeros((1,1))], axis=1)\n",
    "                        E_r = np.concatenate([E_r, np.zeros((1,1))], axis=1)\n",
    "                        E_nr= np.concatenate([E_nr, np.zeros((1,1))], axis=1)\n",
    "                t_counts = transition_matrix_action(episodes[:e+1])\n",
    "\n",
    "        # ---- compute online tensor & KL -----------------------------\n",
    "        denom = t_counts.sum(axis=2, keepdims=True); denom[denom==0] = 1\n",
    "        P_model = t_counts / denom\n",
    "        kl = aligned_kl(P_true, env_gt, P_model, env, grid_size)\n",
    "        kl_trace.append(kl)\n",
    "\n",
    "    return kl_trace\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6.  Main – run multiple seeds\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    seeds = [0, 1, 2, 3, 4]\n",
    "    # cues  = np.array([5, 6, 9, 10])\n",
    "    cues = [6]\n",
    "\n",
    "    # 1. build the full Cartesian product  → 20 pairs\n",
    "    pairs = list(itertools.product(cues, seeds))   # (cue, seed)\n",
    "\n",
    "    # 2. shuffle in-place for a random order\n",
    "    random.shuffle(pairs)\n",
    "    \n",
    "    kl_mat = []\n",
    "    for pair in pairs:\n",
    "        # print(f\"Seed {sd} …\")\n",
    "        cue = pair[0]\n",
    "        sd = pair[1]\n",
    "        kl_mat.append(run_single_seed(cue, sd, size=5))    \n",
    "    # for sd in seeds:\n",
    "    #     print(f\"Seed {sd} …\")\n",
    "    #     kl_mat.append(run_single_seed(sd))\n",
    "\n",
    "    # pad ragged to rectangular with NaNs\n",
    "    max_len = max(len(v) for v in kl_mat)\n",
    "    kl_padded = np.full((len(pairs), max_len), np.nan)\n",
    "    for i, v in enumerate(kl_mat):\n",
    "        kl_padded[i, :len(v)] = v\n",
    "\n",
    "    plot_kl_curve_se(kl_padded)\n",
    "    plt.savefig(\"kl_curve.png\", dpi=300)\n",
    "    first = np.nanmean(kl_padded[:,0]); last = np.nanmean(kl_padded[:,-1])\n",
    "    print(f\"KL drops from {first:.3f} to {last:.3f} in {max_len} episodes (mean of {len(seeds)} seeds).  See kl_curve.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "size = 4\n",
    "env_size = (size,size)\n",
    "rewarded_terminal = env_size[0]*env_size[1]-1\n",
    "cue_states = [5]\n",
    "env = GridEnvRightDownNoSelf(env_size=env_size, \n",
    "                             rewarded_terminal = [rewarded_terminal],\n",
    "                             cue_states=cue_states)\n",
    "# env = GridEnv(env_size=env_size, \n",
    "#                              rewarded_terminal = [rewarded_terminal],\n",
    "#                              cue_states=cue_states)\n",
    "# env = GridEnvRightDownNoSelf(cue_states=[6])\n",
    "\n",
    "n_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "episodes = generate_dataset(env, n_episodes, max_steps_per_episode)\n",
    "\n",
    "n_states = max(max(pair[0]) for pair in episodes) + 1\n",
    "E_r = np.zeros((1,n_states))\n",
    "E_nr = np.zeros((1,n_states))\n",
    "C = np.zeros((1,n_states))\n",
    "\n",
    "# denominators.shape\n",
    "actions = [0,1]\n",
    "iterations=100\n",
    "used_cues = []\n",
    "graphiter = 0\n",
    "savename='cued'\n",
    "# env.plot_graph(transition_probs,'initial',savename=savename)  \n",
    "done_split = False\n",
    "# for i in range(iterations):\n",
    "for e, episode in enumerate(episodes):\n",
    "    # print(\"Iteration {}\".format(e))\n",
    "    \n",
    "    observations = episode[0]\n",
    "    actions = episode[1]\n",
    "    \n",
    "    if e == 0 :    \n",
    "        transition_counts = transition_matrix_action_trial_by_trial(episodes,episode)\n",
    "    else: \n",
    "        transition_counts = transition_matrix_action_trial_by_trial(episodes,episode, transition_counts)    \n",
    "        \n",
    "\n",
    "\n",
    "    # entropies, confidence = compute_transition_entropies_weighted(transition_counts)\n",
    "    entropies, confidence_counts = compute_transition_entropies_thresholded(transition_counts)\n",
    "    # stochastic_pairs = find_stochastic_state_actions_by_entropy_weighted(entropies, confidence, threshold=1) # (s,a,sprime,sprime2)\n",
    "    stochastic_pairs = find_stochastic_state_actions_by_entropy_thresholded(entropies, confidence_counts, n_threshold=3) # (s,a,sprime,sprime2)\n",
    "\n",
    "    if stochastic_pairs: \n",
    "        cues = []\n",
    "        for (s, a) in stochastic_pairs: # s is something like 15 (->16)\n",
    "            # print(\"Stochastic pairs: {}\".format((s,a)))\n",
    "            sprime, sprime2 = get_successor_states(transition_counts,s,a) # this is something like 16, 17\n",
    "            # cue = calculate_backward_contingency(episodes, sprime, sprime2, env_size)\n",
    "            E_r, E_nr, C = accumulate_conditioned_eligibility_traces(E_r, E_nr, C, episode[0], sprime, sprime2, \n",
    "                                                                  n_states, lam = 0.8, gamma=0.9)\n",
    "            cue = calculate_backward_contingency_trial_by_trial(E_r, E_nr, C, env_size, n_threshold = 20)\n",
    "            # cue = calculate_backward_contingency_trial_by_trial(E_r, E_nr, episodes, sprime, sprime2, env_size)\n",
    "            cues.append(cue)\n",
    "            # continue            \n",
    "        # split out the successor states\n",
    "        unique_cues = np.unique([x for sublist in cues for x in sublist])\n",
    "        split = False\n",
    "        done_split = True\n",
    "        for cue in unique_cues:\n",
    "            if cue in used_cues: \n",
    "                continue\n",
    "            if split == True: \n",
    "                continue\n",
    "            split=True # just split one at a time\n",
    "\n",
    "            if cue > env.num_unique_states-1: #17: # cloned state, so need to get the valid actions from the original state\n",
    "                valid_actions = env.get_valid_actions(env.clone_dict[cue])\n",
    "            else:\n",
    "                valid_actions = env.get_valid_actions(cue)\n",
    "            for a in valid_actions:\n",
    "                # print(cue,a)\n",
    "                successor = get_successor_states(transition_counts,cue,a)[0] # suppose 7\n",
    "                if successor in env.reverse_clone_dict: # this has been created before\n",
    "                    existing_clone = env.reverse_clone_dict[successor]\n",
    "                    for d, seq in enumerate(episodes):\n",
    "                        states_seq = seq[0]\n",
    "                        if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                            if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                # 1. modify dataset\n",
    "                                episodes[d][0] = [existing_clone if x==successor else x for x in episodes[d][0]] \n",
    "\n",
    "                else:    # hasn't been created before. split        \n",
    "                    # split this as a function of whether it came from cue (6) vs. others\n",
    "                    # has_state(sequence,)\n",
    "                    n_unique_states = len(get_unique_states(episodes))\n",
    "                    new_clone = n_unique_states            \n",
    "                    \n",
    "                    # clone_map.append((successor,new_clone))\n",
    "                    env.add_clone_dict(new_clone, successor)\n",
    "                    # clone_dict[new_clone] = successor\n",
    "                    env.add_reverse_clone_dict(new_clone, successor)\n",
    "                    # reverse_clone_dict[successor] = new_clone\n",
    "                    for d, seq in enumerate(episodes):\n",
    "                        \n",
    "                        states_seq = seq[0]\n",
    "                        if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                            if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                # 1. modify dataset\n",
    "\n",
    "                                episodes[d][0] = [new_clone if x==successor else x for x in episodes[d][0]] \n",
    "                                \n",
    "                    # update number of states\n",
    "                    n_states = max(max(pair[0]) for pair in episodes) + 1          \n",
    "                    \n",
    "                    \n",
    "                    # update ET matrix\n",
    "                    C = np.concatenate([C, np.zeros((1, 1))], axis=1)\n",
    "                    E_r = np.concatenate([E_r, np.zeros((1, 1))], axis=1)\n",
    "                    E_nr = np.concatenate([E_nr, np.zeros((1, 1))], axis=1)\n",
    "                          \n",
    "                                    \n",
    "                # 2. modify transition count\n",
    "                transition_counts = transition_matrix_action(episodes[:e])\n",
    "                denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "                denominators[denominators == 0] = 1\n",
    "                transition_probs = transition_counts / denominators\n",
    "                \n",
    "\n",
    "                # graphiter = 0\n",
    "                env.plot_graph(transition_probs,e, cue, new_clone,savename=savename)\n",
    "                graphiter+=1\n",
    "            used_cues.append(cue)\n",
    "    \n",
    "        print('\\n')\n",
    "    elif done_split:\n",
    "        print('Finished splitting at episode {}'.format(e))\n",
    "        break\n",
    "    \n",
    "    # including merging process\n",
    "    # if len(cues[0]) > 0:\n",
    "        # merging\n",
    "title_name = 'final' + str(e)            \n",
    "env.plot_graph(transition_probs,title_name, savename=savename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "size = 5\n",
    "env_size = (size,size)\n",
    "rewarded_terminal = env_size[0]*env_size[1]-1\n",
    "cue_states = [6]\n",
    "env = GridEnvRightDownNoSelf(env_size=env_size, \n",
    "                             rewarded_terminal = [rewarded_terminal],\n",
    "                             cue_states=cue_states)\n",
    "# env = GridEnv(env_size=env_size, \n",
    "#                              rewarded_terminal = [rewarded_terminal],\n",
    "#                              cue_states=cue_states)\n",
    "# env = GridEnvRightDownNoSelf(cue_states=[6])\n",
    "\n",
    "n_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "dataset = generate_dataset(env, n_episodes, max_steps_per_episode)\n",
    "transition_counts = transition_matrix_action(dataset)\n",
    "denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "denominators[denominators == 0] = 1\n",
    "transition_probs = transition_counts / denominators\n",
    "# denominators.shape\n",
    "actions = [0,1]\n",
    "iterations=100\n",
    "used_cues = []\n",
    "graphiter = 0\n",
    "savename='cued'\n",
    "env.plot_graph(transition_probs,'initial',savename=savename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_r_ = np.reshape(E_r[-1,1:env_size[0]*env_size[1]+1], (env_size[0],env_size[1]))\n",
    "E_r_ = np.transpose(E_r_)\n",
    "sns.heatmap(E_r_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_nr_ = np.reshape(E_nr[-1,1:env_size[0]*env_size[1]+1], (env_size[0],env_size[1]))\n",
    "E_nr_ = np.transpose(E_nr_)\n",
    "sns.heatmap(E_nr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_nr_ = E_nr[-1,1:env_size[0]*env_size[1]+1]\n",
    "# E_nr_ = np.transpose(E_nr_)\n",
    "E_nr_\n",
    "\n",
    "E_r_ = E_r[-1,1:env_size[0]*env_size[1]+1]\n",
    "E_r_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_contingency = E_r_ / (E_r_ + E_nr_ )\n",
    "sns.heatmap(E_contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "dataset = generate_dataset(env, n_episodes, max_steps_per_episode)\n",
    "\n",
    "sr = successor_representations(dataset, gamma=0.9, alpha=0.1, n_passes=5)\n",
    "\n",
    "sns.heatmap(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = predecessor_representations(dataset, gamma=0.9, alpha=0.1, n_passes=5)\n",
    "\n",
    "sns.heatmap(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=1e-9\n",
    "# E_r = \n",
    "for s in range(np.shape(sr)[0]):\n",
    "    dist = sr[s,:]\n",
    "    p_nonzero = dist[dist > tol]\n",
    "    \n",
    "    # Normalize them so they sum to 1\n",
    "    p_nonzero /= p_nonzero.sum()\n",
    "    \n",
    "    # Shannon entropy in bits\n",
    "    #  E = - sum(p * log2(p))\n",
    "    ent = -np.sum(p_nonzero * np.log2(p_nonzero))    \n",
    "    print('Entropy for state {} is {}'.format(s,ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generate_dataset(env, n_episodes, max_steps_per_episode)\n",
    "max(max(pair[0]) for pair in dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Functions for contingency & splitting\n",
    "\n",
    "# def get_unique_states(dataset):\n",
    "#     all_states = []\n",
    "#     for states_seq, _ in dataset:\n",
    "#         all_states.extend(states_seq)  # Flatten the list\n",
    "#     unique_states = np.unique(all_states)\n",
    "#     return unique_states\n",
    "\n",
    "# def get_unique_states_from_env(env):\n",
    "#     return [x for x in env.pos_to_state.values()]\n",
    "\n",
    "# def has_state(sequence, state):\n",
    "#     \"\"\"Return True if the episode's state sequence contains state=5.\"\"\"\n",
    "#     return state in sequence\n",
    "\n",
    "# def has_transition(s,sprime,sequence):\n",
    "#     \"\"\"Return True if the episode's state sequence contains a transition 15->16.\"\"\"\n",
    "#     for i in range(len(sequence) - 1):\n",
    "#         if sequence[i] == s and sequence[i + 1] == sprime:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# # s=12\n",
    "# # sprime=16\n",
    "# # sprime2 = 17\n",
    "# def calculate_contingency(dataset, s, sprime, sprime2):\n",
    "#     unique_states = get_unique_states(dataset)\n",
    "#     contingency_states = []\n",
    "#     for curr_state in unique_states:\n",
    "#         # if curr_state<100:\n",
    "#         if (curr_state < s or curr_state > 17):    # maybe here\n",
    "#             # print(curr_state)\n",
    "#             # episodes_with_state = 0\n",
    "#             # episodes_with_state_and_transition = 0\n",
    "#             # other =0\n",
    "#             # curr_state = 6\n",
    "\n",
    "#             total = 0\n",
    "#             a=0\n",
    "#             b=0\n",
    "#             c=0\n",
    "#             d=0\n",
    "#             conditioned_contingency=0\n",
    "#             # print(\"Current state: {}\".format(curr_state))\n",
    "#             for states_seq, actions_seq in dataset:\n",
    "#                 if has_state(states_seq,s):\n",
    "#                     total += 1\n",
    "#                     if has_state(states_seq, curr_state):\n",
    "                    \n",
    "                        \n",
    "#                         # episodes_with_state += 1\n",
    "#                         if has_transition(s,sprime,states_seq): \n",
    "#                             # episodes_with_state_and_transition += 1   \n",
    "#                             a += 1\n",
    "#                             # if curr_state==18:\n",
    "#                             #     print('a:')\n",
    "#                             #     print(states_seq)\n",
    "#                             # print('transition: {}'.format(states_seq))\n",
    "#                         elif has_transition(s,sprime2, states_seq): \n",
    "#                             # print(states_seq)\n",
    "#                             b+=1\n",
    "#                             # if curr_state==18:\n",
    "#                             #     print('b:')\n",
    "#                             #     print(states_seq)\n",
    "#                     else: \n",
    "#                         # print('here')\n",
    "#                         if has_transition(s,sprime,states_seq): \n",
    "#                             # episodes_with_state_and_transition += 1   \n",
    "#                             c += 1\n",
    "#                             # if curr_state==18:\n",
    "#                             #     print('c:')\n",
    "#                             #     print(states_seq)                            \n",
    "                            \n",
    "#                             # print('transition: {}'.format(states_seq))\n",
    "#                         elif has_transition(s,sprime2, states_seq): \n",
    "#                             # print(states_seq)\n",
    "#                             d+=1\n",
    "#                             # if curr_state==18:\n",
    "#                             #     print('d:')\n",
    "#                             #     print(states_seq)                            \n",
    "#                     assert total == a+b+c+d\n",
    "#             # if curr_state == 18: \n",
    "#             #     print(a/(a+b), d/(c+d))\n",
    "#             #     print(a,b,c,d)\n",
    "#             # if a+b != 0: \n",
    "#             #     print(\"forward contingency: {}\".format(a/(a+b)))\n",
    "#             # else: \n",
    "#             #     print(\"no forward contingency\")\n",
    "#             # if c+d != 0: \n",
    "#             #     print(\"backward contingency: {}\".format(d/(c+d)))\n",
    "#             # else: \n",
    "#             #     print(\"no backward contingency\")\n",
    "#             if a+b !=0 and c+d != 0: \n",
    "#                 # if (a/(a+b)==1 and d/(c+d)==1):\n",
    "#                 if a/(a+b)==1: # and d/(c+d)==1):\n",
    "                    \n",
    "#                     contingency_states.append(curr_state)\n",
    "#         print(f\"state {curr_state} has a value {a} and b value {b} leading to a/(a+b) = {a/(a+b) if a+b != 0 else -1}\")\n",
    "#         # print(f\"state {curr_state} has a value {a} and c value {c} leading to a/(a+c) = {a/(a+c) if a+c != 0 else -1}\")\n",
    "        \n",
    "#     print(f\"contigency states: {contingency_states}\")\n",
    "#     return contingency_states\n",
    "\n",
    "# def calculate_contingency_tmaze(dataset, s, sprime, sprime2):\n",
    "#     unique_states = get_unique_states(dataset)\n",
    "#     contingency_states = []\n",
    "#     for curr_state in unique_states:\n",
    "#         # if curr_state<100:\n",
    "#         # if (curr_state < s or curr_state > 17):\n",
    "#             # print(curr_state)\n",
    "#             # episodes_with_state = 0\n",
    "#             # episodes_with_state_and_transition = 0\n",
    "#             # other =0\n",
    "#             # curr_state = 6\n",
    "\n",
    "#         total = 0\n",
    "#         a=0\n",
    "#         b=0\n",
    "#         c=0\n",
    "#         d=0\n",
    "#         conditioned_contingency=0\n",
    "#         # print(\"Current state: {}\".format(curr_state))\n",
    "#         for states_seq, actions_seq in dataset:\n",
    "#             if has_state(states_seq,s):\n",
    "#                 total += 1\n",
    "#                 if has_state(states_seq, curr_state):\n",
    "                \n",
    "                    \n",
    "#                     # episodes_with_state += 1\n",
    "#                     if has_transition(s,sprime,states_seq): \n",
    "#                         # episodes_with_state_and_transition += 1   \n",
    "#                         a += 1\n",
    "#                         # if curr_state==18:\n",
    "#                         #     print('a:')\n",
    "#                         #     print(states_seq)\n",
    "#                         # print('transition: {}'.format(states_seq))\n",
    "#                     elif has_transition(s,sprime2, states_seq): \n",
    "#                         # print(states_seq)\n",
    "#                         b+=1\n",
    "#                         # if curr_state==18:\n",
    "#                         #     print('b:')\n",
    "#                         #     print(states_seq)\n",
    "#                 else: \n",
    "#                     # print('here')\n",
    "#                     if has_transition(s,sprime,states_seq): \n",
    "#                         # episodes_with_state_and_transition += 1   \n",
    "#                         c += 1\n",
    "#                         # if curr_state==18:\n",
    "#                         #     print('c:')\n",
    "#                         #     print(states_seq)                            \n",
    "                        \n",
    "#                         # print('transition: {}'.format(states_seq))\n",
    "#                     elif has_transition(s,sprime2, states_seq): \n",
    "#                         # print(states_seq)\n",
    "#                         d+=1\n",
    "#                         # if curr_state==18:\n",
    "#                         #     print('d:')\n",
    "#                         #     print(states_seq)                            \n",
    "#                 # assert total == a+b+c+d\n",
    "#         # if curr_state == 18: \n",
    "#         #     print(a/(a+b), d/(c+d))\n",
    "#         #     print(a,b,c,d)\n",
    "#         # if a+b != 0: \n",
    "#         #     print(\"forward contingency: {}\".format(a/(a+b)))\n",
    "#         # else: \n",
    "#         #     print(\"no forward contingency\")\n",
    "#         # if c+d != 0: \n",
    "#         #     print(\"backward contingency: {}\".format(d/(c+d)))\n",
    "#         # else: \n",
    "#         #     print(\"no backward contingency\")\n",
    "#         if a+b !=0 and c+d != 0: \n",
    "#             # if (a/(a+b)==1 and d/(c+d)==1):\n",
    "#             if a/(a+b)==1: # and d/(c+d)==1):\n",
    "                \n",
    "#                 contingency_states.append(curr_state)\n",
    "\n",
    "#     return contingency_states\n",
    "\n",
    "                \n",
    "# def get_successor_states(transition_counts,s,a):\n",
    "#     next_states = transition_counts[s,a]\n",
    "#     sprime = np.where(next_states!=0)[0]\n",
    "#     return sprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(dataset[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    print(np.shape(dataset[i][0]), np.shape(dataset[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "env = GridEnvDivergingSingleReward(cue_state=6)\n",
    "\n",
    "n_episodes = 500\n",
    "max_steps_per_episode = 10\n",
    "\n",
    "dataset = generate_dataset(env, n_episodes, max_steps_per_episode)\n",
    "transition_counts = transition_matrix_action(dataset)\n",
    "denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "denominators[denominators == 0] = 1\n",
    "transition_probs = transition_counts / denominators\n",
    "# denominators.shape\n",
    "actions = [0,1]\n",
    "iterations=10\n",
    "used_cues = []\n",
    "# clone_map = []\n",
    "clone_dict = {}\n",
    "reverse_clone_dict = {}\n",
    "graphiter = 0\n",
    "savename='cued'\n",
    "plot_graph(transition_probs,'initial',savename=savename)  \n",
    "# Actual code\n",
    "for i in range(iterations):\n",
    "    print(\"Iteration {}\".format(i))\n",
    "    # figure out which transitions are highly stochastic\n",
    "    entropies = compute_transition_entropies(transition_probs)\n",
    "    stochastic_pairs = find_stochastic_state_actions_by_entropy(entropies, eps=1e-9) # (s,a,sprime,sprime2)\n",
    "    if stochastic_pairs: \n",
    "        cues = []\n",
    "        for (s, a) in stochastic_pairs:\n",
    "            print(\"Stochastic pairs: {}\".format((s,a)))\n",
    "            # if len(vali)\n",
    "            # valid_actions = env.get_valid_actions(s)\n",
    "            tmp = get_successor_states(transition_counts,s,a)\n",
    "            print(tmp)\n",
    "            sprime, sprime2 = tmp\n",
    "\n",
    "            print(f\"successor states: {sprime}, {sprime2}\")\n",
    "\n",
    "            cue = calculate_contingency(dataset, s, sprime, sprime2)\n",
    "            # print(cue)\n",
    "            cues.append(cue)\n",
    "            \n",
    "        # split out the successor states\n",
    "        unique_cues = np.unique([x for sublist in cues for x in sublist])\n",
    "        split = False\n",
    "        \n",
    "        for cue in unique_cues:\n",
    "            if cue in used_cues: \n",
    "                continue\n",
    "            if split == True: \n",
    "                continue\n",
    "            if cue > 17:\n",
    "                print(\"Current cue: {} (clone of {})\".format(cue, clone_dict[cue]))\n",
    "            else:\n",
    "                print(\"Current cue: {}\".format(cue))\n",
    "            \n",
    "            # if cue not in used_cues:\n",
    "            split=True # just split one at a time\n",
    "            # valid actions\n",
    "            # valid_actions = env.get_valid_actions(cue)\n",
    "            # for a in valid_actions:  \n",
    "            if cue > 17: # cloned state, so need to get the valid actions from the original state\n",
    "                valid_actions = env.get_valid_actions(clone_dict[cue])\n",
    "            else:\n",
    "                valid_actions = env.get_valid_actions(cue)\n",
    "            # clone_orig = clone_dict[]\n",
    "            for a in valid_actions:\n",
    "                # print(cue,a)\n",
    "                successor = get_successor_states(transition_counts,cue,a)[0] # suppose 7\n",
    "                print(f\"successor of cue {cue} is {successor}\")\n",
    "                # if reverse_clone_dict[successor]: # this has been created before\n",
    "                if successor in reverse_clone_dict and reverse_clone_dict:\n",
    "                    existing_clone = reverse_clone_dict[successor]\n",
    "                    for d, seq in enumerate(dataset):\n",
    "                        states_seq = seq[0]\n",
    "                        if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                            if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                # 1. modify dataset\n",
    "\n",
    "                                dataset[d][0] = [existing_clone if x==successor else x for x in dataset[d][0]] \n",
    "\n",
    "                else:    # hasn't been created before. split        \n",
    "                    # split this as a function of whether it came from cue (6) vs. others\n",
    "                    # has_state(sequence,)\n",
    "                    print(2)\n",
    "                    n_unique_states = env.num_unique_states\n",
    "                    new_clone = n_unique_states + 1   \n",
    "\n",
    "                    # update the num of new states\n",
    "                    env.num_unique_states = new_clone\n",
    "\n",
    "                    print(f\"new clone {new_clone} created\")         \n",
    "                    \n",
    "                    # clone_map.append((successor,new_clone))\n",
    "                    clone_dict[new_clone] = successor\n",
    "                    reverse_clone_dict[successor] = new_clone\n",
    "                    for d, seq in enumerate(dataset):\n",
    "                        states_seq = seq[0]\n",
    "                        if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                            if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                # 1. modify dataset\n",
    "\n",
    "                                dataset[d][0] = [new_clone if x==successor else x for x in dataset[d][0]] \n",
    "                # 2. modify transition count\n",
    "                transition_counts =transition_matrix_action(dataset)\n",
    "                denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "                denominators[denominators == 0] = 1\n",
    "                transition_probs = transition_counts / denominators\n",
    "                # graphiter = 0\n",
    "                plot_graph(transition_probs,graphiter, cue, new_clone,savename=savename)\n",
    "                graphiter+=1\n",
    "            used_cues.append(cue)\n",
    "    \n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('Finished splitting at iteration {}'.format(i))\n",
    "        break        \n",
    "plot_graph(transition_probs,'final', savename=savename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Dataset with no cue (just stochastic transition to 16 or 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "env = GridEnvRightDownNoCue()\n",
    "\n",
    "n_episodes = 500\n",
    "max_steps_per_episode = 10\n",
    "\n",
    "dataset = generate_dataset(env, n_episodes, max_steps_per_episode)\n",
    "transition_counts = transition_matrix_action(dataset)\n",
    "denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "denominators[denominators == 0] = 1\n",
    "transition_probs = transition_counts / denominators\n",
    "# denominators.shape\n",
    "actions = [0,1]\n",
    "iterations=10\n",
    "used_cues = []\n",
    "# clone_map = []\n",
    "clone_dict = {}\n",
    "reverse_clone_dict = {}\n",
    "graphiter = 0\n",
    "savename = 'nocue'\n",
    "env.plot_graph(transition_probs,'initial',savename=savename)  \n",
    "# Actual code\n",
    "for i in range(iterations):\n",
    "    print(\"Iteration {}\".format(i))\n",
    "    entropies = compute_transition_entropies(transition_probs)\n",
    "    stochastic_pairs = find_stochastic_state_actions_by_entropy(entropies, eps=1e-9) # (s,a,sprime,sprime2)\n",
    "    \n",
    "    if stochastic_pairs: \n",
    "        cues = []\n",
    "        for (s, a) in stochastic_pairs:\n",
    "            print(\"Stochastic pairs: {}\".format((s,a)))\n",
    "            # if len(vali)\n",
    "            # valid_actions = env.get_valid_actions(s)\n",
    "            sprime, sprime2 = get_successor_states(transition_counts,s,a)\n",
    "            cue = calculate_contingency(dataset, s, sprime, sprime2)\n",
    "            print(cue)\n",
    "            cues.append(cue)\n",
    "            \n",
    "        # split out the successor states\n",
    "        unique_cues = np.unique([x for sublist in cues for x in sublist])\n",
    "        split = False\n",
    "        \n",
    "        for cue in unique_cues:\n",
    "            if cue in used_cues: \n",
    "                continue\n",
    "            if split == True: \n",
    "                continue\n",
    "            if cue > 17:\n",
    "                print(\"Current cue: {} (clone of {})\".format(cue, clone_dict[cue]))\n",
    "            else:\n",
    "                print(\"Current cue: {}\".format(cue))\n",
    "            \n",
    "            # if cue not in used_cues:\n",
    "            split=True # just split one at a time\n",
    "            # valid actions\n",
    "            # valid_actions = env.get_valid_actions(cue)\n",
    "            # for a in valid_actions:  \n",
    "            if cue > 17: # cloned state, so need to get the valid actions from the original state\n",
    "                valid_actions = env.get_valid_actions(clone_dict[cue])\n",
    "            else:\n",
    "                valid_actions = env.get_valid_actions(cue)\n",
    "            # clone_orig = clone_dict[]\n",
    "            for a in valid_actions:\n",
    "                # print(cue,a)\n",
    "                successor = get_successor_states(transition_counts,cue,a)[0] # suppose 7\n",
    "                # if reverse_clone_dict[successor]: # this has been created before\n",
    "                if successor in reverse_clone_dict:\n",
    "                    existing_clone = reverse_clone_dict[successor]\n",
    "                    for d, seq in enumerate(dataset):\n",
    "                        states_seq = seq[0]\n",
    "                        if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                            if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                # 1. modify dataset\n",
    "\n",
    "                                dataset[d][0] = [existing_clone if x==successor else x for x in dataset[d][0]] \n",
    "\n",
    "                else:    # hasn't been created before. split        \n",
    "                    # split this as a function of whether it came from cue (6) vs. others\n",
    "                    # has_state(sequence,)\n",
    "                    n_unique_states = len(get_unique_states(dataset))\n",
    "                    new_clone = n_unique_states + 1            \n",
    "                    \n",
    "                    # clone_map.append((successor,new_clone))\n",
    "                    clone_dict[new_clone] = successor\n",
    "                    reverse_clone_dict[successor] = new_clone\n",
    "                    for d, seq in enumerate(dataset):\n",
    "                        states_seq = seq[0]\n",
    "                        if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                            if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                # 1. modify dataset\n",
    "\n",
    "                                dataset[d][0] = [new_clone if x==successor else x for x in dataset[d][0]] \n",
    "                # 2. modify transition count\n",
    "                transition_counts = transition_matrix_action(dataset)\n",
    "                denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "                denominators[denominators == 0] = 1\n",
    "                transition_probs = transition_counts / denominators\n",
    "                # graphiter = 0\n",
    "                env.plot_graph(transition_probs,graphiter, cue, new_clone,savename=savename)\n",
    "                graphiter+=1\n",
    "            used_cues.append(cue)\n",
    "    \n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('Finished splitting at iteration {}'.format(i))\n",
    "        break        \n",
    "env.plot_graph(transition_probs,'final',savename=savename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# Splitter cell experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_nogrid(T, niter, highlight_node=0, highlight_node_2=0, save=True, savename='nogrid'):\n",
    "    # plot_graph_nogrid(transition_probs,graphiter, cue, new_clone,savename=savename)\n",
    "    # transition_probs,graphiter, cue, new_clone,savename=savename\n",
    "    n_state = np.shape(T)[0]\n",
    "    n_action = np.shape(T)[1]\n",
    "    # T = np.random.rand(n_state, n_action, n_state)\n",
    "    # Normalize so that each [s,a,:] sums to 1 (like proper probabilities)\n",
    "    # for s in range(n_state):\n",
    "    #     for a in range(n_action):\n",
    "    #         T[s, a, :] /= T[s, a, :].sum()\n",
    "\n",
    "    # Initialize a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges\n",
    "    for s in range(n_state):\n",
    "        for a in range(n_action):\n",
    "            for s_next in range(n_state):\n",
    "                prob = T[s, a, s_next]\n",
    "                if prob > 0:\n",
    "                    # You can store an edge label that contains the action \n",
    "                    # and probability.\n",
    "                    G.add_edge(\n",
    "                        f\"{s}\", f\"{s_next}\", \n",
    "                        label=f\"A{a}, p={prob:.2f}\"\n",
    "                    )\n",
    "    # Create a color list; default 'lightblue', but 'red' for special_node\n",
    "    colors = []\n",
    "    for node in G.nodes():\n",
    "        if node == highlight_node:\n",
    "            colors.append(\"red\")\n",
    "        elif node == highlight_node_2:\n",
    "            colors.append(\"yellow\")\n",
    "        else:\n",
    "            colors.append(\"lightblue\")\n",
    "    # Layout the graph\n",
    "    pos = nx.shell_layout(G)  # you can choose any layout\n",
    "\n",
    "    # Draw nodes and edges\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1500, node_color=colors)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')\n",
    "    nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=15)\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Iteration {}\".format(niter))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(transition_probs)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Talking points\n",
    "# - why is there a random extra one?\n",
    "# - how should the paths for continuous T-Maze be generated - just stop at a certain length or once there is a rewward?\n",
    "# - should the cue's randomly change for the continuous T-Maze?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CT_walks(model:ContinuousTMaze, num_observations:int=100) -> list:\n",
    "    \"\"\"Function that returns a list of observed states with debugging info\"\"\"\n",
    "    dataset = []\n",
    "    start_end_pairs = []  # Keep track of start/end pairs\n",
    "\n",
    "    for i in range(num_observations):\n",
    "        model.reset()\n",
    "        start_state = model.current_state\n",
    "        states = [model.current_state]\n",
    "        actions = []\n",
    "\n",
    "        while model.running:\n",
    "            valid_actions = model.get_valid_actions(model.current_state)\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            action = random.choice(valid_actions)\n",
    "            model.step(action)\n",
    "\n",
    "            actions.append(action)\n",
    "            states.append(model.current_state)\n",
    "        \n",
    "        end_state = states[-1]\n",
    "        start_end_pairs.append((start_state, end_state))\n",
    "        states.append(states[0])  # add the starting state to the end\n",
    "        dataset.append([states, actions])\n",
    "        \n",
    "        # Print debugging info every 10 walks\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Walk {i}:\")\n",
    "            print(f\"  Start state: {start_state}\")\n",
    "            print(f\"  Path: {states}\")\n",
    "            print(f\"  End state: {end_state}\")\n",
    "            print()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary of start/end state pairs:\")\n",
    "    from collections import Counter\n",
    "    pair_counts = Counter(start_end_pairs)\n",
    "    for pair, count in pair_counts.items():\n",
    "        print(f\"Start: {pair[0]} -> End: {pair[1]}: {count} times\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# TODO: CHECK TO SEE IF ITS THE ALTERNATING TMAZE OR JUST RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ContinuousTMaze()\n",
    "\n",
    "dataset = generate_CT_walks(env, 100)\n",
    "\n",
    "for x, _ in dataset:\n",
    "    if 5 in x:\n",
    "        print(\"sdfjhasodfguowf\")\n",
    "\n",
    "transition_counts = transition_matrix_action(dataset)\n",
    "\n",
    "denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "denominators[denominators == 0] = 1\n",
    "transition_probs = transition_counts / denominators\n",
    "# denominators.shape\n",
    "actions = [0,1]\n",
    "iterations=20\n",
    "used_cues = []\n",
    "# clone_map = []\n",
    "clone_dict = {}\n",
    "reverse_clone_dict = {}\n",
    "graphiter = 0\n",
    "savename='ctmaze'\n",
    "env.plot_graph(transition_probs,'initial',savename=savename)  \n",
    "# Actual code\n",
    "for i in range(iterations):\n",
    "    print(\"Iteration {}\".format(i))\n",
    "    entropies = compute_transition_entropies(transition_probs)\n",
    "    stochastic_pairs = find_stochastic_state_actions_by_entropy(entropies, eps=1e-9) # (s,a,sprime,sprime2)\n",
    "    \n",
    "    if stochastic_pairs: \n",
    "        cues = []\n",
    "        for (s, a) in stochastic_pairs:\n",
    "            print(\"Stochastic pairs: {}\".format((s,a)))\n",
    "            # if len(vali)\n",
    "            # valid_actions = env.get_valid_actions(s)\n",
    "            sprime, sprime2 = get_successor_states(transition_counts,s,a)\n",
    "            cue = calculate_contingency_tmaze(dataset, s, sprime, sprime2)\n",
    "            print(cue)\n",
    "            cues.append(cue)\n",
    "            \n",
    "        # split out the successor states\n",
    "        unique_cues = np.unique([x for sublist in cues for x in sublist])\n",
    "        print(unique_cues)\n",
    "        split = False\n",
    "        \n",
    "        for cue in unique_cues:\n",
    "            if cue in used_cues: \n",
    "                continue\n",
    "            if split == True: \n",
    "                continue\n",
    "            if cue > 6:\n",
    "                print(\"Current cue: {} (clone of {})\".format(cue, clone_dict[cue]))\n",
    "            else:\n",
    "                print(\"Current cue: {}\".format(cue))\n",
    "            \n",
    "            # if cue not in used_cues:\n",
    "            split=True # just split one at a time\n",
    "            # valid actions\n",
    "            # valid_actions = env.get_valid_actions(cue)\n",
    "            # for a in valid_actions:  \n",
    "            # if cue > 6: # cloned state, so need to get the valid actions from the original state\n",
    "            #     valid_actions = env.get_valid_actions(clone_dict[cue])\n",
    "            # else:\n",
    "            #     valid_actions = env.get_valid_actions(cue)\n",
    "            # clone_orig = clone_dict[]\n",
    "            for a in actions:\n",
    "                # print(cue,a)\n",
    "                if get_successor_states(transition_counts,cue,a).size>0:\n",
    "                    successor = get_successor_states(transition_counts,cue,a)[0] # suppose 7\n",
    "                    # if reverse_clone_dict[successor]: # this has been created before\n",
    "                    if successor in reverse_clone_dict:\n",
    "                        existing_clone = reverse_clone_dict[successor]\n",
    "                        for d, seq in enumerate(dataset):\n",
    "                            states_seq = seq[0]\n",
    "                            if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                                if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                    # 1. modify dataset\n",
    "\n",
    "                                    dataset[d][0] = [existing_clone if x==successor else x for x in dataset[d][0]] \n",
    "\n",
    "                    else:    # hasn't been created before. split        \n",
    "                        # split this as a function of whether it came from cue (6) vs. others\n",
    "                        # has_state(sequence,)\n",
    "                        n_unique_states = len(get_unique_states(dataset))\n",
    "                        new_clone = n_unique_states + 1            \n",
    "                        \n",
    "                        # clone_map.append((successor,new_clone))\n",
    "                        clone_dict[new_clone] = successor\n",
    "                        reverse_clone_dict[successor] = new_clone\n",
    "                        for d, seq in enumerate(dataset):\n",
    "                            states_seq = seq[0]\n",
    "                            if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                                if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                    # 1. modify dataset\n",
    "\n",
    "                                    dataset[d][0] = [new_clone if x==successor else x for x in dataset[d][0]] \n",
    "                    # 2. modify transition count\n",
    "                    transition_counts = transition_matrix_action(dataset)\n",
    "                    denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "                    denominators[denominators == 0] = 1\n",
    "                    transition_probs = transition_counts / denominators\n",
    "                    # graphiter = 0\n",
    "                    env.plot_graph(transition_probs,graphiter, cue, new_clone,savename=savename)\n",
    "                    graphiter+=1\n",
    "            used_cues.append(cue)\n",
    "    \n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('Finished splitting at iteration {}'.format(i))\n",
    "        break        \n",
    "env.plot_graph(transition_probs,'final', savename=savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_1 = [1,0,3] # start 1, left turn, rewarded\n",
    "observation_2 = [2,0,5] # start 2, right turn, rewarded\n",
    "observation_3 = [1,0,6] # start 1, right turn, no reward\n",
    "observation_4 = [2,0,4] # start 2, left turn, no reward\n",
    "# 2: left turn, 4: right turn, 5: reward, 6: no reward\n",
    "actions_1 = [0,1]\n",
    "actions_2 = [0,2]\n",
    "actions_3 = [0,2]\n",
    "actions_4 = [0,1]\n",
    "n_data = 100 #25\n",
    "super_observations = np.array(([observation_1] * n_data) +\n",
    "                          ([observation_2] * n_data) +\n",
    "                          ([observation_3] * n_data) +\n",
    "                          ([observation_4] * n_data))\n",
    "# np.random.shuffle(super_observations)\n",
    "# Build super_actions with the same structure\n",
    "super_actions = np.array(([actions_1] * n_data) +\n",
    "                         ([actions_2] * n_data) +\n",
    "                         ([actions_3] * n_data) +\n",
    "                         ([actions_4] * n_data))\n",
    "\n",
    "# We now have 100 rows in each (25 x 4 = 100).\n",
    "# Shuffle them in the *same* order using a random permutation of indices\n",
    "permutation = np.random.permutation(len(super_observations))\n",
    "\n",
    "# Apply the permutation to both arrays\n",
    "super_observations = super_observations[permutation]\n",
    "super_actions = super_actions[permutation]\n",
    "dataset=[]\n",
    "for l in range(len(super_observations)):\n",
    "    dataset.append([super_observations[l], super_actions[l]])\n",
    "\n",
    "\n",
    "\n",
    "transition_counts = transition_matrix_action(dataset)\n",
    "denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "denominators[denominators == 0] = 1\n",
    "transition_probs = transition_counts / denominators\n",
    "# denominators.shape\n",
    "actions = [0,1,2]\n",
    "iterations=20\n",
    "used_cues = []\n",
    "# clone_map = []\n",
    "clone_dict = {}\n",
    "reverse_clone_dict = {}\n",
    "graphiter = 0\n",
    "savename='tmaze'\n",
    "plot_graph_nogrid(transition_probs,'initial',savename=savename)  \n",
    "# Actual code\n",
    "for i in range(iterations):\n",
    "    print(\"Iteration {}\".format(i))\n",
    "    entropies = compute_transition_entropies(transition_probs)\n",
    "    stochastic_pairs = find_stochastic_state_actions_by_entropy(entropies, eps=1e-9) # (s,a,sprime,sprime2)\n",
    "    \n",
    "    if stochastic_pairs: \n",
    "        cues = []\n",
    "        for (s, a) in stochastic_pairs:\n",
    "            print(\"Stochastic pairs: {}\".format((s,a)))\n",
    "            # if len(vali)\n",
    "            # valid_actions = env.get_valid_actions(s)\n",
    "            sprime, sprime2 = get_successor_states(transition_counts,s,a)\n",
    "            cue = calculate_contingency_tmaze(dataset, s, sprime, sprime2)\n",
    "            print(cue)\n",
    "            cues.append(cue)\n",
    "            \n",
    "        # split out the successor states\n",
    "        unique_cues = np.unique([x for sublist in cues for x in sublist])\n",
    "        print(unique_cues)\n",
    "        split = False\n",
    "        \n",
    "        for cue in unique_cues:\n",
    "            if cue in used_cues: \n",
    "                continue\n",
    "            if split == True: \n",
    "                continue\n",
    "            if cue > 6:\n",
    "                print(\"Current cue: {} (clone of {})\".format(cue, clone_dict[cue]))\n",
    "            else:\n",
    "                print(\"Current cue: {}\".format(cue))\n",
    "            \n",
    "            # if cue not in used_cues:\n",
    "            split=True # just split one at a time\n",
    "            # valid actions\n",
    "            # valid_actions = env.get_valid_actions(cue)\n",
    "            # for a in valid_actions:  \n",
    "            # if cue > 6: # cloned state, so need to get the valid actions from the original state\n",
    "            #     valid_actions = env.get_valid_actions(clone_dict[cue])\n",
    "            # else:\n",
    "            #     valid_actions = env.get_valid_actions(cue)\n",
    "            # clone_orig = clone_dict[]\n",
    "            for a in actions:\n",
    "                # print(cue,a)\n",
    "                if get_successor_states(transition_counts,cue,a).size>0:\n",
    "                    successor = get_successor_states(transition_counts,cue,a)[0] # suppose 7\n",
    "                    # if reverse_clone_dict[successor]: # this has been created before\n",
    "                    if successor in reverse_clone_dict:\n",
    "                        existing_clone = reverse_clone_dict[successor]\n",
    "                        for d, seq in enumerate(dataset):\n",
    "                            states_seq = seq[0]\n",
    "                            if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                                if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                    # 1. modify dataset\n",
    "\n",
    "                                    dataset[d][0] = [existing_clone if x==successor else x for x in dataset[d][0]] \n",
    "\n",
    "                    else:    # hasn't been created before. split        \n",
    "                        # split this as a function of whether it came from cue (6) vs. others\n",
    "                        # has_state(sequence,)\n",
    "                        n_unique_states = len(get_unique_states(dataset))\n",
    "                        new_clone = n_unique_states + 1            \n",
    "                        \n",
    "                        # clone_map.append((successor,new_clone))\n",
    "                        clone_dict[new_clone] = successor\n",
    "                        reverse_clone_dict[successor] = new_clone\n",
    "                        for d, seq in enumerate(dataset):\n",
    "                            states_seq = seq[0]\n",
    "                            if has_state(states_seq, successor): # get all the sequence that has 7\n",
    "                                if has_transition(cue, successor,states_seq): # this is sequence that 6->7, clone 7\n",
    "                                    # 1. modify dataset\n",
    "\n",
    "                                    dataset[d][0] = [new_clone if x==successor else x for x in dataset[d][0]] \n",
    "                    # 2. modify transition count\n",
    "                    transition_counts = transition_matrix_action(dataset)\n",
    "                    denominators = transition_counts.sum(axis=2, keepdims=True)\n",
    "                    denominators[denominators == 0] = 1\n",
    "                    transition_probs = transition_counts / denominators\n",
    "                    # graphiter = 0\n",
    "                    plot_graph_nogrid(transition_probs,graphiter, cue, new_clone,savename=savename)\n",
    "                    graphiter+=1\n",
    "            used_cues.append(cue)\n",
    "    \n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('Finished splitting at iteration {}'.format(i))\n",
    "        break        \n",
    "plot_graph_nogrid(transition_probs,'final', savename=savename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_successor_states(transition_counts,cue,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_successor_states(transition_counts,1,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cscg",
   "language": "python",
   "name": "cscg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
